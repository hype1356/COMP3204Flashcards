{
    "__type__": "Deck",
    "children": [
        {
            "__type__": "Deck",
            "children": [],
            "crowdanki_uuid": "e04b0e45-d758-11ef-b75c-204ef6f14df6",
            "deck_config_uuid": "0aa25128-d1e9-11ef-92a1-204ef6f14df6",
            "desc": "",
            "dyn": 0,
            "extendNew": 0,
            "extendRev": 0,
            "media_files": [
                "paste-062025283f6eba49fc57a4c59fd9e6b12d2ab400.jpg",
                "paste-3da94841be34cb1270a78cecf3df73a1cb447a66.jpg",
                "paste-86fd902d7bcc5b452c19e00404735718e04b136a.jpg",
                "paste-8eb1657984ab24d0b55390ef10ea0ee069891efa.jpg",
                "paste-c4a6a0cd87023042604025b18c065a7f65ee36f1.jpg"
            ],
            "name": "Week 1",
            "newLimit": null,
            "newLimitToday": null,
            "notes": [
                {
                    "__type__": "Note",
                    "fields": [
                        "Function of the eye is to form an image on the ... (on ...)",
                        "retina, fovea"
                    ],
                    "guid": "x?^{KHmVY%",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "In the human eye, the lens is ..., rather than ...",
                        "shaped, moved"
                    ],
                    "guid": "xB*SUZ6SzM",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are the two types of light receptors in the eye, and how many of each are there?",
                        "Cones (10^7) and rods (10^8)"
                    ],
                    "guid": "CsP)3%=[u~",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Cones are associated with ..., while rods are associated with ... vision",
                        "colour, greylevel"
                    ],
                    "guid": "Aqpq?/7MrY",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are the terms for vision based on cones and rods, respectively?",
                        "photopic (cones) and scotopic (rods)"
                    ],
                    "guid": "mo0ACmR@W3",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are the three types of cones, and what wavelengths do they correspond to?",
                        "S - short wavelength (blue)\nM - medium wavelength (green)\nL - long wavelength (red)"
                    ],
                    "guid": "DQNGQ(N,:<",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Insufficient bandwidth of optic nerve implies ...",
                        "coding"
                    ],
                    "guid": "nXNo4].Hpb",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Which color sensor's response is poor in human vision?",
                        "Blue response (S sensors)"
                    ],
                    "guid": "ja=f<Tfp*F",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Which color sensor's response dominates in human vision?",
                        "Green response (M sensors)"
                    ],
                    "guid": "jjQ:QjWC$6",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the overall response in human vision a combination of?",
                        "Overall response from S (short wavelength - blue), M (Medium wavelength - green), and L (Long wavelength - red) sensors"
                    ],
                    "guid": "bc7DzCm|gj",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What must human vision compensate for, similar to a camera?",
                        "The poor blue sensors"
                    ],
                    "guid": "ODZBH2p2x%",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What mathematical concept is relevant to neural processing of sensory information?",
                        "Weber's law"
                    ],
                    "guid": "w1?y5Aj/%8",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "The second stage of visual processing involves associative and what other brain structure?",
                        "occipital cortices"
                    ],
                    "guid": "jXg@SDC@|7",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Visual processing involves the creation of ... and ...",
                        "links and patterns"
                    ],
                    "guid": "cf;?9dis!s",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are Mach bands?",
                        "Mach bands are not in the image: your vision introduces them"
                    ],
                    "guid": "IPoA2^4.*,",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are Mach bands a result of?",
                        "brightness adaption"
                    ],
                    "guid": "Am:U{TY+H5",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What does (a) show?<br><img src=\"paste-3da94841be34cb1270a78cecf3df73a1cb447a66.jpg\">",
                        "image showing the Mach band effect"
                    ],
                    "guid": "H7-y~cwbU!",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What does (b) show?<br><img src=\"paste-3da94841be34cb1270a78cecf3df73a1cb447a66.jpg\">",
                        "intensity of the grayscale image (a)"
                    ],
                    "guid": "nT%fn%c(T&",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What does (c) show?<br><img src=\"paste-3da94841be34cb1270a78cecf3df73a1cb447a66.jpg\">",
                        "perceived intensity across (a) as perceived by a human observer"
                    ],
                    "guid": "NFu|?QTN%q",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How human vision uses edges",
                        "The human eye needs training and can be deceived"
                    ],
                    "guid": "rVdzDykw[(",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "In static illusions, what does measurement need?",
                        "comparison"
                    ],
                    "guid": "knxLUm]$n,",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "In Benham's disk, what are illusions a consequence of?",
                        "complex function"
                    ],
                    "guid": "tqK$G~[d4t",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "<img src=\"paste-86fd902d7bcc5b452c19e00404735718e04b136a.jpg\"><br>The Most Significant Bit carries the...",
                        "most information"
                    ],
                    "guid": "f.]t<s1[hH",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "<img src=\"paste-86fd902d7bcc5b452c19e00404735718e04b136a.jpg\"><br>bit 0 is...",
                        "noise"
                    ],
                    "guid": "vDkIKPEX>0",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "<img src=\"paste-86fd902d7bcc5b452c19e00404735718e04b136a.jpg\"><br>bit 4 is the...",
                        "lighting"
                    ],
                    "guid": "m/4JV9VBaG",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the formula for an exponential function with an imaginary number?",
                        "\\[e^{jωt} = \\cos(ωt) + j\\sin(ωt) \\]<br>"
                    ],
                    "guid": "b|4z^vc4~M",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "The relationship between the frequency&nbsp;\\(ξ\\)&nbsp;and the period&nbsp;\\(T\\)&nbsp;is:",
                        "\\[ξ = \\frac{1}{T}\\]<br>"
                    ],
                    "guid": "Qm1X/)E@J[",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the formula for the Fourier transform of signal&nbsp;\\(p\\)&nbsp;at angular frequency&nbsp;\\(ω\\)?",
                        "\\[Fp(\\omega)=\\int^\\infty_{-\\infty}p(t)e^{-j\\omega t}dt\\]<br>where:<br>\\(p(t)\\): A time-variant signal, <br>\\(j\\): the complex number&nbsp;\\(j = \\sqrt{−1}\\), <br>\\(ω\\): angular frequency<br>\\(e^{-jwt}=\\cos(\\omega t)-j\\sin(\\omega t)\\)<br>"
                    ],
                    "guid": "HW`j=0ZqNc",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the formula for the inverse Fourier transform?",
                        "\\[p(t)=\\frac{1}{2\\pi}\\int^\\infty_{-\\infty}Fp(\\omega)e^{j\\omega t}d\\omega\\]<br>where:<br>\\(p(t)\\): Original signal in time domain<br>\\(Fp(ω)\\): Fourier Coefficients<br>\\(e^{jwt}=\\cos(\\omega t)+j\\sin(\\omega t)\\)<br>"
                    ],
                    "guid": "Q?{.$Gak@e",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What does the Fourier transform do?",
                        "It takes addition in the time domain and separates it into the frequency domain"
                    ],
                    "guid": "Gfh{NA]qm!",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Magnitude of the Fourier transform of a pulse <br>\\[Fp(ω) = ∫^\\infty_{-\\infty}p(t)e^{-jωt} dt = Re(Fp(ω)) + j Im(Fp(ω))\\]<br>",
                        "\\[|Fp(ω)| = \\sqrt{Re(Fp(ω))^2 + Im(Fp(ω))^2)^2}\\]<img src=\"paste-c4a6a0cd87023042604025b18c065a7f65ee36f1.jpg\"><br>"
                    ],
                    "guid": "mlhi_QoeFy",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Phase of the Fourier transform of a pulse <br>\\[Fp(ω) = ∫^\\infty_{-\\infty}p(t)e^{-jωt} dt = Re(Fp(ω)) + j Im(Fp(ω))\\]<br>",
                        "\\[\\arg(Fp(ω)) = \\tan^{-1}\\left(\\frac{Im(Fp(ω))}{Re(Fp(ω))}\\right)\\]<br><img src=\"paste-8eb1657984ab24d0b55390ef10ea0ee069891efa.jpg\"><br>"
                    ],
                    "guid": "LPps#`V1Av",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Major Components of the Visual System (Simplified Model)",
                        "<img src=\"paste-062025283f6eba49fc57a4c59fd9e6b12d2ab400.jpg\">"
                    ],
                    "guid": "fiq[#OdG|T",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                }
            ],
            "reviewLimit": null,
            "reviewLimitToday": null
        },
        {
            "__type__": "Deck",
            "children": [],
            "crowdanki_uuid": "e04b837d-d758-11ef-91d2-204ef6f14df6",
            "deck_config_uuid": "0aa25128-d1e9-11ef-92a1-204ef6f14df6",
            "desc": "",
            "dyn": 0,
            "extendNew": 0,
            "extendRev": 0,
            "media_files": [
                "paste-066d3740ab42c1730345c722de42e9c674a18dc0.jpg",
                "paste-0a33c89a2f1be465b44ef84aa375b7b72e418fdf.jpg",
                "paste-29f64c6ee603dc29c25408afe7409a03879fcaf2.jpg",
                "paste-34066571f1ad5f9628f62f2d968f7fa370ec2867.jpg",
                "paste-3f0ca1e47fc3f7a7b064e5ac581848ee46793ad2.jpg",
                "paste-4117aa8cd9e20ed3ec29cf6d7d8f61528564b0ff.jpg",
                "paste-4de6a66a0d917ca51b43021b9020d2644a0a0f75.jpg",
                "paste-588080bfaefad2a70065aa9b022c920cff1ca4ec.jpg",
                "paste-6a56863d29cae862c53da90af46f884274d20b09.jpg",
                "paste-6d28fae7d385b01c3fb8d1b19ad5b58620586bfc.jpg",
                "paste-72677c3e7e179f1974e17f51f0937dcf855e2d94.jpg",
                "paste-7c93e525537acae4bb042ce96660a43300863c3f.jpg",
                "paste-b32e0f0babbea4b90dce45b6809f007d1a56d180.jpg",
                "paste-d03ad316ada2645a0d425b4ed1b409b52cf3c911.jpg",
                "paste-e6b8d9124dea38274f2edf806bf485490f171be4.jpg",
                "paste-f33eefd97dd33615a323db53015c93f93716b63f.jpg",
                "paste-f9f6d8f7efc36b28042eb41a7eada932c9c69d80.jpg"
            ],
            "name": "Week 2",
            "newLimit": null,
            "newLimitToday": null,
            "notes": [
                {
                    "__type__": "Note",
                    "fields": [
                        "2D Fourier Transform - Forward Transform equation",
                        "\\[FP_{u,v} = \\frac{1}{N^2}\\sum^{N-1}_{x=0}\\sum^{N-1}_{y=0}P_{x,y}e^{-j\\left(\\frac{2\\pi}{N}\\right)(ux+vy)}\\]where:<br>\\(x, y\\): Two dimentions of space<br>\\(u, v\\): Two dimentions of frequency<br>"
                    ],
                    "guid": "sD8VY)_9!Q",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "2D Fourier Transform - Inverse transform equation",
                        "\\[P_{x,y}=\\sum^{N-1}_{u=0}\\sum^{N-1}_{v=0}FP_{u,v}e^{j\\left(\\frac{2\\pi}{N}\\right)(ux+vy)}\\]where:<br>\\(x,y\\): Two dimentions of space<br>\\(u,v\\): Two dimentions of frequency<br>"
                    ],
                    "guid": "LrBN7-&K6j",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the effect of shifting an image on its Fourier transform?",
                        "The magnitude of the Fourier transform remains the same, but the phase changes. The magnitude is shift invariant."
                    ],
                    "guid": "v}%18`}lv6",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What does Fourier analysis provide access to in image processing?",
                        "Fourier analysis provides access to the frequency components of an image."
                    ],
                    "guid": "K<~hfG$g1{",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the effect of a low-pass filter on an image, and what does its Fourier transform look like?",
                        "A low-pass filter blurs the image (a), and its Fourier transform (b) shows a concentration of energy in the center (low frequencies).<br><img src=\"paste-6d28fae7d385b01c3fb8d1b19ad5b58620586bfc.jpg\">&nbsp;<img src=\"paste-066d3740ab42c1730345c722de42e9c674a18dc0.jpg\">"
                    ],
                    "guid": "n:6n7_8NI>",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the effect of a high-pass filter on an image, and what does its Fourier transform look like?",
                        "A high-pass filter highlights edges and details (c), and its Fourier transform (d) shows energy concentrated away from the center (high frequencies).<br><img src=\"paste-e6b8d9124dea38274f2edf806bf485490f171be4.jpg\">&nbsp;<img src=\"paste-34066571f1ad5f9628f62f2d968f7fa370ec2867.jpg\">"
                    ],
                    "guid": "O=GBa3&IdK",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Applications of 2D Fourier Transforms",
                        "<ul><li>Understanding and analysis</li><li>Speeding up algorithms</li><li>Representation (invariance)</li><li>Coding</li><li>Recognition/ understanding (e.g. texture)</li></ul>"
                    ],
                    "guid": "j5Gzu?DC2F",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is Signal Sampling?",
                        "Signal sampling is the process of converting a continuous signal (e.g., a sound wave, a light wave) into a discrete sequence of values at specific time intervals.<br><img src=\"paste-b32e0f0babbea4b90dce45b6809f007d1a56d180.jpg\">"
                    ],
                    "guid": "j$U0mTwj56",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the Nyquist-Shannon Sampling Theorem?",
                        "To accurately reconstruct a continuous signal from its samples, the sampling rate must be at least twice the highest frequency<span style=\"font-size: 16.6667px;\"> </span>present in the original signal."
                    ],
                    "guid": "rXF]Gli2W+",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Bad Sampling (also known as _)",
                        "Aliasing, causes <br><ul><li>Loss of information: The original signal cannot be accurately reconstructed from the sampled data.</li><li>Distortion: The sampled signal misrepresents the true nature of the original signal.</li></ul><div><img src=\"paste-7c93e525537acae4bb042ce96660a43300863c3f.jpg\"><br></div>"
                    ],
                    "guid": "jy_u~2I4R]",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "&nbsp;What is a Histogram? (In Image Processing)",
                        "A histogram is a graphical representation of the distribution of pixel intensities (brightness levels) in an image.<br><br><strong>Axes:</strong>\n<ul data-sourcepos=\"12:5-13:104\">\n<li data-sourcepos=\"12:5-12:119\"><strong>X-axis:</strong> Represents the range of possible pixel intensity values (e.g., 0-255 for an 8-bit grayscale image).</li>\n<li data-sourcepos=\"13:5-13:104\"><strong>Y-axis:</strong> Represents the number of pixels in the image that have a particular intensity value.</li></ul><div><img src=\"paste-72677c3e7e179f1974e17f51f0937dcf855e2d94.jpg\"><br></div>"
                    ],
                    "guid": "tOcgz)U=]>",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What does a Histogram show about an image?",
                        "<li><strong>Contrast:</strong> The histogram visually represents the <strong>contrast</strong> of an image. Contrast is the difference in brightness between the lightest and darkest areas of an image.\n<ul>\n<li><strong>Wide Histogram:</strong> Indicates high contrast (a wide range of brightness levels are used).</li>\n<li><strong>Narrow Histogram:</strong> Indicates low contrast (a narrow range of brightness levels are used).</li>\n</ul>\n</li>\n<li><strong>Brightness Distribution:</strong>  It shows how many pixels are at each brightness level.</li>\n<li><strong>Dominant Tones:</strong> Peaks in the histogram indicate the dominant brightness levels in the image.</li>\n<li><strong>Exposure:</strong> Can give clues about the image's exposure.\n<ul>\n<li><strong>Shifted to the left:</strong> Might indicate underexposure (image is too dark).</li>\n<li><strong>Shifted to the right:</strong> Might indicate overexposure (image is too bright).</li></ul></li>"
                    ],
                    "guid": "6i^B/3vsV",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the equasion used to modify image intensity?",
                        "Image intensity can be modified by applying a linear transformation to each pixel's intensity value.<br>\\[N_{x,y}=k\\times O_{x,y}+l\\]where:<br>\\(N\\): New Image<br>\\(k\\): Scalar<br>\\(l\\): Level<br>\\(O\\): Old Image<br>"
                    ],
                    "guid": "e*),:>}{Fy",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How does the histogram change when an image is brightened?",
                        "The entire histogram shifts towards higher intensity values (to the right on the x-axis)."
                    ],
                    "guid": "p!un2nkE]y",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are Intensity Mappings?",
                        "Intensity mappings are operations that transform the pixel intensity values of an image according to a specific function or rule.&nbsp;Each pixel's original intensity value (input) is mapped to a new intensity value (output) based on the mapping function."
                    ],
                    "guid": "uJbOqxJqAR",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Intensity Mapping - Copy",
                        "<img src=\"paste-3f0ca1e47fc3f7a7b064e5ac581848ee46793ad2.jpg\">"
                    ],
                    "guid": "E{_jnqL`Vn",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Intensity Mapping - Brightness Inversion",
                        "<img src=\"paste-6a56863d29cae862c53da90af46f884274d20b09.jpg\">"
                    ],
                    "guid": "L!in4[GT|k",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Intensity Mapping - Brightness Addition",
                        "<img src=\"paste-0a33c89a2f1be465b44ef84aa375b7b72e418fdf.jpg\">"
                    ],
                    "guid": "nhB5@VAB/%",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Intensity Mapping - Brightness Scaling by Multiplication",
                        "<img src=\"paste-29f64c6ee603dc29c25408afe7409a03879fcaf2.jpg\">"
                    ],
                    "guid": "sIRl@!-a3(",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is Histogram Equalization?",
                        "Histogram equalization is a technique in image processing used to improve the contrast of an image by redistributing its pixel intensity values.&nbsp;"
                    ],
                    "guid": "MB#?$j76I}",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Goal of Histogram Equalization - Histogram Shape",
                        "Ideally, histogram equalization aims to produce an image with a flat histogram.&nbsp;A flat histogram means that all intensity levels are equally represented in the image, maximizing the use of the available dynamic range."
                    ],
                    "guid": "vRP5?W2#J!",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Histogram Equalization - Formula Derivation<br><img src=\"paste-f33eefd97dd33615a323db53015c93f93716b63f.jpg\"><img src=\"paste-f9f6d8f7efc36b28042eb41a7eada932c9c69d80.jpg\">",
                        "<img src=\"paste-4de6a66a0d917ca51b43021b9020d2644a0a0f75.jpg\">"
                    ],
                    "guid": "p7|v4:v/Ys",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is Intensity Normalization?",
                        "Intensity normalization is a process that adjusts the intensity values of an image to a standard range."
                    ],
                    "guid": "CKyZsO*BJW",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Key Differences - Intensity Normalization vs. Histogram Equalization",
                        "<li><strong>Intensity Normalization:</strong>\n<ul>\n<li>Seems to aim for a more uniform distribution of intensities, like histogram equalization.</li>\n<li>Specific algorithm might differ from standard histogram equalization.</li>\n<li>\"Grey levels all weigh the same.\"</li>\n</ul>\n</li>\n<li><strong>Histogram Equalization:</strong>\n<ul>\n<li>Explicitly aims for a flat (uniform) histogram.</li>\n<li>Based on mapping the cumulative distribution function (CDF).</li>\n<li>\"Grey levels have different weights\" - meaning the mapping emphasizes a more even distribution for visual perception.</li>\n<li>\"Aimed for human vision.\"</li></ul></li>"
                    ],
                    "guid": "ogR7KIarA,",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How to find the Inverse of a Template",
                        "The inverse template is created by flipping the original template horizontally and vertically. This is equivalent to rotating the template by 180 degrees."
                    ],
                    "guid": "u0*s=HUDEZ",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is Template Convolution?",
                        "Template convolution is a fundamental operation in image processing where a small matrix (the template, also called a kernel or filter) is slid across an image to produce a new image.&nbsp;At each position, the template's elements are multiplied by the corresponding pixel values in the image, and the results are summed up to produce a single output pixel value. The template is inverted before performing the convolution."
                    ],
                    "guid": "K.RNO5Q*m)",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Convolution vs. Correlation",
                        "The difference lies in whether the template is flipped or not. This affects how the template interacts with the image and the resulting output."
                    ],
                    "guid": "e!-]Pc>0l@",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is an Averaging Operator in Image Processing?",
                        "An averaging operator (also known as a mean filter or box filter) is a type of linear smoothing filter used in image processing to reduce noise and blur an image.&nbsp;It works by replacing each pixel's value with the average of the pixel values within a defined neighborhood (window or kernel) around it. The neighborhood is typically defined by a square kernel (e.g., 3x3, 5x5, 7x7) where all the weights are equal.<br><img src=\"paste-4117aa8cd9e20ed3ec29cf6d7d8f61528564b0ff.jpg\"><img src=\"paste-588080bfaefad2a70065aa9b022c920cff1ca4ec.jpg\">"
                    ],
                    "guid": "G#|;)3O0.B",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Averaging Operator - Formula",
                        "\\[N_{x,y}=\\frac{1}{9}\\sum_{i\\in3}\\sum_{j\\in3}o_{x(i),y(j)}\\]<img src=\"paste-d03ad316ada2645a0d425b4ed1b409b52cf3c911.jpg\"><img src=\"paste-4117aa8cd9e20ed3ec29cf6d7d8f61528564b0ff.jpg\"><br>"
                    ],
                    "guid": "D$#e*<yk#Z",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the relationship between window size of averaging operator and blurring?",
                        "There is a direct relationship between window size and the amount of blurring:\n<ul>\n<li><strong>Larger Window Size:</strong>  More blurring.</li>\n<li><strong>Smaller Window Size:</strong> Less blurring.</li></ul><div>A larger window averages the pixel values over a larger area, resulting in a smoother, more blurred image.<br></div>"
                    ],
                    "guid": "rhd?yM,eF5",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "2D Gaussian Function - Formula",
                        "\\[g(x,y,\\sigma)=\\frac{1}{2\\pi\\sigma^2}e^{\\frac{-(x^2+y^2)}{2\\sigma^2}}\\]where:<br>\\(x,y\\): coordinates relative to the centrer of the Gaussian<br>\\(\\sigma\\): The standard deviation, controls the spread or width of the bell curve. This value squared (\\(\\sigma^2\\)) is the variance.<br>"
                    ],
                    "guid": "j?%A1Yp&1M",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Gaussian - Compromise between Variance and Window Size",
                        "There's a relationship between the desired amount of smoothing (controlled by variance σ²) and the size of the template (window size) needed to achieve that smoothing.<br><br><strong>Larger Variance (σ²):</strong> Requires a larger window size to accurately represent the wider Gaussian curve.<br><strong>Smaller Variance (σ²):</strong> Can be represented with a smaller window size.<br><br>Choosing a window size that is too small for a given variance can lead to inaccurate filtering because the template doesn't fully capture the Gaussian's influence."
                    ],
                    "guid": "f6^NVcU9dH",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What does the Convolution Theorem state?",
                        "The convolution theorem states that convolution in the spatial domain is equivalent to point-by-point multiplication in the frequency domain."
                    ],
                    "guid": "I[75BHNfL",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Convolution Theorem - Mathematial Representation",
                        "\\[P\\times T=\\Im^1(\\Im(P).\\times\\Im(T))\\]where:<br>\\(P\\): Picture<br>\\(T\\): Template<br>\\(\\Im(x)\\): Fourier transform of&nbsp;\\(x\\)<br>\\(.\\times\\): <b>Point by Point</b> multiplication<br>"
                    ],
                    "guid": "A~u&m!VhG2",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why use the Fourier Transform for Convolution?",
                        "For larger template sizes (generally 7x7 or greater), performing convolution via the Fast Fourier Transform (FFT) is computationally faster than direct convolution in the spatial domain."
                    ],
                    "guid": "wt)W2.n)#P",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                }
            ],
            "reviewLimit": null,
            "reviewLimitToday": null
        },
        {
            "__type__": "Deck",
            "children": [],
            "crowdanki_uuid": "e04bd1c4-d758-11ef-a3fd-204ef6f14df6",
            "deck_config_uuid": "0aa25128-d1e9-11ef-92a1-204ef6f14df6",
            "desc": "",
            "dyn": 0,
            "extendNew": 0,
            "extendRev": 0,
            "media_files": [
                "paste-0612255933856fe8d61c2145c7f8fef0b1bbd289.jpg",
                "paste-230f0bbfce769d5a3d1dffb3f58f57eac04a7f15.jpg",
                "paste-49120322c1afb64005b20dc4d7ee3f691b142043.jpg",
                "paste-5ee45506f6195f2e80497ff87530bbbfa99001cb.jpg",
                "paste-61f0b76d4d2fb083bfb98923d80d0e19a518ec2c.jpg",
                "paste-623638dc681bd4ad067d367fd9dae84b60164867.jpg",
                "paste-63cc5d7930053dd77944d899d728ae977788dd90.jpg",
                "paste-7705d52b66e1402453ca3f79cb8a6a8715870a91.jpg",
                "paste-7a2367dba24d07b97d071d47bde07a149e0d9bb8.jpg",
                "paste-7fdcd41a3d98745653f1dc1f731ca86184187ced.jpg",
                "paste-826f0b6c87499521146a6c468e3b82065fe33d62.jpg",
                "paste-8291002046e9c9f2636960b90dec8a73b7ddd3de.jpg",
                "paste-9e4a792228f6501f7daea4fb8d7e5c718811ce8d.jpg",
                "paste-b85aaa1c17586e8128bc6e0190b33bf3bc57e094.jpg",
                "paste-ebf94c13ece6c952d30a462332a7f031c99a3009.jpg",
                "paste-f1b190387f6fca308640bb6ac78277625fe8ab77.jpg",
                "paste-f8b1443b9a497b5d4c080e143c667d2d3ac17309.jpg"
            ],
            "name": "Week 3",
            "newLimit": null,
            "newLimitToday": null,
            "notes": [
                {
                    "__type__": "Note",
                    "fields": [
                        "What is an edge?",
                        "An edge is a significant local change in image intensity. It usually corresponds to a discontinuity in:\n<ul>\n<li><strong>Brightness:</strong> A sharp transition from a dark to a light area or vice versa.</li>\n<li><strong>Color:</strong> A sudden change in color.</li>\n<li><strong>Texture:</strong> A shift from one texture pattern to another.</li></ul>"
                    ],
                    "guid": "p?(Q#+u*1m",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is First-Order Edge Detection?",
                        "First-order edge detection methods find edges by calculating the <em>gradient</em> (rate of change of intensity) of an image. <br>Edges are associated with rapid changes in pixel intensity. The gradient is a measure of how quickly the intensity is changing in a particular direction.<br>These methods use the first-order derivative (or an approximation of it) to estimate the gradient.<br>Typically produce an \"edge map\" where the magnitude of the gradient (edge strength) is represented at each pixel."
                    ],
                    "guid": "h{3vf#_1Ni",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "First Order Edge Detection - Formula<br><img src=\"paste-7705d52b66e1402453ca3f79cb8a6a8715870a91.jpg\">",
                        "<b>Vertical Edges,&nbsp;</b>\\(Ex\\):<br>\\(Ex_{x,y}=|P_{x,y}-P_{x+1,y}|\\)<br><b>Horizontal Edges,&nbsp;</b>\\(Ey\\):<br>\\(Ey_{x,y}=|P_{x,y}-P_{x,y+1}|\\)<br><b>Vertical and Horizontal Edges:<br></b>\\(E_{x,y}=|2\\times P_{x,y}-P_{x+1,y}-P_{x,y+1}|\\)"
                    ],
                    "guid": "GZ|}5<);FB",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "First Order Edge Detection - Implementation",
                        "<b>Template:<br><img src=\"paste-b85aaa1c17586e8128bc6e0190b33bf3bc57e094.jpg\"><br>Code:<br></b><img src=\"paste-7a2367dba24d07b97d071d47bde07a149e0d9bb8.jpg\">"
                    ],
                    "guid": "cMSby`?XAw",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is a Taylor Series?",
                        "A Taylor series is a representation of a function as an infinite sum of terms that are calculated from the values of the function's derivatives at a single point. Used to&nbsp;approximate a function using a polynomial, making it easier to analyze or compute.&nbsp;The more terms you include in the series, the better the approximation becomes (generally)."
                    ],
                    "guid": "zLvMNjDyfU",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Taylor Series - General Formula",
                        "\\[f(t+\\Delta t)=f(t)+f'(t)\\Delta t+\\frac{f''(t)}{2!}(\\Delta t)^2+\\frac{f'''(t)}{3!}(\\Delta t)^3+\\;...++\\frac{f^n(t)}{n!}(\\Delta t)^n\\]<img src=\"paste-63cc5d7930053dd77944d899d728ae977788dd90.jpg\"><br>"
                    ],
                    "guid": "jSsd&SUFcf",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why are these templates called \"improved\" first-order differences?<br><img src=\"paste-f1b190387f6fca308640bb6ac78277625fe8ab77.jpg\">",
                        "They use a wider range of pixels (3 instead of 2) for a more accurate approximation of the derivative.<br><br>This reduces the impact of noise and provides a smoother result."
                    ],
                    "guid": "u(y.)$MkBT",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How do you determine the direction of an edge?",
                        "\\[\\Theta=\\tan^{-1}\\left(\\frac{M_y}{M_x}\\right)\\]<img src=\"paste-9e4a792228f6501f7daea4fb8d7e5c718811ce8d.jpg\"><br>"
                    ],
                    "guid": "w=R?H@Us!R",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How do you calculate the overall magnitude of an edge?",
                        "\\[M=\\sqrt{M_x^2+M^2_y}\\]<br><img src=\"paste-9e4a792228f6501f7daea4fb8d7e5c718811ce8d.jpg\"><br>"
                    ],
                    "guid": "si4>e&gW8f",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the Prewitt operator used for?",
                        "It's an edge detection operator that finds edges in an image by calculating the gradient of the image intensity."
                    ],
                    "guid": "Q]bs(.?MJ_",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How does the Prewitt operator work?",
                        "It uses two 3x3 kernels (or templates) to calculate the gradient in the x-direction (Mx) and y-direction (My). These kernels approximate the derivatives."
                    ],
                    "guid": "w6s?I2#~h/",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What does \"average improved horizontal and vertical operators\" mean in the context of the Prewitt operator?",
                        "<strong>Improved:</strong>  The Prewitt operator uses a 3x3 kernel, which gives a more accurate approximation of the gradient than simpler 2x2 kernels.<br><br>\n<strong>Average:</strong> It effectively averages the intensity differences across 3 pixels in each direction. This helps to smooth out noise and provide a more robust edge detection."
                    ],
                    "guid": "^Y2SL{(4&",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What does the Mx template for the Prewitt operator look like?",
                        "<img src=\"paste-623638dc681bd4ad067d367fd9dae84b60164867.jpg\">"
                    ],
                    "guid": "e/ykr0O~]S",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What does the My template for the Prewitt operator look like?",
                        "<img src=\"paste-f8b1443b9a497b5d4c080e143c667d2d3ac17309.jpg\">"
                    ],
                    "guid": "LFfO%iZmU$",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What does the vector format representation of the Prewitt Operator show?<br><img src=\"paste-49120322c1afb64005b20dc4d7ee3f691b142043.jpg\">",
                        "The vector format uses arrows to show the gradient at each pixel. The direction of the arrow indicates the direction of the gradient (pointing towards the area of greatest intensity increase), and the length of the arrow often represents the magnitude (strength) of the gradient."
                    ],
                    "guid": "zj&DYC${]9",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the Sobel operator?",
                        "A popular edge detection operator that calculates the gradient of image intensity to find edges. It's known for being relatively simple but effective."
                    ],
                    "guid": "hJb1Fo|[$/",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How is the Sobel operator related to the Prewitt operator?",
                        "The Sobel operator essentially doubles the center coefficients in the Prewitt templates. This gives more weight to the central pixels when calculating the gradient.<br><img src=\"paste-230f0bbfce769d5a3d1dffb3f58f57eac04a7f15.jpg\">"
                    ],
                    "guid": "D,^^(g%=yL",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is Pascal's triangle used for in the context of the Sobel operator?",
                        "It's used to generate the coefficients for Sobel kernels of different sizes (e.g., 3x3, 5x5, 7x7). This allows you to create Sobel operators that capture edge information over a wider area."
                    ],
                    "guid": "PJ8$XpQd7>",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How do you use Pascal's triangle for \"averaging\" in a Sobel kernel?",
                        "Each row in Pascal's triangle corresponds to the coefficients for averaging across a certain window size.<br><br>Example: The row \"1 2 1\"&nbsp;gives you the coefficients for averaging across 3 pixels, which is used in the standard 3x3 Sobel operator."
                    ],
                    "guid": "sN3M|d`mw,",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How do you use Pascal's triangle for \"differencing\" in a Sobel kernel?",
                        "<ol><li>Take the coefficients from the corresponding \"averaging\" row in Pascal's triangle.</li><li>\nMirror the coefficients (reverse the order).</li><li>Change the sign of the mirrored half.</li></ol><div>Example: <code>1 2 1</code> (averaging) becomes <code>1 0 -1</code> (differencing) for the 3x3 Sobel.<br></div>"
                    ],
                    "guid": "y|.$}::XiD",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are the benefits of using larger Sobel kernels (e.g., 5x5)?",
                        "<li>Capture more context: They consider a larger neighborhood of pixels when calculating the gradient.</li>\n<li>Smoother results: Can help reduce noise and produce smoother edges.</li>\n<li>Potentially detect more subtle edges.</li>"
                    ],
                    "guid": "O@I4LLQRUP",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are the potential drawbacks of using larger Sobel kernels?",
                        "<li>Increased computation:  They require more calculations.</li>\n<li>Blurring: Can potentially blur finer details in the image.</li>\n<li>May not be suitable for detecting sharp, localized edges.</li>"
                    ],
                    "guid": "DtKi!89.<|",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are some potential issues or limitations of the Sobel operator",
                        "<li><strong>Blurred edges:</strong> The Sobel operator can sometimes produce slightly blurred edges, especially with larger kernels.</li>\n<li><strong>Noisy edges:</strong> It can be sensitive to noise in the image, leading to the detection of spurious edges. This is why thresholding is often used to clean up the results.</li>"
                    ],
                    "guid": "E}XB_k=a,p",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are the three main objectives of the Canny edge detection operator?",
                        "<li>Optimal detection with no spurious responses (minimize false edges).</li>\n<li>Good localization (detected edges should be close to the true edges).</li>\n<li>Single response (avoid multiple responses for a single edge).</li>"
                    ],
                    "guid": "tyn%#>vD{$",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are the four main steps in the Canny edge detection algorithm?",
                        "<ol><li>Gaussian smoothing</li><li>\nSobel operator</li><li>\nNon-maximum suppression</li><li>\nThresholding with hysteresis to connect edge points</li></ol>"
                    ],
                    "guid": "G0QZiqKG(N",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why is Gaussian smoothing used in the Canny edge detector?",
                        "To reduce noise in the image before edge detection. This helps to prevent the detection of false edges caused by noise."
                    ],
                    "guid": "Vbt]6~P+6",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the purpose of non-maximum suppression in edge detectors?",
                        "To thin the edges by identifying and preserving only the local maxima in the gradient magnitude image. This helps to create thin, one-pixel-wide edges."
                    ],
                    "guid": "vM>r$]aN};",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is hysteresis thresholding, and why is it used in the Canny edge detector?",
                        "It uses two thresholds (high and low) to identify strong and weak edges. Weak edges are kept only if they are connected to strong edges. This helps to create continuous edges while reducing noise."
                    ],
                    "guid": "I1yX7Apx++",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How does the Canny edge detector improve upon simpler methods like just using the Sobel operator?",
                        "<li>It addresses noise reduction (Gaussian smoothing).</li>\n<li>It produces thinner, more precise edges (non-maximum suppression).</li>\n<li>It creates more connected and continuous edges (hysteresis thresholding).</li>\n<li>Overall, it aims for more accurate and reliable edge detection.</li>"
                    ],
                    "guid": "O1/@VtN1vb",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why is interpolation needed in non-maximum suppression?",
                        "Because the direction of the edge might not perfectly align with the pixel grid. Interpolation allows us to estimate the gradient magnitude at points that lie between the actual pixels."
                    ],
                    "guid": "B^*aDo/OO",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What type of interpolation is commonly used in non-maximum suppression?",
                        "Linear interpolation. This involves estimating the value at a point by linearly interpolating between the values of its neighboring pixels."
                    ],
                    "guid": "F7bJ*IJ8Sb",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How is interpolation used to determine if a pixel is a local maximum?",
                        "<ol><li>Determine the edge direction at the pixel.</li><li>\nIdentify the two neighboring pixels that lie along the edge direction (M1 and M2 in the diagram).</li><li>\nUse linear interpolation to estimate the gradient magnitude at M1 and M2.</li><li>\nIf the pixel's gradient magnitude is greater than both M1 and M2, it's a local maximum (edge pixel). Otherwise, it's suppressed.</li></ol>"
                    ],
                    "guid": "d*(}]^?ar?",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are the benefits of using interpolation in non-maximum suppression?",
                        "<li>More accurate edge localization: By considering points between pixels, it helps to more accurately identify the true edge location.</li>\n<li>Improved edge thinness:  Leads to thinner and more precise edges.</li>"
                    ],
                    "guid": "D5cIjVS*5(",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How does hysteresis thresholding work?",
                        "<li>Pixels with a gradient magnitude above the upper threshold are considered strong edge pixels.</li>\n<li>Pixels below the lower threshold are discarded as non-edges.</li>\n<li>Pixels with a gradient magnitude between the two thresholds are considered weak edge pixels.</li>\n<li>Weak pixels are only kept if they are connected to strong edge pixels.</li>"
                    ],
                    "guid": "f|.-VU#9;C",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the purpose of the upper threshold in hysteresis thresholding?",
                        "To identify strong, definite edges in the image. These are pixels with a high gradient magnitude that are very likely to be part of a true edge."
                    ],
                    "guid": "xXt>K;Vcqe",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the purpose of the lower threshold in hysteresis thresholding?",
                        "To help identify potential edge pixels that might be part of a true edge but have a weaker gradient magnitude. These pixels are only kept if they connect to strong edges."
                    ],
                    "guid": "d{y`3k0qZ5",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are the benefits of using hysteresis thresholding in edge detection?",
                        "<li>Reduces noise: Helps to eliminate spurious edges caused by noise.</li>\n<li>Creates continuous edges:  Connects weak edges to strong edges, leading to more continuous and complete edges.</li>\n<li>More robust edge detection:  Provides a more robust way to identify edges compared to using a single threshold.</li>"
                    ],
                    "guid": "CkYf?p3#V$",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "In the diagram, what do the blue and red arrows represent?<br><img src=\"paste-7fdcd41a3d98745653f1dc1f731ca86184187ced.jpg\">",
                        "<li>Blue arrows: Indicate pixels that are above the upper threshold (strong edges) or below the lower threshold (non-edges).</li>\n<li>Red arrows: Indicate pixels that fall between the two thresholds (weak edges). These pixels will be evaluated based on their connectivity to strong edges.</li>"
                    ],
                    "guid": "v//U00P#_^",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How does non-maximum suppression \"walk along the top of the ridge\"?<br><img src=\"paste-0612255933856fe8d61c2145c7f8fef0b1bbd289.jpg\">",
                        "It examines the gradient magnitude at neighboring pixels along the direction of the edge. If the current pixel has the highest magnitude among its neighbors in that direction, it is kept as an edge pixel; otherwise, it is suppressed."
                    ],
                    "guid": "B{b:~X^lq7",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the difference between hysteresis thresholding and uniform thresholding?",
                        "<li><strong>Hysteresis Thresholding:</strong> Uses <em>two</em> thresholds (upper and lower) to identify strong edges and weak edges connected to them.</li>\n<li><strong>Uniform Thresholding:</strong> Uses a <em>single</em> threshold. Any pixel with a gradient magnitude above the threshold is considered an edge.</li>"
                    ],
                    "guid": "I|6sq<ZZ<7",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why does lowering the threshold in uniform thresholding not always produce better results?",
                        "While it might capture more true edges, it also increases the likelihood of including noise and creating a more fragmented edge map."
                    ],
                    "guid": "lu(:apEXz9",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How does hysteresis thresholding address the limitations of uniform thresholding?",
                        "By using two thresholds, it can identify strong edges while also preserving weaker edges that are connected to them. This helps to create a more complete and accurate edge map."
                    ],
                    "guid": "M_x_|Opa-^",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "In which situations might uniform thresholding be sufficient?",
                        "<li>When the image has high contrast and well-defined edges.</li>\n<li>When computational efficiency is a primary concern.</li>\n<li>When a simple edge map is sufficient for the application.</li>"
                    ],
                    "guid": "hTp*[]wR<o",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Canny vs. Sobel - Noise Comparison",
                        "The Canny edge-detected image have less noise compared to the Sobel image."
                    ],
                    "guid": "yaM@1E;ug=",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Canny vs. Sobel - Detail Comparison",
                        "The lines in the Canny edge-detected image are thinner compared to the Sobel image."
                    ],
                    "guid": "LPMjwa!c[W",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is first-order edge detection?",
                        "<li>It uses the first derivative of the image intensity.</li>\n<li>It looks for areas where the intensity changes rapidly (high gradient magnitude).</li>\n<li>Often involves thresholding to identify edge pixels.</li>\n<li>Examples: Sobel, Prewitt, Canny (partially)</li><img src=\"paste-5ee45506f6195f2e80497ff87530bbbfa99001cb.jpg\"><br>"
                    ],
                    "guid": "Nd33DaV48O",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is second-order edge detection?",
                        "<li>It uses the second derivative of the image intensity.</li>\n<li>It looks for areas where the first derivative has a maximum or minimum (inflection points).</li>\n<li>Often involves finding zero-crossings in the second derivative.</li>\n<li>Example: Laplacian of Gaussian (LoG)</li><img src=\"paste-5ee45506f6195f2e80497ff87530bbbfa99001cb.jpg\"><br>"
                    ],
                    "guid": "f=0k[XzP_I",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the Laplacian operator?",
                        "A second-order derivative operator used in image processing to detect edges. It essentially measures the rate of change of the gradient in the image."
                    ],
                    "guid": "H_%Op>1oIc",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How is the Laplacian operator different from first-order edge detectors like Sobel?",
                        "<li><strong>Sobel:</strong>  Calculates the first derivative of the image intensity, looking for areas of rapid intensity change.</li>\n<li><strong>Laplacian:</strong> Calculates the second derivative, looking for places where the rate of intensity change has a maximum or minimum (inflection points).</li>"
                    ],
                    "guid": "bxh=lvsr[I",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the typical template used for the Laplacian operator?",
                        "<img src=\"paste-61f0b76d4d2fb083bfb98923d80d0e19a518ec2c.jpg\">"
                    ],
                    "guid": "HM);5U24/]",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How are edges typically detected using the Laplacian operator?",
                        "By finding the zero-crossings in the Laplacian-filtered image. A zero-crossing is where the output of the Laplacian changes sign (from positive to negative or vice versa)."
                    ],
                    "guid": "G{M8}N^DKa",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is a potential drawback of the Laplacian operator?",
                        "It can be very sensitive to noise in the image. This can lead to the detection of many false edges."
                    ],
                    "guid": "Mjt1B=[H?f",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How can the sensitivity to noise be reduced when using the Laplacian operator?",
                        "By first smoothing the image with a Gaussian filter. This combined approach is known as the Laplacian of Gaussian (LoG)."
                    ],
                    "guid": "elWd)s&>o:",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the fundamental concept behind edge detection?",
                        "Differentiation. Edge detection techniques essentially try to find areas in the image where the intensity changes rapidly, which corresponds to finding the derivatives (rate of change) of the image intensity."
                    ],
                    "guid": "JHcePF&};Q",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the role of a Gaussian function in edge detection?",
                        "It has a smoothing effect. By convolving an image with a Gaussian kernel, you reduce noise and smooth out the image, which can improve the accuracy of edge detection."
                    ],
                    "guid": "oWGtu:^i(_",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the formula for a 2D Gaussian function?",
                        "\\[g(x,y,\\sigma)=e^{\\frac{-(x^2+y^2)}{2\\sigma^2}}\\]where:<br>\\(x\\)&nbsp;and&nbsp;\\(y\\)&nbsp;are the spatial coordinates<br>\\(\\sigma\\)&nbsp;is the standard deviation, controlling the width of the Gaussian<br>"
                    ],
                    "guid": "E8~>D^sqy2",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why is it sometimes useful to add a constant&nbsp;\\(\\left(\\frac{1}{2\\pi\\sigma^2}\\right)\\)&nbsp;to the Gaussian function?",
                        "To normalize the Gaussian function so that the area under the curve integrates to 1. This can be helpful in certain applications."
                    ],
                    "guid": "o`&UPmw=!",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the Laplacian of Gaussian (LoG)?",
                        "It's a second-order edge detection method that combines the Laplacian operator with Gaussian smoothing. This helps to locate edges accurately while reducing noise."
                    ],
                    "guid": "je3r<W/Zyj",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the significance of the σ (sigma) parameter in the LoG?",
                        "It represents the standard deviation of the Gaussian function. It controls the amount of smoothing applied to the image before the Laplacian is calculated. A larger sigma results in more smoothing."
                    ],
                    "guid": "i2tT;q1e0,",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is another name for the Laplacian of Gaussian?",
                        "The Marr-Hildreth operator."
                    ],
                    "guid": "h*#p}gbvV@",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the shape of the Laplacian of Gaussian operator?",
                        "<img src=\"paste-ebf94c13ece6c952d30a462332a7f031c99a3009.jpg\"><br>It's called the \"Mexican hat operator\""
                    ],
                    "guid": "v=^A9b,a=#",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is a zero-crossing in the context of image processing?",
                        "A location in an image where the value of a function (like the second derivative of the image intensity) changes its sign, going from positive to negative or vice versa."
                    ],
                    "guid": "EK^bakGUWw",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why are zero-crossings important for edge detection?",
                        "Zero-crossings in the second derivative of the image intensity often correspond to the locations of edges. This is because edges represent points of rapid change in intensity."
                    ],
                    "guid": "dbK%@aqtXG",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How can you detect zero-crossings in a 2D image?",
                        "Examine the values of neighboring pixels. If there's a sign change among the neighbors, it indicates a potential zero-crossing."
                    ],
                    "guid": "QH/Ney4H0c",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why is zero-crossing detection often used with the Laplacian operator?",
                        "The Laplacian operator is a second-order derivative operator. Zero-crossings in the Laplacian-filtered image can accurately locate edges. This is the basis of the Laplacian of Gaussian (LoG) edge detector."
                    ],
                    "guid": "itGB<>ma0Y",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the effect of using a small template and small σ (sigma) in the LoG?",
                        "It detects fine, local edges in the image. This is because a small sigma results in less smoothing, preserving finer details."
                    ],
                    "guid": "y2~oA3.D}5",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the effect of using a large template and large σ in the LoG?",
                        "It detects coarser, more global edges in the image. A large sigma leads to more smoothing, blurring out finer details and highlighting larger-scale edges."
                    ],
                    "guid": "Ihy[Ge:AZ1",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How do you choose the appropriate template size and σ for the LoG?",
                        "<div>It depends on the specific application and the type of edges you want to detect.</div><ul>\n<li>For fine details: Use a small template and small sigma.</li>\n<li>For larger structures: Use a large template and large sigma.</li></ul>"
                    ],
                    "guid": "pBZug.}.cu",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is template matching?",
                        "A technique in image processing where you have a template (a small image) that you want to find within a larger image."
                    ],
                    "guid": "Ab6.?SvYr2",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How does template matching work?",
                        "You slide the template across the larger image and compare the template with the corresponding region in the image at each position."
                    ],
                    "guid": "EUj%uw&2xS",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the \"accumulator space\" in template matching?",
                        "It's a representation that shows how well the template matches the image at different positions. High values in the accumulator space indicate good matches."
                    ],
                    "guid": "D(wpdJEhVP",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What does the formula&nbsp;\\(|| I_Ω - T ||^2\\)&nbsp;represent?",
                        "It's a common way to measure the difference between the template (\\(T\\)) and the image region (\\(I_Ω\\)). It calculates the sum of squared differences between the pixel values."
                    ],
                    "guid": "r0:oJjT]SQ",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are some applications of template matching?",
                        "<li>Object detection (finding specific objects in images)</li>\n<li>Pattern recognition</li>\n<li>Image alignment</li>\n<li>Optical character recognition (OCR)</li>"
                    ],
                    "guid": "f#m]}DF8Y_",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "The formula for measuring the difference between the template and the image region.",
                        "\\[||I_\\Omega-T||^2=\\sum(I_\\Omega I_\\Omega-2I_\\Omega T+TT)\\]Where:<br>\\(T\\): Template<br>\\(I_\\Omega\\):&nbsp; Image Region&nbsp;<br>"
                    ],
                    "guid": "M~{Y0}d[p7",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "In the formula&nbsp;\\(||I_\\Omega-T||^2=\\sum(I_\\Omega I_\\Omega-2I_\\Omega T+TT)\\), <br>why is&nbsp;\\(\\sum TT\\)&nbsp;considered a fixed constant?",
                        "Because the template (\\(T\\)) doesn't change during the matching process. The sum of squared pixel values in the template remains the same."
                    ],
                    "guid": "L?^&cC<l}T",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "In the formula&nbsp;\\(||I_\\Omega-T||^2=\\sum(I_\\Omega I_\\Omega-2I_\\Omega T+TT)\\),<br>Why is&nbsp;\\(\\sum I_ΩI_Ω\\)&nbsp;considered approximately constant?",
                        "While the image region (\\(I_Ω\\)) changes as the template slides, the sum of squared pixel values within a small region often doesn't vary drastically."
                    ],
                    "guid": "BBDJ1e+j&C",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How is template matching related to correlation and convolution?",
                        "<li><strong>Correlation:</strong>  Template matching is essentially a normalized cross-correlation operation.</li>\n<li><strong>Convolution:</strong>  It's similar to convolution, but without flipping the template.</li>"
                    ],
                    "guid": "ea<TLsD5w&",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How can the Fourier transform be used for efficient template matching?",
                        "Both the image and the template can be transformed to the frequency domain using the Fourier transform. Correlation in the spatial domain is equivalent to multiplication in the frequency domain, which can be computationally faster for large images and templates."
                    ],
                    "guid": "n%{$:Ss75(",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What does the 3D plot in the \"accumulator space\" represent?<br><img src=\"paste-826f0b6c87499521146a6c468e3b82065fe33d62.jpg\">",
                        "It shows the similarity measure (e.g., correlation) between the template and the image at different positions. The peaks in the plot indicate the locations where the template matches the image well."
                    ],
                    "guid": "xt^#p3YE?w",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is convolution in image processing?",
                        "It's an operation where you slide a flipped kernel (a small matrix) across an image, performing a weighted sum of the pixel values in the neighborhood of each pixel. It's often used for blurring, sharpening, and edge detection."
                    ],
                    "guid": "CoaSs(bdmh",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is correlation in image processing?",
                        "It's similar to convolution, but the kernel is not flipped. It's an operation where you slide a kernel (a small matrix) across an image, performing a weighted sum of the pixel values in the neighborhood of each pixel.&nbsp;It's often used for template matching to find occurrences of a pattern within an image."
                    ],
                    "guid": "KRITDN^LMG",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the key difference between convolution and correlation?",
                        "Convolution involves flipping the kernel (rotating it 180 degrees) before sliding it across the image, while correlation does not."
                    ],
                    "guid": "yfdDU}SYbV",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How can the Fourier transform be used for template matching (correlation)?",
                        "You can flip the template, take the Fourier transforms of the image and the flipped template, multiply them in the frequency domain, and then take the inverse Fourier transform."
                    ],
                    "guid": "PbCQffb&x:",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the Hough Transform used for in image processing?",
                        "It's a feature extraction technique primarily used to identify lines (but can also be generalized for other shapes) within an image."
                    ],
                    "guid": "G8;idvIlsV",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How does the Hough Transform compare to template matching in terms of performance?",
                        "It generally achieves similar results to template matching in terms of finding lines, but it's often faster and more efficient."
                    ],
                    "guid": "NmQ^nz`2Op",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How does the Hough Transform represent lines?",
                        "It uses a parameter space (often called the \"accumulator space\") where each point represents a line.  A common way to do this is using the \"\\(m, c\\)\" parameterization (slope and y-intercept) of a line:&nbsp;\\(y = mx + c\\)"
                    ],
                    "guid": "H~~P:FD@D<",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the \"principle of duality\" in the context of the Hough Transform?",
                        "It refers to the idea that a point in the image space corresponds to a line in the parameter space, and vice versa. This duality allows you to detect lines by finding intersecting lines in the parameter space."
                    ],
                    "guid": "wMZkruLv?y",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How does the Hough Transform identify lines in the image?",
                        "By finding points in the accumulator space where many lines intersect. These intersection points correspond to the parameters (\\(m, c\\)) of the lines detected in the image."
                    ],
                    "guid": "d0jX^60Dv=",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Pseudocode for Hough Transform",
                        "<img src=\"paste-8291002046e9c9f2636960b90dec8a73b7ddd3de.jpg\">"
                    ],
                    "guid": "ss_M))P1)/",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is a potential problem with using the standard Hough Transform (with&nbsp;\\(y = mx + c\\)) to represent lines?",
                        "The slope (\\(m\\)) and y-intercept (\\(c\\)) can tend to infinity for vertical lines. This can cause issues in the parameter space representation and computation."
                    ],
                    "guid": "v`C/lER[3b",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How can the problem with using the standard Hough Transform (with&nbsp;\\(y = mx + c\\)) to represent lines&nbsp;be addressed?",
                        "By changing the parameterization of the line. Instead of using slope-intercept form, we can use the \"foot of normal\" representation."
                    ],
                    "guid": "JfUMqJ1^$P",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the \"foot of normal\" representation of a line?",
                        "<div>&nbsp;It represents a line using the following parameters:</div><ul>\n<li>\\(r\\): The perpendicular distance from the origin to the line.</li>\n<li>\\(θ\\): The angle between the x-axis and the line connecting the origin to the closest point on the line.</li></ul>"
                    ],
                    "guid": "n_%c>#aANx",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the equation for the \"foot of normal\" representation?",
                        "\\[r = x\\cos θ + y\\sin θ\\]where:<br>\\(r\\)&nbsp;: The length of the line segment perpendicular to the line and passing through the origin.<br><code>\\(θ\\)&nbsp;</code>: The angle between the x-axis and this line segment.<br>"
                    ],
                    "guid": "v=5l0}VA8|",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the Hough Transform called when using this \"foot of normal\" representation?",
                        "The Polar Hough Transform."
                    ],
                    "guid": "i8C?Wl7jzD",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "<div>What are the advantages of using the Polar Hough Transform?<div></div></div><div></div>",
                        "<li>It avoids the problem of infinite values for vertical lines.</li>\n<li>It provides a bounded parameter space, making computation and representation easier.</li>"
                    ],
                    "guid": "bMIR.WtlU_",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                }
            ],
            "reviewLimit": null,
            "reviewLimitToday": null
        },
        {
            "__type__": "Deck",
            "children": [],
            "crowdanki_uuid": "e04c94ee-d758-11ef-8d54-204ef6f14df6",
            "deck_config_uuid": "0aa25128-d1e9-11ef-92a1-204ef6f14df6",
            "desc": "",
            "dyn": 0,
            "extendNew": 0,
            "extendRev": 0,
            "media_files": [
                "Linear_RGB_color_wheel.png",
                "paste-13789c28b886e47febaa00428020acd82f2cc2bd.jpg",
                "paste.jpg"
            ],
            "name": "Week 4",
            "newLimit": null,
            "newLimitToday": null,
            "notes": [
                {
                    "__type__": "Note",
                    "fields": [
                        "<div>What is the equation of a circle?</div><div><br></div>",
                        "<div>\\[(x - x_0)^2 + (y - y_0)^2 = r^2\\]<br></div><div>where:</div><ul><li>\\((x, y)\\)&nbsp;are the coordinates of a point on the circle.</li><li>\\((x_0, y_0)\\)&nbsp;are the coordinates of the center of the circle.</li><li>\\(r\\)&nbsp;is the radius of the circle.</li></ul>"
                    ],
                    "guid": "dl!Ur*J-aV",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How does the Hough Transform for circles work?",
                        "It uses a 3D parameter space (accumulator space) with parameters&nbsp;\\(x_0,y_0\\)&nbsp;(center coordinates), and&nbsp;\\(r\\)&nbsp;(radius). Each point in this space represents a possible circle."
                    ],
                    "guid": "Hq+P?-+8q~",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the \"principle of duality\" in the context of the Hough Transform for circles?",
                        "A point in the image space corresponds to a circle in the parameter space, and a point in the parameter space corresponds to a circle in the image space."
                    ],
                    "guid": "jKz}!-rUn7",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How does the Hough Transform identify circles in the image?",
                        "By finding peaks in the 3D accumulator space. These peaks correspond to the parameters (\\(x_0, y_0, r\\)) of the circles detected in the image."
                    ],
                    "guid": "n~ogK9,+%:",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How does \"circle voting\" work in the Hough Transform?",
                        "For each edge point in the image space, the Hough Transform casts \"votes\" in the accumulator space for all possible circles that could pass through that point. These votes accumulate in the rings."
                    ],
                    "guid": "c|?vu:Xs7a",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How are circles detected in the accumulator space?",
                        "By identifying points where many rings intersect. These intersection points correspond to the parameters (\\(x_0,y_0,r\\)) of circles in the image. The more rings intersect at a point, the stronger the evidence for a circle with those parameters."
                    ],
                    "guid": "Pa5`GHb(l5",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Pseudocode for circle voting",
                        "<img src=\"paste-13789c28b886e47febaa00428020acd82f2cc2bd.jpg\">"
                    ],
                    "guid": "w/mrGLgynv",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are conic sections?",
                        "Curves formed by intersecting a cone with a plane. Examples include ellipses, parabolas, and hyperbolas."
                    ],
                    "guid": "w)hCl=)LDR",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the general equation of an ellipse?",
                        "<div>\\[(x - x_0)^2 / a^2 + (y - y_0)^2 / b^2 = 1\\]<br></div><div>where:</div><ul>\n<li><code>(\\(x, y\\))</code> are coordinates of a point on the ellipse.</li>\n<li><code>(\\(x_0, y_0\\))</code> are coordinates of the center of the ellipse.</li>\n<li>\\(a\\)&nbsp;is the semi-major axis (half of the longer axis).</li>\n<li>\\(b\\)&nbsp;is the semi-minor axis (half of the shorter axis).</li></ul>"
                    ],
                    "guid": "Hxb?f|`;aR",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why does detecting ellipses with the Hough Transform become computationally expensive?",
                        "Because the parameter space becomes very large. If each parameter has 100 possible values, the accumulator space would require 100 x 100 x 100 x 100 = 10⁸ entries, which is 0.1 GB. Adding rotation as a parameter would further increase this to 10 GB."
                    ],
                    "guid": "HnG>SUx$$b",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the consequence of this high dimensionality in the Hough Transform for ellipses?",
                        "<div>&nbsp;It motivates the need for approaches to:</div><ul>\n<li><strong>Save memory:</strong> Reduce the size of the accumulator space or find more efficient ways to represent it.</li>\n<li><strong>Improve speed:</strong> Optimize the algorithm to reduce computation time.</li></ul>"
                    ],
                    "guid": "eND~!hidtC",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why is it still desirable to use the Hough Transform for ellipses, despite the computational cost?",
                        "Because the Hough Transform provides an optimal result. It guarantees finding the best-fit ellipse (or other shape) given the data."
                    ],
                    "guid": "Ke0x]t6l^0",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "<div>What is the main challenge with the standard Hough Transform for circles, how can it be sped up and how does it speed it up?<div></div></div><div></div>",
                        "The 3D accumulator space (\\(x_0,y_0,r\\)) can be computationally expensive, especially for large images. It can be sped up by using the gradient information (edge direction) to reduce the dimensionality of the accumulator space from 3D to 2D. This speeds up the hough transform by reducing the dimensionality of the accumulator space, it reduces the memory required and the number of computations needed to find circles."
                    ],
                    "guid": "r(}gZyFD*T",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How is the 2D accumulator space constructed for the hough transform?",
                        "By first getting the gradient information by&nbsp;differentiating the equation of the circle with respect to&nbsp;\\(x\\). This gives you:&nbsp;\\(dy/dx = -(x - x_0) / (y - y_0)\\)&nbsp;where&nbsp;\\(dy/dx\\)&nbsp;represents the slope of the tangent line at a point on the circle, which is the edge direction.<br><br>Then, by using the gradient information (\\(dy/dx\\)) and the original circle equation, you can derive the equation:&nbsp;\\(y - y_0 = \\frac{r}{\\sqrt{1 + \\left(\\frac{dy}{dx}\\right)^2}}\\).<br><br>This equation only has two unknowns (\\(y_0\\)&nbsp;and&nbsp;\\(r\\)), effectively reducing the parameter space to 2D."
                    ],
                    "guid": "b*a`DgveaY",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the Generalized Hough Transform?",
                        "An extension of the Hough Transform that can detect arbitrary shapes (any shape, not just lines or circles) in an image."
                    ],
                    "guid": "w<r(7<#=8H",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is an R-table (or look-up table) in the context of the Generalized Hough Transform?",
                        "A table that stores information about the shape to be detected. It typically contains the distance and angle from the shape's centroid to its boundary points."
                    ],
                    "guid": "Ew_3.=*dwv",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How is the R-table constructed?",
                        "<ol><li>Choose a reference point within the shape (usually the centroid).</li><li>For each boundary point of the shape, calculate:</li><ol><li>The distance (\\(r\\)) from the reference point to the boundary point.</li><li>The angle (\\(θ\\)) between a reference line (e.g., horizontal line) and the line connecting the reference point to the boundary point.</li></ol><li>Store these <code>(\\(r, θ\\))</code> pairs in the R-table, often discretized into bins.</li></ol>"
                    ],
                    "guid": "ob`v&bvQ?K",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How is the R-table used in the Generalized Hough Transform?",
                        "<ol><li>For each edge point in the image, access the R-table.</li><li>\nFor each <code>(\\(r, θ\\))</code> entry in the table, calculate the potential location of the shape's reference point.</li><li>\nCast votes in the accumulator space for these potential reference point locations.</li></ol>"
                    ],
                    "guid": "DJlJ&1%>A]",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are the advantages of using the Generalized Hough Transform?",
                        "<li>It can detect complex shapes that are not easily described by simple equations.</li>\n<li>It's relatively robust to noise and occlusion.</li>"
                    ],
                    "guid": "yutCsuf)uA",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are some challenges of the Generalized Hough Transform?",
                        "<li>It can be computationally expensive, especially for complex shapes.</li>\n<li>The accuracy depends on the quality of the edge detection and the choice of reference point.</li>"
                    ],
                    "guid": "d^lXG2E69F",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why is the edge direction alone not a unique description for constructing the R-table?",
                        "Because multiple points on the shape's boundary might have the same edge direction. This can lead to noise in the accumulator space. Using both&nbsp;\\(r\\)&nbsp;and&nbsp;\\(α\\)&nbsp;provides a more unique description."
                    ],
                    "guid": "p@+vfjQIj|",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What information is stored in the R-table?",
                        "<div>For each boundary point of the shape, it stores:</div><ul>\n<li>The distance (\\(r\\)) from a reference point (usually the centroid) to the boundary point.</li>\n<li>The angle (\\(\\alpha\\))&nbsp;between a reference line and the line connecting the reference point to the boundary point.</li></ul>"
                    ],
                    "guid": "JU#Bo&vT~y",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the purpose of the R-table in the Generalized Hough Transform?",
                        "To store information about the shape you want to detect. This information is used to cast votes in the accumulator space for potential locations of the shape."
                    ],
                    "guid": "he#&M5Wtaq",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are three desirable qualities in a CV system?",
                        "<ul><li><div><strong>Robust:</strong>&nbsp;The system performs well even with variations in input or noise.</div></li><li><div><strong>Invariant:</strong>&nbsp;The system's output remains the same despite changes in the input that shouldn't affect the result (e.g., lighting, orientation).</div></li><li><div><strong>Repeatable:</strong>&nbsp;The system produces the same output for the same input consistently.</div></li></ul>"
                    ],
                    "guid": "hR$n`+@C(`",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What does it mean for a CV system to be&nbsp;<b>robust</b>?",
                        "A robust CV system is one that can handle variations in input data, noise, and unexpected situations without failing or producing significantly inaccurate results. It can generalize well to unseen data."
                    ],
                    "guid": "c=JuML#_|K",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What does it mean for a CV system to be&nbsp;<strong>invariant</strong>?",
                        "An invariant CV system produces the same output even when the input is transformed in ways that shouldn't affect the result. Examples include changes in lighting, scale, rotation, or viewpoint."
                    ],
                    "guid": "IF^`5|{`tt",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What does it mean for a CV system to be&nbsp;<strong>repeatable</strong>?",
                        "A repeatable CV system consistently produces the same output when given the same input. This ensures reliability and predictability. Its a measure of&nbsp;<i>robustness</i>."
                    ],
                    "guid": "z!/$1N/sIu",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are&nbsp;<strong>constraints</strong>&nbsp;in the context of CV system design?",
                        "Constraints are limitations or restrictions that are applied to the CV system to make it work effectively within a specific context. They are often used to simplify the problem, reduce computational cost, or improve accuracy by limiting the range of possible inputs or outputs."
                    ],
                    "guid": "O3tYr7F8&V",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "In designing CV systems, what is the relationship between&nbsp;<strong>robustness</strong>,&nbsp;<strong>invariance</strong>,&nbsp;<strong>repeatability</strong>, and&nbsp;<strong>constraints</strong>?",
                        "<ul><li><div><strong>Robustness</strong>,&nbsp;<strong>invariance</strong>, and&nbsp;<strong>repeatability</strong>&nbsp;are qualities you&nbsp;<span style=\"font-style: italic;\">want</span>&nbsp;in your CV system.</div></li><li><div><strong>Constraints</strong>&nbsp;are what you&nbsp;<span style=\"font-style: italic;\">apply</span>&nbsp;to the system to help achieve these qualities and make the system work effectively.</div></li><li><div><strong>Invariance</strong>&nbsp;is what you design your system to be.</div></li></ul>"
                    ],
                    "guid": "k6Fu&S`h7<",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why is robustness important for vision systems?",
                        "Because real-world conditions are often unpredictable and can change. A robust system can handle these variations and still produce reliable results."
                    ],
                    "guid": "zN81-Zw}xF",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "In the context of software constraints, what is the desired type of algorithm?",
                        "The goal is to use&nbsp;<strong>simple</strong>&nbsp;but&nbsp;<strong>fast</strong>&nbsp;algorithms."
                    ],
                    "guid": "Nx9nKxy),P",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the RGB color space?",
                        "A way to represent colors using three components: Red, Green, and Blue. Each component represents the intensity of the corresponding color."
                    ],
                    "guid": "H}^G&:7`Vy",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How do most physical image sensors capture color?",
                        "They capture color in the RGB format using sensors that are sensitive to red, green, and blue light."
                    ],
                    "guid": "vp*}@`&0)8",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What does it mean that RGB \"couples\" brightness (luminance) with each channel?",
                        "The values of the red, green, and blue components are not independent of the brightness of the color. This means that changes in illumination can affect all three channels."
                    ],
                    "guid": "Lr.P/)7oYw",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why does coupling brightness with each channel make illumination invariance difficult?",
                        "Because changes in lighting conditions can change the values of all three RGB components, making it harder to recognize colors consistently under different lighting."
                    ],
                    "guid": "Q2DKy[Q^~8",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What does HSV stand for?",
                        "Hue, Saturation, Value"
                    ],
                    "guid": "Q~cQ.!=b&A",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the HSV color space?",
                        "An alternative representation of colors, often used in computer graphics and image processing, that separates color information into three components: hue, saturation, and value."
                    ],
                    "guid": "FGZr`DrUx^",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What does \"hue\" represent in the HSV color space?",
                        "It encodes the pure color, represented as an angle from 0 to 360 degrees. For example, red is at 0 degrees (and also 360 degrees).<br><img alt=\"Color wheel - Wikipedia\" src=\"Linear_RGB_color_wheel.png\">"
                    ],
                    "guid": "M!YXC[x!Oe",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What does \"saturation\" represent in the HSV color space?",
                        "It describes the intensity or purity of the color. A high saturation means a vivid color, while a low saturation means a more muted or grayer color.<br><img alt=\"HSV · Hyperskill\" src=\"paste.jpg\">"
                    ],
                    "guid": "g2GpzX4P{;",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What does \"value\" represent in the HSV color space?",
                        "It encodes the brightness of the color. A high value means a bright color, while a low value means a darker color.<br><img alt=\"HSV · Hyperskill\" src=\"paste.jpg\">"
                    ],
                    "guid": "yp&.o6QC`+",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How can the HSV color space help achieve invariance to lighting?",
                        "By using just the hue (H) component, or the hue and saturation (H &amp; S) components, you can make color recognition less sensitive to changes in brightness. This is because the value (V) component, which encodes brightness, is separated from the color information."
                    ],
                    "guid": "y;cb[G?z56",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are some types of hardware constraints that can affect computer vision systems?",
                        "<li>Lenses: Different lenses have different properties (focal length, sharpness, aperture) that can affect the image quality.</li>\n<li>Sensors: Different sensors have different sensitivities and capabilities (e.g., infrared sensors).</li>\n<li>Filters: Different filters can be used to modify the light reaching the sensor (e.g., polarizing filters, color filters).</li>"
                    ],
                    "guid": "M>B(=|T[_C",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are the main components of many computer vision applications that involve machine learning?",
                        "<li>Image input</li>\n<li>Feature Extractor</li>\n<li>Machine Learning model</li>\n<li>Output/Result</li>"
                    ],
                    "guid": "MEqD]DmA9:",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is a feature vector?",
                        "A mathematical vector that represents an object or pattern in terms of its features. It's essentially a list of numbers where each number represents a specific feature."
                    ],
                    "guid": "qbJ6kVaK>Q",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Can a feature vector have a variable number of elements?",
                        "<strong></strong>No, a feature vector has a fixed number of elements.&nbsp;The number of elements is the dimensionality of the vector"
                    ],
                    "guid": "sX-otULd-O",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the dimensionality of a feature vector?",
                        "The number of elements in the vector."
                    ],
                    "guid": "yAH!H`b)H;",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What does a feature vector represent in a feature space?",
                        "It represents a point or a direction in the feature space."
                    ],
                    "guid": "Fv#)(flCq`",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Can vectors with different dimensionalities exist in the same feature space?",
                        "No, vectors of differing dimensionality cannot exist in the same feature space. They belong to different feature spaces."
                    ],
                    "guid": "y@^Er%__Hs",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are some simple features that can be extracted from an image to create a feature vector?",
                        "<li>Dimensions of the image (height, width, number of color channels)</li>\n<li>Color mean (average color of the image)</li>\n<li>Mean and standard deviation of color values</li>\n<li>Color histogram (distribution of color intensities)</li>"
                    ],
                    "guid": "J^kSE&OG%y",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is a color histogram?",
                        "A representation of the distribution of color intensities in an image. It shows how many pixels have each possible color value."
                    ],
                    "guid": "|s4RM;f>F",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How can a color histogram be represented as a feature vector?",
                        "<li>As a 3 x 256 array, where each row represents a color channel (R, G, B) and each column represents a color intensity level.</li>\n<li>As a 256³ vector, where each element represents the number of pixels with a specific combination of red, green, and blue intensity levels.</li>"
                    ],
                    "guid": "bf?_SMgb}*",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are some common distance metrics used in feature space?",
                        "<li>Euclidean distance</li>\n<li>Manhattan distance</li>\n<li>Cosine similarity</li>"
                    ],
                    "guid": "e!+|_b]uJ<",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How can you determine the similarity between two images in feature space?",
                        "By measuring the distance between their corresponding feature vectors. Smaller distances indicate higher similarity."
                    ],
                    "guid": "JmA}tCq:I5",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is a feature space?",
                        "A multi-dimensional space where each dimension represents a feature extracted from an image or object.  Images are represented as points (feature vectors) in this space."
                    ],
                    "guid": "w?}o{U>R6m",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the goal of feature extraction in computer vision?",
                        "To transform raw image data into a set of meaningful features that can be used for tasks like classification or object recognition."
                    ],
                    "guid": "MUFq`AMF1Z",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How are feature extractors designed to handle similar images?",
                        "They are often designed to produce feature vectors that are close together in the feature space for similar images."
                    ],
                    "guid": "Ec3r=&ip(k",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is Euclidean distance?",
                        "It's the most intuitive way to measure the distance between two points. It's also known as L2 distance. It can be visualised as&nbsp;the straight-line distance between two points in space."
                    ],
                    "guid": "r)MDd6ZQfy",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the formula for Euclidean distance?",
                        "\\[D_2(p,q)=\\sqrt{\\sum^n_{i=1}(p_i-q_i)^2}\\]where,<br>\\(p\\)&nbsp;and&nbsp;\\(q\\)&nbsp;are the two points (or vectors)<br>\\(p_i\\)&nbsp;and&nbsp;\\(q_i\\)&nbsp;are coordinates of the points in the i-th dimension<br>\\(n\\)&nbsp;is the number of dimensions<br>"
                    ],
                    "guid": "H}|3:8_gy~",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are some other ways to represent Euclidean distance?",
                        "<li>\\(||p - q||\\)&nbsp;(the magnitude of the difference between the vectors)</li>\n<li>\\(\\sqrt{((p - q) ⋅ (p - q))}\\)&nbsp;(the square root of the dot product of the difference vector with itself)</li>"
                    ],
                    "guid": "v~O9lmceAK",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "In the context of feature vectors, what does Euclidean distance measure?",
                        "The similarity between two feature vectors. Smaller Euclidean distance means higher similarity."
                    ],
                    "guid": "Cv=)Q0oe>-",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is cosine similarity?",
                        "A measure of similarity between two non-zero vectors. It calculates the cosine of the angle between the vectors."
                    ],
                    "guid": "cK6O4Z6J=h",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Is cosine similarity a distance metric?",
                        "No, it's not a distance metric. It measures the angle between vectors, not the distance between them."
                    ],
                    "guid": "EhPBZ!W,1i",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the formula for cosine similarity?",
                        "\\[cos(θ) = \\frac{p ⋅ q}{||p||\\;||q||}\\]where:<br>\\(p\\)&nbsp;and&nbsp;\\(q\\)&nbsp;are the two vectors.<br>\\(p ⋅ q\\)&nbsp;is the dot product of the vectors.<br>\\(||p||\\)&nbsp;and&nbsp;\\(||q||\\)&nbsp;are the magnitudes (lengths) of the vectors.<br>"
                    ],
                    "guid": "M}>NAcIWB~",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "When is cosine similarity particularly useful?",
                        "When you don't care about the magnitudes (lengths) of the vectors, but only about their orientation or direction."
                    ],
                    "guid": ".Mbp/_772",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the main goal when choosing features for computer vision?",
                        "To select features that allow you to distinguish between objects or classes of interest."
                    ],
                    "guid": "dMKZ*C:I1J",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why is it generally recommended to keep the number of features small?",
                        "Because machine learning algorithms can become more complex and less effective as the dimensionality of the feature space increases. This is often referred to as the \"curse of dimensionality.\""
                    ],
                    "guid": "OIu#H<ypFL",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the \"curse of dimensionality\"?",
                        "A phenomenon in machine learning where the performance of algorithms can degrade as the number of features (dimensions) increases. This is because the data becomes more sparse and harder to model in high-dimensional spaces."
                    ],
                    "guid": "n>kXo${tvj",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is classification in machine learning?",
                        "The process of assigning a class label to an object or input. This involves categorizing the input into one of several predefined classes."
                    ],
                    "guid": "rjTnG%mpPB",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is supervised machine learning?",
                        "A type of machine learning where the algorithm learns from labeled data. This means the training data includes input examples along with their correct output labels."
                    ],
                    "guid": "zHyMstj|^#",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How does a supervised machine learning algorithm learn to classify?",
                        "It uses a set of pre-labeled training data to learn the relationship between the input features and the corresponding class labels."
                    ],
                    "guid": "p([N5:-$wt",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is a binary classifier?",
                        "A classifier that assigns objects to one of two possible classes"
                    ],
                    "guid": "AT!SG1$F;P",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is a multiclass classifier?",
                        "A classifier that assigns objects to one of many possible classes"
                    ],
                    "guid": "hq{]IAvH80",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the main goal of a linear classifier?",
                        "The main goal of a linear classifier is to learn a&nbsp;<strong>hyperplane</strong>&nbsp;that separates two classes in feature space with&nbsp;<strong>minimum error</strong>."
                    ],
                    "guid": "myaSTV<#`L",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is a&nbsp;<strong>hyperplane</strong>&nbsp;in the context of linear classifiers?",
                        "A hyperplane is a decision boundary that separates data points of different classes. In a 2D feature space, it's a line; in a 3D feature space, it's a plane; and in higher dimensions, it's a hyperplane."
                    ],
                    "guid": "xc3sNok%(j",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How do you use a learned hyperplane to classify a new image?",
                        "To classify a new image, you determine which side of the hyperplane the image's feature vector falls on. This tells you which class the image likely belongs to."
                    ],
                    "guid": "C`T{(bwUc@",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Is there only one possible hyperplane that can separate two classes?",
                        "No, there can be many possible hyperplanes that separate two classes. Different linear classification algorithms apply different constraints when learning the classifier, leading to different hyperplanes."
                    ],
                    "guid": "gd?~diR_tn",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How do different linear classification algorithms choose a specific hyperplane if there are multiple possibilities?",
                        "Different linear classification algorithms apply different&nbsp;<strong>constraints</strong>&nbsp;or criteria when learning the hyperplane. These constraints help determine which hyperplane is considered \"best\" based on factors like maximizing the margin (distance between the hyperplane and the closest data points) or minimizing misclassification errors."
                    ],
                    "guid": "l&&A%wup!S",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "When do linear classifiers work best?",
                        "Linear classifiers work best when the data is&nbsp;<strong>linearly separable</strong>&nbsp;(meaning the classes can be separated by a straight line or hyperplane)."
                    ],
                    "guid": "I[Q/~,J1Lb",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are&nbsp;<strong>outliers</strong>&nbsp;in the context of classification?",
                        "Outliers are data points that do not fit the general pattern of their class and may be misclassified."
                    ],
                    "guid": "PZ?|zc9g{M",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is a&nbsp;<strong>false positive</strong>&nbsp;in binary classification?",
                        "&nbsp;A false positive is when a data point is classified as belonging to the positive class, but it actually belongs to the negative class."
                    ],
                    "guid": "z?k#ke[#xO",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is a&nbsp;<strong>false negative</strong>&nbsp;in binary classification?",
                        "A false negative is when a data point is classified as belonging to the negative class, but it actually belongs to the positive class."
                    ],
                    "guid": "yC?N,@E/(L",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What do non-linear binary classifiers, such as Kernel Support Vector Machines (KSVMs), learn?",
                        "Non-linear binary classifiers, such as Kernel Support Vector Machines, learn&nbsp;<strong>non-linear decision boundaries</strong>&nbsp;to separate classes."
                    ],
                    "guid": "E#azpKDU;o",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is a potential drawback of using non-linear classifiers",
                        "A potential drawback of using non-linear classifiers is&nbsp;<strong>overfitting</strong>."
                    ],
                    "guid": "L7*|v5}+^V",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What does&nbsp;<strong>overfitting</strong>&nbsp;mean in the context of machine learning?",
                        "Overfitting means that the classifier has learned the training data too well, including the noise and outliers. As a result, it may not generalize well to new, unseen data. It may lose generality."
                    ],
                    "guid": "POi(LUxSW*",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How might the decision boundary learned by a non-linear classifier, like a Kernel Support Vector Machine, differ from that of a linear classifier when applied to the same dataset?",
                        "A non-linear classifier, like a KSVM, would learn a curved or more complex decision boundary that adapts to the non-linear distribution of the data, as opposed to the straight-line boundary learned by a linear classifier."
                    ],
                    "guid": "O|Ibq1><]e",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is clustering?",
                        "A technique for grouping data points into clusters based on their similarity.  The goal is to put similar data points in the same cluster and dissimilar data points in different clusters"
                    ],
                    "guid": "c`/FQLOgAu",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the key difference between clustering and classification?",
                        "Clustering is an <em>unsupervised</em> learning technique, meaning it doesn't require labeled data. Classification, on the other hand, is a <em>supervised</em> learning technique that requires labeled data."
                    ],
                    "guid": "J%<[tJFR*h",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How does clustering work with feature vectors?",
                        "Items with similar feature vectors (i.e., vectors that are close together in the feature space) should be grouped together in the same cluster."
                    ],
                    "guid": "EM}_lA<tHo",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are disjoint clustering methods?",
                        "Clustering methods that assign each data point to only one cluster. This results in non-overlapping clusters."
                    ],
                    "guid": "iob}GZrjio",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Are there clustering methods that create overlapping clusters?",
                        "Yes, some clustering methods allow data points to belong to multiple clusters. These are called overlapping or fuzzy clustering methods."
                    ],
                    "guid": "wujgDjFKG%",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is unsupervised machine learning?",
                        "A type of machine learning where the algorithm learns from unlabeled data. This means the training data consists only of input examples without any specified output labels or categories. The algorithm must discover patterns, structures, or relationships in the data on its own."
                    ],
                    "guid": "o]OlOgoA/7",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is K-Means clustering?",
                        "A popular clustering algorithm that groups data points into K clusters, where each cluster is represented by its center point (centroid)."
                    ],
                    "guid": "t0y~]._@}-",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How do you determine the value of K in K-Means?",
                        "The value of K (the number of clusters) is a hyperparameter that needs to be chosen beforehand. There are various methods to help determine a suitable value for K, such as the elbow method or silhouette analysis."
                    ],
                    "guid": "o(*HvA-6V]",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How does the K-Means algorithm start?",
                        "It starts by randomly choosing K initial cluster centers (centroids)."
                    ],
                    "guid": "dw9Nr=eiwc",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are the iterative steps in the K-Means algorithm?",
                        "<li><strong>Assignment:</strong> Each data point is assigned to its closest centroid.</li>\n<li><strong>Update:</strong> The centroid of each cluster is recomputed as the mean of all the points assigned to it.</li>"
                    ],
                    "guid": "OD`9GzQ}Ix",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "When does the K-Means algorithm stop iterating?",
                        "When the centroids no longer move significantly between iterations, or a maximum number of iterations is reached."
                    ],
                    "guid": "d#?)O:h/YW",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How are the final clusters formed in K-Means?",
                        "By assigning all points to their nearest centroid after the algorithm has converged."
                    ],
                    "guid": "r[{kkR`[*H",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What happens if a centroid has no points assigned to it during the iteration?",
                        "It is randomly re-initialized to a new point to prevent it from getting stuck."
                    ],
                    "guid": "p%q|s2PyKd",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                }
            ],
            "reviewLimit": null,
            "reviewLimitToday": null
        },
        {
            "__type__": "Deck",
            "children": [],
            "crowdanki_uuid": "e04d7f4a-d758-11ef-bb49-204ef6f14df6",
            "deck_config_uuid": "0aa25128-d1e9-11ef-92a1-204ef6f14df6",
            "desc": "",
            "dyn": 0,
            "extendNew": 0,
            "extendRev": 0,
            "media_files": [
                "paste-0e3f8835dd50dabeedc45ace2a586815c7d9f632.jpg",
                "paste-eda7c6ec23b44a8998c17fcb5502bfc8850fd0ce.jpg"
            ],
            "name": "Week 5",
            "newLimit": null,
            "newLimitToday": null,
            "notes": [
                {
                    "__type__": "Note",
                    "fields": [
                        "What is variance?",
                        "Variance is a measure of how spread out a set of data is. A high variance indicates that the data points are far from the mean, while a low variance indicates that the data points are clustered closely around the mean."
                    ],
                    "guid": "I~`c<9?-MA",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the formula for variance (\\(σ^2\\))?",
                        "<div>\\[σ^2(x) = \\frac{1}{n}\\sum^n_{i=1}(x_i - μ)^2\\]where:</div>\\(x_i\\)&nbsp;<span style=\"white-space: pre-wrap;\">represents each individual data point.<br></span>\\(\\mu\\)&nbsp;<span style=\"white-space: pre-wrap;\">is the mean (average) of the data set.<br></span>\\(n\\)&nbsp;<span style=\"white-space: pre-wrap;\">is the number of data points in the set.</span>"
                    ],
                    "guid": "EN41{tx(/_",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is covariance?",
                        "<div>Covariance measures how two variables change together. It indicates whether the variables tend to increase or decrease together, or if they change independently of each other.&nbsp;&nbsp;<div><div><div><div></div></div></div></div></div><div></div>"
                    ],
                    "guid": "B>!AgO$I)g",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the formula for covariance (\\(σ(x, y)\\))?",
                        "\\[σ(x, y) = \\frac{1}{n}\\sum^n_{i=1}(x - μ_x)(y - μ_y)\\]where:<br>\\(x\\)&nbsp;and&nbsp;\\(y\\)&nbsp;are individual data points<br>\\(\\mu_x\\)&nbsp;is the mean of x<br>\\(\\mu_y\\)&nbsp;is the mean of y<br>\\(n\\)&nbsp;is the number of data points<br>"
                    ],
                    "guid": "omtS/q&DFB",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How are variance and covariance related?",
                        "Variance is a special case of covariance. It's what you get when you calculate the covariance of a variable with itself.<br><br>\\(σ(x, x) = σ^2(x)\\)<br><br>This means that the variance of a variable&nbsp;\\(x\\)&nbsp;is the same as the covariance between&nbsp;\\(x\\)&nbsp;and&nbsp;\\(x\\)."
                    ],
                    "guid": "IX:ziqmsu{",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What does a covariance of 0 indicate about two variables?",
                        "<li>The variables are <strong>uncorrelated</strong>. This means there's no linear relationship between them.</li>\n<li>It often suggests the variables are <strong>independent</strong>, meaning changes in one variable don't affect the other. (However, it's important to note that uncorrelated variables are not <em>always</em> independent.)</li><li>Covariance is related to correlation (<code>ρ(x, y)</code>) by the following formula:</li>\\[ρ(x, y) = \\frac{σ(x, y)}{\\sqrt{\\sigma^2(x)}\\sqrt{\\sigma^2(y)}}\\]<br>"
                    ],
                    "guid": "uK!:`#>=?=",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "<strong></strong>What is a covariance matrix?",
                        "A square matrix that shows the covariance between all possible pairs of variables in a multivariate dataset."
                    ],
                    "guid": "ue5r.EKR>I",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What does the entry in the i-th row and j-th column of a covariance matrix represent?",
                        "The covariance between the i-th and j-th variables in the dataset."
                    ],
                    "guid": "xD[>#idF>>",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What does it mean if the covariance between two variables is positive? Negative? Zero?",
                        "<li><strong>Positive:</strong> The variables tend to increase or decrease together.</li>\n<li><strong>Negative:</strong> One variable tends to increase when the other decreases.</li>\n<li><strong>Zero:</strong> The variables are uncorrelated (no linear relationship).</li>"
                    ],
                    "guid": "C&~.{@Ct))",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are the diagonal elements of a covariance matrix?",
                        "The variances of the individual variables. This is because the covariance of a variable with itself is its variance."
                    ],
                    "guid": "ugk9ka2iV6",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Is a covariance matrix symmetric?",
                        "Yes, a covariance matrix is always symmetric. This is because the covariance between variable X and Y is the same as the covariance between variable Y and X."
                    ],
                    "guid": "C]C$n=jqD)",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How is a covariance matrix useful in computer vision?",
                        "<div>&nbsp;It can be used for tasks like:</div><ul>\n<li>Dimensionality reduction (e.g., Principal Component Analysis)</li>\n<li>Feature extraction</li>\n<li>Classification</li>\n<li>Clustering</li></ul>"
                    ],
                    "guid": "f.jN~E=,(u",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is mean centering?",
                        "A data preprocessing technique where you subtract the mean of each feature (dimension) from each data point."
                    ],
                    "guid": "dbM>qG9>rg",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How do you perform mean centering?",
                        "<ol><li>Calculate the mean of each feature (dimension) across all data points.</li><li>Subtract the corresponding mean from each feature value in each data point.</li></ol>"
                    ],
                    "guid": "L^MXly.t*N",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the effect of mean centering on the data?",
                        "It shifts the data so that the mean of each feature becomes zero.  Geometrically, it translates all the data points so that their average position is at the origin."
                    ],
                    "guid": "lWmfRAH>w`",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "<div>Why is mean centering useful?<div></div></div><div></div>",
                        "<li>It can help remove bias from the data.</li>\n<li>It can make some machine learning algorithms converge faster.</li>\n<li>It can be helpful for certain types of data analysis and visualization.</li>"
                    ],
                    "guid": "k[5v!*.w@",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "<div>What does the expression&nbsp;\\(Z^TZ\\)&nbsp;calculate?<div></div></div><div></div>",
                        "It computes the dot product of the transpose of&nbsp;\\(Z\\)&nbsp;with&nbsp;\\(Z\\). This operation is essential in calculating the covariance matrix."
                    ],
                    "guid": "h1IhWe+&<=",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why is mean-centering important when calculating the covariance matrix?",
                        "Mean-centering helps to remove bias from the data and ensures that the covariance is calculated around the center of the data."
                    ],
                    "guid": "d<z*NXTsdM",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is a basis in linear algebra?",
                        "A set of&nbsp;\\(n\\)&nbsp;linearly independent vectors that span an&nbsp;\\(n\\)-dimensional space. This means that any vector in that space can be expressed as a linear combination of the basis vectors."
                    ],
                    "guid": "r,[h`{Ie]Y",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What does it mean for vectors to be linearly independent?",
                        "No vector in the set can be written as a linear combination of the other vectors.  In other words, each vector adds a unique direction to the set."
                    ],
                    "guid": "DW>8<xQW^?",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is an orthogonal basis?",
                        "A basis where all the vectors are orthogonal to each other. This means the dot product of any two distinct vectors in the basis is zero."
                    ],
                    "guid": "I!#7{--JNR",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How does a basis form a \"coordinate system\"?",
                        "The basis vectors define the axes of the coordinate system. Any vector in the space can be uniquely represented by its coordinates with respect to these axes."
                    ],
                    "guid": "iO&i02F/p-",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How many possible bases are there for a given vector space?",
                        "There are an infinite number of possible bases for a given vector space. You can choose different sets of linearly independent vectors to form a basis."
                    ],
                    "guid": "w;K*^KSQ)f",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the first principal axis (or principal axis) of a dataset?",
                        "The vector that describes the direction of greatest variance in the data."
                    ],
                    "guid": "foA,!YLtFJ",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What does \"variance\" mean in the context of a principle axis?",
                        "Variance is a measure of how spread out the data is along a particular direction. A higher variance means the data points are more spread out."
                    ],
                    "guid": "CphVn@v5ML",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How is the first principal axis found?",
                        "It's found by a mathematical technique called Principal Component Analysis (PCA). PCA finds the eigenvectors of the covariance matrix of the data. The eigenvector corresponding to the largest eigenvalue is the first principal axis."
                    ],
                    "guid": "jah>QO[Q.}",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why is the first principal axis important?",
                        "It captures the most important direction of variation in the data. This can be useful for dimensionality reduction, as you can project the data onto this axis and retain most of the information."
                    ],
                    "guid": "l%6)osA>gS",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the second principal axis?",
                        "The vector that describes the direction of the second greatest variance in the data, and is orthogonal (perpendicular) to the first principal axis."
                    ],
                    "guid": "cX~HLK[0rk",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What does \"orthogonal\" mean?",
                        "Orthogonal means perpendicular. The two axes form a 90-degree angle."
                    ],
                    "guid": "yrC`T%HUbn",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How is the second principal axis found?",
                        "It's also found using PCA. It's the eigenvector of the covariance matrix corresponding to the second largest eigenvalue."
                    ],
                    "guid": "uR6Z}~?i3}",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why is the second principal axis important?",
                        "<div><div>It captures the second most important direction of variation in the data. When combined with the first principal axis, it provides a more complete picture of the data's variability.<div></div></div></div><div></div><div><div>Sources and related content</div><div><div><div><div><a href=\"/saved-info\"><div></div></a></div></div></div></div></div>"
                    ],
                    "guid": ">7JS;ZYSi",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "In general, what can you say about the set of&nbsp;\\(n\\)&nbsp;principal axes in an&nbsp;\\(n\\)-dimensional space?",
                        "They form a basis for that space. This means that any vector in the space can be expressed as a linear combination of the principal axes."
                    ],
                    "guid": "Do[a1_/(G,",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is special about the basis formed by the principal axes?",
                        "It's an orthogonal basis, meaning all the principal axes are mutually perpendicular to each other."
                    ],
                    "guid": "PX*!]xK}n5",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why is it useful that the principal axes form a basis?",
                        "It allows you to represent the data in a new coordinate system where the axes are aligned with the directions of greatest variance. This can be helpful for dimensionality reduction and data visualization."
                    ],
                    "guid": "eL/{c_6~9A",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the equation that defines the relationship between eigenvectors and eigenvalues of a matrix?",
                        "\\[Av = \\lambda v\\]where:<br>\\(A\\)&nbsp;is a square matrix<br>\\(v\\)&nbsp;is an eigenvector of&nbsp;\\(A\\). It's a non-zero vector that, when multiplied by A, only changes in scale (it's stretched or shrunk), not in direction.<br>\\(\\lambda\\)&nbsp;is an eigenvalue of&nbsp;\\(A\\).&nbsp;It's a scalar that represents the factor by which the eigenvector is scaled when multiplied by&nbsp;\\(A\\).<br>"
                    ],
                    "guid": "C_-E>S5IsI",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the maximum number of eigenvector-eigenvalue pairs a matrix can have?",
                        "For an&nbsp;\\(n \\times n\\)&nbsp;matrix, there are at most&nbsp;\\(n\\)&nbsp;eigenvector-eigenvalue pairs."
                    ],
                    "guid": "x3B7.>p9m`",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is a symmetric matrix?",
                        "A square matrix that is equal to its transpose. This means that if you flip the matrix across its diagonal, it remains the same."
                    ],
                    "guid": "yOOUkUKxJ>",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "If a matrix is symmetric, what can you say about its eigenvectors?",
                        "The set of eigenvectors of a symmetric matrix is orthogonal. This means that all the eigenvectors are perpendicular to each other."
                    ],
                    "guid": "g{ZYB-/OfL",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is eigendecomposition?",
                        "A way of decomposing (breaking down) a square matrix into a product of three matrices:<br>\\[A = QΛQ^T\\]Where:<br>\\(A\\)&nbsp;is the original square matrix<br>\\(Q\\)&nbsp;is a matrix whose columns are the eigenvectors of&nbsp;<code>\\(A\\)<br>\\(Λ\\)&nbsp;is a diagonal matrix whose diagonal elements are the eigenvalues of&nbsp;\\(A\\).<br><div></div><sup></sup><div></div></code>"
                    ],
                    "guid": "b6EDuJ<U6b",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is a diagonal matrix?",
                        "A matrix where all the non-diagonal elements are zero."
                    ],
                    "guid": "Mi)M8E79N;",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the formula for reversing a linear transform, and what condition must be met?",
                        "\\[T=ZW^{-1}\\]The transformaton matrix&nbsp;\\(W\\)&nbsp;<b>must </b>be <b>invertable</b><br>"
                    ],
                    "guid": "tCB#P>cO6d",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "When is a linear transform considered a \"lossy\" process?",
                        "A linear transform is lossy when the dimensionality of the input space (\\(Z\\)) and the output space (\\(T\\)) are different."
                    ],
                    "guid": "g(lxIQYXjK",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What does a linear transform matrix&nbsp;\\(W\\)&nbsp;do?",
                        "A linear transform&nbsp;\\(W\\)&nbsp;projects data from one space into another."
                    ],
                    "guid": "Js>K?*&[TR",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How is the original data represented in a linear transform, given the formula&nbsp;\\(T = ZW\\)?",
                        "The original data is stored in the rows of matrix&nbsp;\\(Z\\)."
                    ],
                    "guid": "t;O7Jhq&{j",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "In the linear transform&nbsp;\\(T = ZW\\), how does the dimensionality of&nbsp;\\(T\\)&nbsp;relate to&nbsp;\\(Z\\)?",
                        "\\(T\\)&nbsp;can have&nbsp;<strong>fewer dimensions</strong>&nbsp;than&nbsp;\\(Z\\)."
                    ],
                    "guid": "lZel2_Pb5(",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "<div>What is PCA?<div></div></div><div></div>",
                        "PCA stands for Principal Component Analysis. It's an orthogonal linear transform that maps data from its original space to a new space defined by the principal axes of the data."
                    ],
                    "guid": "tSNP-H}]kq",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the transform matrix&nbsp;\\(W\\)&nbsp;in PCA?",
                        "It's the eigenvector matrix&nbsp;\\(Q\\)&nbsp;from the eigendecomposition of the covariance matrix of the data."
                    ],
                    "guid": "xw}kxUU~g>",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How is dimensionality reduction achieved in PCA?",
                        "By removing the eigenvectors (columns of&nbsp;\\(Q\\)) that correspond to low eigenvalues. This means keeping only the eigenvectors that capture the most significant variance in the data."
                    ],
                    "guid": "j(+,.ZMkM{",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How do you decide which eigenvectors to keep in PCA?",
                        "Typically, you sort the eigenvectors in descending order of their corresponding eigenvalues and keep the first&nbsp;\\(L\\)&nbsp;columns of&nbsp;\\(Q\\), where&nbsp;\\(L\\)&nbsp;is the desired number of dimensions in the reduced space."
                    ],
                    "guid": "oTauyU+nR>",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why is PCA useful?",
                        "<li>Reduces the dimensionality of data while preserving important information.</li>\n<li>Can be used for feature extraction, visualization, and noise reduction.</li>\n<li>Helps to overcome the \"curse of dimensionality\" in machine learning.</li>"
                    ],
                    "guid": "IkiF~%J;S",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the first step in the PCA algorithm?",
                        "Mean-center the data vectors. This means subtracting the mean of each feature from each data point."
                    ],
                    "guid": "w;{[Vl;8y-",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are the steps in the PCA algorithm?",
                        "<img src=\"paste-0e3f8835dd50dabeedc45ace2a586815c7d9f632.jpg\">"
                    ],
                    "guid": "C_bKotOX&d",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are global features in computer vision?",
                        "Features that are extracted from the entire image. They describe the image as a whole, rather than focusing on specific regions or parts."
                    ],
                    "guid": "Ef,F}jm.z~",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How are global features extracted?",
                        "An algorithm analyzes the entire image and computes a single feature vector that represents the image's overall characteristics."
                    ],
                    "guid": "Mu`B@)aN$n",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are some examples of global features?",
                        "<li>Color histograms</li>\n<li>Texture descriptors (e.g., Haralick features)</li>\n<li>Shape descriptors (e.g., Hu moments)</li>\n<li>Global edge histograms</li>"
                    ],
                    "guid": "bJ7SXFt7$N",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are grid or block-based features?",
                        "Features extracted from distinct blocks or regions of an image, rather than from the entire image at once."
                    ],
                    "guid": "CV({<0oo*.",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How are grid or block-based features extracted?",
                        "<ol><li>The image is divided into a grid of blocks.</li><li>\nFeatures are extracted from each block independently.</li></ol>"
                    ],
                    "guid": "gx>,Id~6!_",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the output of a grid-based feature extractor?",
                        "Multiple feature vectors, one for each block in the grid."
                    ],
                    "guid": "e9YHcy}_3>",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are some examples of features that can be extracted from each block?",
                        "<li>Local histograms</li>\n<li>Texture descriptors (e.g., Local Binary Patterns - LBP)</li>\n<li>Edge information</li>\n<li>Keypoint descriptors (e.g., SIFT, SURF)</li>"
                    ],
                    "guid": "bA_;h8D6&=",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are region-based features?",
                        "Features extracted from specific regions or segments of an image, as opposed to the entire image or a fixed grid."
                    ],
                    "guid": "u3K,_?80C4",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How are region-based features extracted?",
                        "<ol><li>The image is segmented into meaningful regions.</li><li>\nFeatures are extracted from each region independently.</li></ol>"
                    ],
                    "guid": "B[a}5Ze#lb",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the output of a region-based feature extractor?",
                        "Multiple feature vectors, one for each region in the segmented image."
                    ],
                    "guid": "y0vb$)[G/d",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are some examples of features that can be extracted from each region?",
                        "<li>Shape descriptors (e.g., area, perimeter, circularity)</li>\n<li>Texture descriptors</li>\n<li>Color histograms</li>\n<li>Local keypoint descriptors</li>"
                    ],
                    "guid": "At;F%];Ibi",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are local features in computer vision?",
                        "Features that are extracted from specific interest points or keypoints in an image. These interest points are locations in the image that are distinctive or unique, such as corners, edges, or blobs."
                    ],
                    "guid": "H[qr16^jnI",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How are local features extracted?",
                        "<li><strong>Interest point detection:</strong> Identify interest points in the image using a keypoint detector (e.g., Harris corner detector, SIFT, SURF).</li>\n<li><strong>Feature description:</strong> For each interest point, extract a feature vector that describes the local image patch around that point.</li>"
                    ],
                    "guid": "gkzOQZ]%]",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the output of a local feature extractor?",
                        "Multiple feature vectors, one for each detected interest point in the image."
                    ],
                    "guid": "e=y!z%G,Zi",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are some examples of local feature descriptors?",
                        "<li>SIFT (Scale-Invariant Feature Transform)</li>\n<li>SURF (Speeded-Up Robust Features)</li>\n<li>ORB (Oriented FAST and Rotated BRIEF)</li>"
                    ],
                    "guid": "Q[ksYD+(|:",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are some applications of local features?",
                        "<li>Object recognition</li>\n<li>Image matching</li>\n<li>3D reconstruction</li>\n<li>Image stitching</li>"
                    ],
                    "guid": "G>?mj9@78]",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is an image histogram?",
                        "A graphical representation of the distribution of pixel intensities in an image. It shows how many pixels have each possible intensity value."
                    ],
                    "guid": "e[MQ*+.D-X",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What type of feature is an image histogram?",
                        "A global feature. It describes the overall distribution of colors or intensities in the entire image."
                    ],
                    "guid": "z_l0|WzMh[",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why is using just the average color not a robust feature?",
                        "<li>It doesn't capture the full distribution of colors in the image.</li>\n<li>It can be sensitive to changes in lighting or the presence of multiple colors.</li>"
                    ],
                    "guid": "io{fgie6=S",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why is a histogram a more robust global feature?",
                        "<li>It captures the distribution of pixel values, providing more information about the image content.</li>\n<li>It can handle images with multiple colors more effectively.</li>"
                    ],
                    "guid": "IoXotatB#P",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is a joint-color histogram?",
                        "A histogram that captures the joint distribution of color channels in an image. It measures how often different combinations of colors occur together."
                    ],
                    "guid": "gB,9AN?#*m",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How is a joint-color histogram different from a regular 3-channel histogram?",
                        "<li><strong>3-channel histogram:</strong> Treats each color channel (R, G, B) independently, resulting in three separate histograms.</li>\n<li><strong>Joint-color histogram:</strong> Considers all color channels together, creating a multi-dimensional histogram that captures the co-occurrence of colors.</li>"
                    ],
                    "guid": "tWgw+?k:1!",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How is the color space represented in a joint-color histogram?",
                        "The color space is quantized into bins. Each bin represents a specific range of color values."
                    ],
                    "guid": "Q/Ij/,*=?u",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What does each bin in a joint-color histogram store?",
                        "The number of pixels in the image that have a color falling within the range represented by that bin."
                    ],
                    "guid": "xxg(.7lJo%",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why is a joint-color histogram \"flattened\" or \"unwrapped\"?",
                        "To represent it as a feature vector. Feature vectors are typically one-dimensional arrays, so the multi-dimensional histogram is converted into a single vector."
                    ],
                    "guid": "mv0>)]f6hi",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is image segmentation?",
                        "The process of partitioning an image into multiple segments or regions."
                    ],
                    "guid": "Rw%k$C1|X",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the goal of segmentation?",
                        "To simplify the image by dividing it into meaningful parts that are easier to analyze."
                    ],
                    "guid": "p?6~6e]v9v",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is a segment in image segmentation?",
                        "A set of pixels in the image that share certain visual characteristics, such as color, texture, or intensity."
                    ],
                    "guid": "ch<QS%]x;W",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How is segmentation related to region-based features?",
                        "Segmentation is the first step in creating region-based features. Once the image is segmented, features can be extracted from each region independently."
                    ],
                    "guid": "M_PO1l64wK",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is global binary thresholding?",
                        "A simple image segmentation technique that divides an image into two segments (foreground and background) based on a single threshold value."
                    ],
                    "guid": "D*YN%}t@zc",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How does global binary thresholding work?",
                        "<ol><li>Choose a threshold value.</li><li>\nPixels with intensity values below the threshold are assigned to one segment (e.g., background).</li><li>\nPixels with intensity values above the threshold are assigned to the other segment (e.g., foreground).</li></ol>"
                    ],
                    "guid": "Q<Lb{c;>~L",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is a binary image?",
                        "An image that has only two possible pixel values, often black and white (or 0 and 1). Global binary thresholding produces a binary image."
                    ],
                    "guid": "J,H(%xA^?z",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are the advantages of global binary thresholding?",
                        "<li>It's very simple and computationally fast.</li>\n<li>It can be effective for images with high contrast between the object and the background.</li>"
                    ],
                    "guid": "rM;KD<LTX9",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are the disadvantages of global binary thresholding?",
                        "<li>It requires a manually set threshold, which might not be optimal for all images.</li>\n<li>It's not robust to changes in lighting conditions.</li>\n<li>It might not work well for images with complex backgrounds or varying object intensities.</li>"
                    ],
                    "guid": "nR^JdO)F(+",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "When can global binary thresholding be effective?",
                        "<div>In applications with:</div><ul>\n<li>Controlled lighting conditions</li>\n<li>High-contrast objects</li>\n<li>Simple backgrounds</li></ul>"
                    ],
                    "guid": "OS4|@.`i5P",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is Otsu's thresholding method?",
                        "An algorithm that automatically finds an optimal threshold value for separating an image into foreground and background."
                    ],
                    "guid": "v=v_RK48Ia",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What assumption does Otsu's method make about the image?",
                        "It assumes that the image histogram has two peaks, representing the foreground and background pixel intensities."
                    ],
                    "guid": "H0+[c(gBTt",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is \"interclass variance\"?",
                        "A measure of how much variation there is between two classes (in this case, the foreground and background)."
                    ],
                    "guid": "v9l;@siYX^",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How does Otsu's method find the threshold?",
                        "It exhaustively searches for the threshold that maximizes the interclass variance. This means it tries all possible threshold values and chooses the one that best separates the foreground and background."
                    ],
                    "guid": "u8;!2zVb3n",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are the advantages of Otsu's method?",
                        "<li>It's automatic; you don't need to manually choose a threshold.</li>\n<li>It often provides a good threshold for separating objects from the background.</li>"
                    ],
                    "guid": "n>i)J3KaPW",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are the limitations of Otsu's method?",
                        "<li>It might not work well for images with complex histograms or more than two prominent peaks.</li>\n<li>It assumes a clear separation between foreground and background intensities.</li>"
                    ],
                    "guid": "rVJ^;Pk&<p",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is adaptive thresholding (or local thresholding)?",
                        "A thresholding technique where a different threshold value is calculated for each pixel in the image based on the local neighborhood around that pixel."
                    ],
                    "guid": "Nra-([&993",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why is adaptive thresholding useful?",
                        "<div>It can handle images with uneven illumination or varying contrast by adapting the threshold to the local characteristics of the image.<div></div></div><div></div>"
                    ],
                    "guid": "A4`%[4E7Yp",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How is the threshold determined for each pixel?",
                        "By analyzing the pixel values in a local neighborhood around the pixel. This neighborhood is usually defined by a square or rectangular window."
                    ],
                    "guid": "ICm/~>RG#g",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is mean adaptive thresholding?",
                        "A type of adaptive thresholding where the threshold for each pixel is calculated based on the mean (average) value of the neighboring pixels."
                    ],
                    "guid": "H(H&<2Qj0O",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How does mean adaptive thresholding work?",
                        "<ol><li>Define a window (neighborhood) around the current pixel.</li><li>\nCalculate the mean intensity value of the pixels within that window.</li><li>\nAdd a constant offset value to the mean.</li><li>\nIf the current pixel's intensity is less than the adjusted mean, set it to 0 (or black). Otherwise, set it to 1 (or white).</li></ol>"
                    ],
                    "guid": "Fng:G7s!:j",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are the parameters involved in mean adaptive thresholding?",
                        "<li><strong>Size of window:</strong>  Determines the size of the neighborhood around each pixel. A larger window considers more neighbors.</li>\n<li><strong>Constant offset value:</strong> A value added to (or subtracted from) the mean to fine-tune the threshold.</li>"
                    ],
                    "guid": "GA={5s0STX",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How does the size of the window affect the result?",
                        "<li>A smaller window is more sensitive to local variations in intensity.</li>\n<li>A larger window provides more smoothing and is less sensitive to noise.</li>"
                    ],
                    "guid": "xwqTU@t2[c",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How does the constant offset value affect the result?",
                        "<li>Adding a positive offset makes the threshold higher, resulting in more pixels being classified as black.</li>\n<li>Adding a negative offset makes the threshold lower, resulting in more pixels being classified as white.</li>"
                    ],
                    "guid": "qb=2iMb0&#",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is a key advantage of adaptive/local thresholding?",
                        "It provides good invariance to uneven lighting and contrast. This means it can adapt to different lighting conditions and variations in image intensity."
                    ],
                    "guid": "G7prILick+",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is a disadvantage of adaptive thresholding compared to global methods?",
                        "It's computationally more expensive. This is because it needs to calculate a different threshold for each pixel in the image."
                    ],
                    "guid": "lKY`[%r_b{",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is a challenge in using adaptive thresholding?",
                        "Choosing the appropriate window size can be difficult. The window size determines the neighborhood of pixels used to calculate the local threshold."
                    ],
                    "guid": "AsAr`zg>8_",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why might the object's distance to the camera be a problem for adaptive thresholding?",
                        "If the object can appear at different distances, its size in the image will change. This can make it difficult to choose a fixed window size that works well for all situations."
                    ],
                    "guid": "gI#9bPcr@C",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How can you address the issue of varying object distances for adaptive thresholding?",
                        "<li>Consider using a multi-scale approach with different window sizes.</li>\n<li>Use a more sophisticated segmentation technique that is not solely reliant on local thresholds.</li>"
                    ],
                    "guid": "vrPTzp?OI+",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How can K-Means clustering be used for image segmentation?",
                        "By clustering the color vectors of the pixels in the image. Each cluster represents a segment in the image."
                    ],
                    "guid": "o7paaT@~p4",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are \"color vectors\"?",
                        "Vectors that represent the color of a pixel. For example, in the RGB color space, a color vector would be <code>[r, g, b]</code>, where <code>r</code>, <code>g</code>, and <code>b</code> are the red, green, and blue values of the pixel."
                    ],
                    "guid": "ra5g{*-(?]",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How are pixels assigned to segments in K-Means segmentation?",
                        "Each pixel is assigned to the segment corresponding to the closest cluster centroid in the color space."
                    ],
                    "guid": "tM7q*UNk1]",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is a cluster centroid?",
                        "The center point of a cluster, representing the average color of the pixels in that cluster."
                    ],
                    "guid": "I)#r:e<<u7",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why is it important for the color space and distance function to be compatible?",
                        "Because the distance between color vectors should reflect the perceptual difference between the colors. If the color space and distance function are not compatible, the clustering might not produce meaningful segments."
                    ],
                    "guid": "oHOP)H:?*<",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is a potential issue with using K-Means for image segmentation?",
                        "A naïve approach might not preserve the continuity of segments. This means you could end up with isolated pixels assigned to a segment, even though they are far away from other pixels in that segment."
                    ],
                    "guid": "rfpp>~5uJb",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How do you include spatial information in the feature vectors?",
                        "Instead of just using the color vector <code>[r, g, b]</code>, use a vector that also includes the pixel's coordinates: <code>[r, g, b, x, y]</code>."
                    ],
                    "guid": "j,RJ*n8nkv",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why is it important to normalize the x and y coordinates?",
                        "To remove the effect of different image sizes. Normalizing ensures that the spatial information is scaled consistently across different images."
                    ],
                    "guid": "bI.w@tw+fP",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How can you control the influence of spatial information in the clustering?",
                        "By scaling the x and y components of the feature vector. Increasing the scale gives more weight to spatial information, while decreasing the scale gives more weight to color information."
                    ],
                    "guid": "Npi?_5US?S",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What does it mean for two pixels to be connected?",
                        "Two pixels are said to be connected if they are spatially adjacent to each other and satisfy certain conditions."
                    ],
                    "guid": "Ji`do39|n",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are the two standard ways of defining pixel adjacency?",
                        "<li><strong>4-connectivity:</strong> A pixel is considered connected to its 4 immediate neighbors (above, below, left, and right).</li>\n<li><strong>8-connectivity:</strong> A pixel is considered connected to its 8 neighbors, including the diagonal neighbors.</li>"
                    ],
                    "guid": "c$%d]+#*V=",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why is pixel connectivity important?",
                        "<div>It's used in various image processing operations, such as:</div><ul>\n<li><strong>Region growing:</strong>  Starting from a seed pixel, you iteratively add connected pixels to form a region.</li>\n<li><strong>Connected component labeling:</strong> Assigning labels to connected sets of pixels to identify distinct objects or regions in the image.</li>\n<li><strong>Contour tracing:</strong> Following the connected boundary pixels of an object.</li></ul>"
                    ],
                    "guid": "p^%!yT6QM:",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is connected component labeling?",
                        "The process of finding and labeling all connected components in a binary image."
                    ],
                    "guid": "z[E!y26y5E",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How does connected component labeling work?",
                        "It scans the image and assigns a unique label to each connected component. Pixels belonging to the same component get the same label."
                    ],
                    "guid": "o(EHvGwcQ+",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are some common algorithms for connected component labeling?",
                        "<li>Two-pass algorithm</li>\n<li>Sequential labeling</li>\n<li>Union-find algorithm</li>"
                    ],
                    "guid": "m~XvtO:N9[",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are the performance tradeoffs in connected component labeling?",
                        "Different algorithms have different tradeoffs between memory usage and processing time. Some algorithms might be faster but require more memory, while others might be slower but use less memory."
                    ],
                    "guid": "L_wJTsCfnJ",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "The Two-pass Union-Find algorithm for connected component labelling",
                        "<img src=\"paste-eda7c6ec23b44a8998c17fcb5502bfc8850fd0ce.jpg\">"
                    ],
                    "guid": "K$%b7^A)4Q",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                }
            ],
            "reviewLimit": null,
            "reviewLimitToday": null
        },
        {
            "__type__": "Deck",
            "children": [],
            "crowdanki_uuid": "e04e5634-d758-11ef-b05b-204ef6f14df6",
            "deck_config_uuid": "0aa25128-d1e9-11ef-92a1-204ef6f14df6",
            "desc": "",
            "dyn": 0,
            "extendNew": 0,
            "extendRev": 0,
            "media_files": [
                "paste-472e1667e5614f72f8eecbc3454b8d7b664c891c.jpg",
                "paste-6968cdd7ff0e55b8136a6c976f332f29fd104c4b.jpg",
                "paste-6b64c2cc37681ad5feb5980853bace5b14108b4d.jpg",
                "paste-8da3aad33e526c47b48e024af87dbe3fd210472e.jpg",
                "paste-dc62ed9c99d79f82ce52d1ad7c4e603736f52aeb.jpg"
            ],
            "name": "Week 6",
            "newLimit": null,
            "newLimitToday": null,
            "notes": [
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the area of a connected component in an image?",
                        "The number of pixels that belong to that connected component."
                    ],
                    "guid": "dA/iaUl9qJ",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the formula for calculating the perimeter of a connected component?",
                        "\\[P(S)=\\sum_j\\sqrt{(x_j-x_{j-1})^2+(y_j-y_{j-1})^2}\\]where:<br>\\(P(S)\\)&nbsp;is the perimeter of the component S<br>\\((x_j, y_j)\\)&nbsp;are the coordinates of the j-th boundary pixel<br><br>It calculates the distance between each pair of consecutive boundary pixels using the Pythagorean theorem and sums those distances to get the total perimeter.<br>"
                    ],
                    "guid": "q7-F5rX6w~",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is compactness in the context of image processing?",
                        "A measure of how tightly packed or circular a shape is. A compact shape has its pixels close together, while a less compact shape is more spread out or elongated."
                    ],
                    "guid": "t+;z|jBja,",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How is compactness often computed?",
                        "As the weighted ratio of the area of the shape to the square of its perimeter:<br>\\[C(s)=\\frac{4\\pi A(s)}{(P(s))^2}\\]where:<br>\\(C(s)\\)&nbsp;is the compactness of shape&nbsp;\\(s\\)<br>\\(A(s)\\)&nbsp;is the area of the shape<br>\\(P(s)\\)&nbsp;is the perimeter of the shape<br>"
                    ],
                    "guid": "L5Q!qv+cP{",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the range of values for compactness?",
                        "<div>Compactness values typically range from 0 to 1.</div><ul>\n<li>A perfect circle has a compactness of 1.</li>\n<li>More elongated or complex shapes have lower compactness values.</li></ul>"
                    ],
                    "guid": "O`w=OMYuJo",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why does the circle have a compactness of 1?",
                        "Because a circle is the most compact shape for a given area. It encloses the most area with the shortest perimeter."
                    ],
                    "guid": "K6!cxr[qkR",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why does the complex shape have a low compactness?",
                        "Because it has a relatively large perimeter compared to its area. It's more spread out and less tightly packed."
                    ],
                    "guid": "LK+Yjr(^&y",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How do I calculate the centre of mass of a componant?",
                        "calculate the mean position of all the pixels in the componant"
                    ],
                    "guid": "iJW1ufn#,g",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is irregularity (or dispersion) in image processing?",
                        "A measure of how spread out or elongated a shape is. It quantifies how much the shape deviates from being perfectly circular or compact."
                    ],
                    "guid": "ktbr@4&R8R",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the formula for calculating irregularity? (\\(I(s)\\))",
                        "\\[I(s)=\\frac{\\pi\\max((x_j-\\bar{x})^2+(y_j-\\bar{y})^2)}{A(s)}\\]where:<br>\\(I(s)\\)&nbsp;is the measure of irregularity for shape&nbsp;\\(s\\)<br>\\((x_j, y_j)\\)&nbsp;are the coordinates of the j-th boundary pixel<br>\\((\\bar{x},\\bar{y})\\)&nbsp;are the coordianates of the centroid (centre) of the shape<br>\\(A(s)\\)&nbsp;is the area of the shape<br>"
                    ],
                    "guid": "tB:$geRbF[",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the formula for calculating irregularity? (\\(IR(s)\\))",
                        "\\[IR(s)=\\frac{\\max(\\sqrt{(x_j-\\bar{x})^2+(y_j-\\bar{y})^2)}}{\\min(\\sqrt{(x_j-\\bar{x})^2+(y_j-\\bar{y})^2)}}\\]where:<br>\\(IR(s)\\)&nbsp;is the measure of irregularity for shape&nbsp; \\(s\\)<br>\\((x_j,y_j)\\)&nbsp;are the coordinates of the j-th boundary pixel<br>\\((\\bar{x},\\bar{y})\\)&nbsp;are the coordinates of the centroid (centre) of the shape<br>\\(A(s)\\)&nbsp;is the area of the shape<br>"
                    ],
                    "guid": "PF60_A6qx_",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are moments in image processing?",
                        "Mathematical measures that describe the distribution of pixels in a shape or an image."
                    ],
                    "guid": "z<73ew~M1",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What types of images can moments be computed for?",
                        "Moments can be computed for any gray-level image, not just binary images."
                    ],
                    "guid": "C}<t0mb|cx",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the formula for a standard two-dimensional Cartesian moment of an image?",
                        "\\[m_{pq}=\\sum_x\\sum_yx^py^qI(x,y)\\Delta A\\]where:<br>\\(p\\)&nbsp;and&nbsp;\\(q\\)&nbsp;are the orders of the moment<br>\\(x\\)&nbsp;and&nbsp;\\(y\\)&nbsp;are the pixel coordinates<br>\\(I(x,y)\\)&nbsp;is the intensity value of the pixel at&nbsp;\\(x,y\\)<br>\\(\\Delta A\\)&nbsp;is the area of the pixel<br>"
                    ],
                    "guid": "w`1AR3U&v^",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How does the formula simplify for a connected component?",
                        "\\[m_{pq}=\\sum_ix_i^py_i^q\\]where:<br>\\((x_i,y_i)\\)&nbsp;are the coordinates of the i-th pixel in the connected component<br>"
                    ],
                    "guid": "D?dYiYj@p8",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the zero-order moment (\\(m_{00}\\)) of a connected component?",
                        "It's simply the area of the component (the number of pixels in the component)."
                    ],
                    "guid": "DQVe4USsUK",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is a limitation of standard 2D moments?",
                        "They are not invariant to translation, rotation, and scaling. This means that if you translate, rotate, or scale the shape, the values of the standard moments will change."
                    ],
                    "guid": "b*q)#`]ny?",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are central moments?",
                        "Moments that are calculated relative to the centroid (center of mass) of the shape."
                    ],
                    "guid": "okp,m$u%*m",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the formula for a central moment?",
                        "\\[\\mu_{pq}=\\sum_i(x_i-\\bar{x})^p(y_i-\\bar{y})^q\\]where:<br>\\(p\\)&nbsp;and&nbsp;\\(q\\)&nbsp;are the orders of the moment<br>\\((x_i,y_i)\\)&nbsp;are the coordinates of the i-th pixel in the shape<br>\\((\\bar{x},\\bar{y})\\)&nbsp;are the coordinates of the centroid of the shape<br>"
                    ],
                    "guid": "yRDe;c;wZb",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the advantage of using central moments over standard moments?",
                        "Central moments are translation invariant. This means that if you translate the shape, the values of the central moments will remain the same."
                    ],
                    "guid": "zpme*w,#<y",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Which central moments have no descriptive power?",
                        "\\(μ_{01}\\)&nbsp;and&nbsp;\\(\\mu_{10}\\)&nbsp;are always&nbsp;\\(0\\), so they don't provide any useful information about the shape."
                    ],
                    "guid": "n_Ge>PT8b<",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are normalized central moments?",
                        "Central moments that have been normalized to be invariant to both scale and translation."
                    ],
                    "guid": "pzd#eal}T6",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What does \"invariant to scale and translation\" mean?",
                        "It means that if you scale or translate the shape, the values of the normalized central moments will remain the same."
                    ],
                    "guid": "I8Xm*$orXY",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the formula for a normalized central moment (\\(η_{pq}\\))?",
                        "\\[\\mu_{pq}=\\frac{\\mu_{pq}}{\\mu_{00}^\\lambda}\\text{ where }\\gamma=\\frac{(p+q)}{2}+1\\]where:<br>\\(\\mu_{pq}\\)&nbsp;is the central moment of order&nbsp;\\((p,q)\\)<br>\\(\\mu_{00}\\)&nbsp;is the zeroth-order central moment (which is the same as the area)<br>"
                    ],
                    "guid": "A;ZrIkfy4x",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why are normalized central moments useful?",
                        "<div>They provide a way to describe the shape of an object that is not affected by its size or position in the image. This makes them useful for tasks like:</div><ul>\n<li>Object recognition</li>\n<li>Shape matching</li>\n<li>Image retrieval</li></ul>"
                    ],
                    "guid": "vET{Y((vs,",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are Hu Moments?",
                        "<div>A set of seven numbers calculated from normalized central moments that are invariant to translation, scale, and rotation.<div></div></div><div></div>"
                    ],
                    "guid": "t0}Y0X(|Ew",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How many Hu Moments are there?",
                        "7"
                    ],
                    "guid": "Ft`6$X/FIz",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the first Hu moment?",
                        "\\[I_1 = \\eta_{20} + \\eta_{02}\\]<br>"
                    ],
                    "guid": "bFHA|Lb_.6",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the second Hu moment?",
                        "\\[I_2 = (\\eta_{20} - \\eta_{02})^2 + 4\\eta_{11}^2\\]<br>"
                    ],
                    "guid": "KuflBY-%RU",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the third Hu moment?",
                        "\\[I_3 = (\\eta_{30} - 3\\eta_{12})^2 + (3\\eta_{21} - \\eta_{03})^2\\]"
                    ],
                    "guid": "Nc/a$]hvp7",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the forth Hu moment?",
                        "\\[I_4 = (\\eta_{30} + \\eta_{12})^2 + (\\eta_{21} + \\eta_{03})^2\\]<br>"
                    ],
                    "guid": "daao+?gxR-",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the fifth Hu moment?",
                        "\\[I_5 = (\\eta_{30} - 3\\eta_{12}) (\\eta_{30} + \\eta_{12})[ (\\eta_{30} + \\eta_{12})^2 - 3 (\\eta_{21} + \\eta_{03})^2] + (3 \\eta_{21} - \\eta_{03}) (\\eta_{21} + \\eta_{03})[ 3(\\eta_{30} + \\eta_{12})^2 -  (\\eta_{21} + \\eta_{03})^2]\\]<br>"
                    ],
                    "guid": "lE-t+`248(",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the sixth Hu moment?",
                        "\\[I_6 =  (\\eta_{20} - \\eta_{02})[(\\eta_{30} + \\eta_{12})^2 - (\\eta_{21} + \\eta_{03})^2] + 4\\eta_{11}(\\eta_{30} + \\eta_{12})(\\eta_{21} + \\eta_{03})\\]<br>"
                    ],
                    "guid": "z=![nA0n]T",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the seventh Hu moment?",
                        "\\[I_7 = (3 \\eta_{21} - \\eta_{03})(\\eta_{30} + \\eta_{12})[(\\eta_{30} + \\eta_{12})^2 - 3(\\eta_{21} + \\eta_{03})^2] - (\\eta_{30} - 3\\eta_{12})(\\eta_{21} + \\eta_{03})[3(\\eta_{30} + \\eta_{12})^2 - (\\eta_{21} + \\eta_{03})^2].\\]<br>"
                    ],
                    "guid": "w}gw4zFyqV",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are chain codes used for in image processing?",
                        "To represent the boundary of a shape in a compact and efficient way."
                    ],
                    "guid": "x1-jV81L2U",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How do you create a chain code?",
                        "<li><strong>Start at a point on the boundary.</strong></li>\n<li><strong>Walk along the boundary</strong> in a specific direction (e.g., clockwise).</li>\n<li><strong>Encode each step</strong> you take as a number representing the direction of movement (e.g., 0 for right, 1 for up-right, 2 for up, etc.).</li>"
                    ],
                    "guid": "I+9&Xe!=&,",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What do the numbers in a chain code represent?",
                        "The direction of movement along the boundary. The specific numbers and their corresponding directions depend on the chosen scheme (e.g., 4-connectivity or 8-connectivity)."
                    ],
                    "guid": "yjVM^_#=O)",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How can you make a chain code invariant to the starting point?",
                        "By cyclically shifting the code so that it forms the smallest possible integer value. This ensures that the same boundary will have the same chain code regardless of where you start tracing it."
                    ],
                    "guid": "fi3z|0,<>F",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are the advantages of using chain codes?",
                        "<li>Compact representation of shape boundaries</li>\n<li>Can be used for shape analysis and recognition</li>\n<li>Relatively simple to compute</li>"
                    ],
                    "guid": "i.ZFBPhig>",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are some limitations of chain codes?",
                        "<li>Can be sensitive to noise and small variations in the boundary</li>\n<li>Might not capture fine details of the shape</li>"
                    ],
                    "guid": "wwcvbNY)N-",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are ASMs (Active Shape Models) and CLMs (Constrained Local Models)?",
                        "They are deformable models used to represent the shape and appearance of objects, often used for tasks like face detection and tracking."
                    ],
                    "guid": "v]BV5{XKqE",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is a PDM (Point Distribution Models)?",
                        "PDM stands for Point Distribution Model. It's a statistical model that represents the shape of an object by a set of points and their variations."
                    ],
                    "guid": "l[w?,c/AO.",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How do ASMs (Active Shape Models)/CLMs (Constrained Local Models)&nbsp;extend PDMs (Point Distribution Models)?",
                        "They add local appearance information around each point in the PDM. This helps to better capture the visual characteristics of the object."
                    ],
                    "guid": "q|%z[4o$mS",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How is local appearance typically represented in ASMs (Active Shape Models)/CLMs (Constrained Local Models)?",
                        "As an image template. This template captures the texture or pattern around each point."
                    ],
                    "guid": "fSf2Lh86SR",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How are ASMs (Active Shape Models)/CLMs (Constrained Local Models)&nbsp;fitted to an image?",
                        "Using a constrained optimization algorithm. This algorithm tries to find the best-fit shape that matches the image while satisfying certain constraints."
                    ],
                    "guid": "Fis>WFyhMo",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are the constraints used in ASMs (Active Shape Models)/CLMs (Constrained Local Models)?",
                        "<li><strong>Plausible shape:</strong> The shape should be consistent with the expected shape of the object (e.g., a face should have two eyes, a nose, and a mouth).</li>\n<li><strong>Good template matching:</strong> The local appearance around each point should match the corresponding template in the model.</li>"
                    ],
                    "guid": "eDC$Zs*9*z",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is an interest point in computer vision?",
                        "A distinctive location in an image that is likely to be detected reliably in different views of the same scene. These points are often corners, edges, or blobs."
                    ],
                    "guid": "di_N`zEm}^",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why is texture variation important for interest points?",
                        "An interest point should have sufficient texture variation in its local neighborhood. This means there should be some changes in intensity or patterns around the point, making it distinctive."
                    ],
                    "guid": "EtY(vwrb$e",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why shouldn't there be \"too much\" texture variation?",
                        "If there's too much variation (like in a highly textured area), it becomes difficult to locate a precise and stable interest point."
                    ],
                    "guid": "j,=&C.[B|_",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the basic idea behind the Harris and Stephens corner detector?",
                        "It searches for corners by looking through a small window and detecting large changes in intensity when the window is shifted in any direction"
                    ],
                    "guid": "h:%hXeB%,O",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "In the context of corner detection, what characterizes a \"flat\" region?",
                        "A \"flat\" region shows no change in intensity in all directions when the window is shifted.<br><img src=\"paste-6b64c2cc37681ad5feb5980853bace5b14108b4d.jpg\">"
                    ],
                    "guid": "mmbCv|S4T~",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "<span style=\"color: rgb(226, 226, 229); background-color: rgb(26, 28, 30);\">In the context of corner detection, what characterizes an \"edge\"?</span>",
                        "An \"edge\" shows no change in intensity along the edge direction, but a significant change in other directions.<br><img src=\"paste-6968cdd7ff0e55b8136a6c976f332f29fd104c4b.jpg\">"
                    ],
                    "guid": "ze#!nqBO+l",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "In the context of corner detection, what characterizes a \"corner\"?",
                        "A \"corner\" shows a significant change in intensity in all directions when the window is shifted.<br><img src=\"paste-472e1667e5614f72f8eecbc3454b8d7b664c891c.jpg\">"
                    ],
                    "guid": "GP#90d9E!q",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Formula for the&nbsp;weighted average change in intensity between a window \nand a shifted version [by&nbsp;\\((𝚫x,𝚫y)\\)] of that window",
                        "<img src=\"paste-dc62ed9c99d79f82ce52d1ad7c4e603736f52aeb.jpg\">"
                    ],
                    "guid": "gsWcGb0s@g",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How is the intensity of the shifted window,&nbsp;\\(I(x_i+Δx,y_i+Δy)\\), approximated in the Harris &amp; Stephens algorithm?",
                        "It's approximated using the first-order terms of the Taylor expansion:<br><img src=\"paste-8da3aad33e526c47b48e024af87dbe3fd210472e.jpg\"><br>Where&nbsp;\\(I_x\\)&nbsp;and&nbsp;\\(I_y\\)&nbsp;are the partial derivatives of the image intensity with respect to&nbsp;\\(x\\)&nbsp;and&nbsp;\\(y\\), respectively."
                    ],
                    "guid": "y*+kPVn+BL",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "After substituting the Taylor expansion and simplifying, what is the final matrix form of the&nbsp;\\(E(x, y)\\)&nbsp;equation?",
                        "\\[E(x, y) = [Δx\\;Δy] M \\begin{bmatrix}\\Delta x \\\\ \\Delta y\\end{bmatrix}\\]<br>"
                    ],
                    "guid": "HT(w9uNfLg",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the structure tensor?",
                        "A square, symmetric matrix that encodes how the local image intensities change with small shifts in position. It's also known as the second moment matrix."
                    ],
                    "guid": "Kq<=NToA-w",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the formula for the structure tensor (\\(M\\))?",
                        "\\[M=\\begin{bmatrix}\\sum_w(I_x(x_i,y_i))^2 &amp; \\sum_wI_x(x_i,y_i)I_y(x_i,y_i) \\\\ \\sum_wI_x(x_i,y_i)I_y(x_i,y_i) &amp; \\sum_w(I_x(x_i,y_i))^2\\end{bmatrix}\\]where:<br>\\(I_x(x_i,y_i)\\)&nbsp;is the image intensity gradient in the x-direction at pixel&nbsp;\\((x_i,y_i)\\)<br>\\(I_y(x_i,y_i)\\)&nbsp;is the image intensity gradient in the y-direction at pixel&nbsp;\\((x_i,y_i)\\)<br>\\(\\sum_w\\)&nbsp;denotes the summation over all pixels in the window<br>"
                    ],
                    "guid": "pjttWk8_yF",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What information does the structure tensor capture?",
                        "It captures the local intensity variations in different directions. The eigenvalues of the structure tensor indicate the strength of these variations."
                    ],
                    "guid": "HHl#GuI-7p",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How is the structure tensor used in the Harris &amp; Stephens corner detector?",
                        "The eigenvalues of the structure tensor are used to determine if a pixel is a corner. A corner is characterized by large intensity variations in both the x and y directions."
                    ],
                    "guid": "C7&j1$ArSR",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What does it mean if the eigenvalues of both the structure tensor is large?",
                        "The pixel is likely a <strong>corner</strong>. This is because there are strong intensity variations in both directions (x and y)."
                    ],
                    "guid": "dB>Gk5UG1q",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What does it mean if the eigenvalue of one structure tensor is much larger than the other?",
                        "The pixel is likely an <strong>edge</strong>. This indicates a strong intensity variation in one direction and a weak variation in the perpendicular direction."
                    ],
                    "guid": "Mq_oJtjljJ",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What if the eigenvalue of both the structure tensors are small?",
                        "The region around the pixel is relatively <strong>flat</strong> (no significant intensity variations)."
                    ],
                    "guid": "p.:S*%}P(_",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the Harris &amp; Stephens response function?",
                        "A function that measures the \"cornerness\" of a pixel based on the determinant and trace of the structure tensor (\\(M\\))."
                    ],
                    "guid": "!][4r=[._",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why is the response function used instead of directly computing eigenvalues?",
                        "Computing eigenvalues can be computationally expensive. The response function provides a more efficient way to identify corners."
                    ],
                    "guid": "JdF0$M9vXr",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the formula for the Harris &amp; Stephens response function (\\(R\\))?",
                        "\\[R=\\det(M)-k\\text{ trace}(M)^2\\]where:<br>\\(\\det(M)=M_{00}M_{11}-M_{01}M_{10}=M_{00}M_{11}-M^2_{10}=\\lambda_1\\lambda_2\\)<br>\\(\\text{trace}(M)=M_{00}+M_{11}=\\lambda_1+\\lambda_2\\)<br>\\(k\\)&nbsp;is a small empirically set constant (usually 0.04 - 0.06)<br>"
                    ],
                    "guid": "Rj?[)M/)L",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How is the Harris &amp; Stephens Response Function used to detect corners?",
                        "Pixels with a high response value (\\(R\\)) are classified as corners. This indicates that they have significant intensity variations in multiple directions."
                    ],
                    "guid": "E&jhF:-q>p",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How is the Harris &amp; Stephens Response Function used to detect edges?",
                        "Pixels with a negative response value ( \\(R\\)) are classified as edges. This indicates that they have significant intensity variations in one direction and weak variation in the perpendicular direction."
                    ],
                    "guid": "GZM0_y;weJ",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What if the&nbsp;Harris &amp; Stephens Response value has a small magnitude?",
                        "The region around the pixel is relatively&nbsp;<strong>flat</strong>&nbsp;(no significant intensity variations)."
                    ],
                    "guid": "fMPH&lJHr1",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the problem of scale in computer vision?",
                        "The fact that objects appear differently in images depending on their distance from the camera. Objects that are closer appear larger and have more detail, while objects that are farther away appear smaller and lose detail."
                    ],
                    "guid": "l1C7%XC|5]",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why is the problem of scale a challenge for computer vision algorithms?",
                        "Many algorithms use a fixed-size processing window or kernel. This means they analyze the image at a single scale. If an object changes scale (due to its distance from the camera), the algorithm might not be able to detect it reliably."
                    ],
                    "guid": "pri|/;(5Lt",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How does the problem of scale affect the Harris corner detector?",
                        "A corner might be detected at one scale but not at another. If the corner is too small (far away), it might be missed. If it's too large (close up), it might not be recognized as a corner."
                    ],
                    "guid": "ztu~kGq%d+",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is scale space theory?",
                        "A formal framework for handling the problem of scale in computer vision. It provides a way to represent an image at multiple scales."
                    ],
                    "guid": "Q?Dsp[Mygj",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How does scale space theory represent an image?",
                        "As a series of increasingly smoothed or blurred images. Each image in the series corresponds to a different scale."
                    ],
                    "guid": "dXE,t:B/Y`",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the scale parameter (\\(t\\))?",
                        "A parameter that represents the amount of smoothing applied to the image. A larger&nbsp;\\(t\\)&nbsp;corresponds to more smoothing"
                    ],
                    "guid": "x,_CRW-Gta",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the key idea in scale space theory?",
                        "Image structures smaller than&nbsp;\\(\\sqrt{t}\\)&nbsp;have been smoothed away at scale&nbsp;\\(t\\). This means that as you increase the scale parameter, finer details in the image are gradually removed."
                    ],
                    "guid": "bu6t[/zQf5",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is Gaussian scale space?",
                        "A scale space representation where the image is smoothed using Gaussian filters with increasing standard deviations (\\(σ\\))."
                    ],
                    "guid": "yPvyV|3[b*",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the formula for Gaussian scale space?",
                        "\\[L(⋅,⋅; t) = g(⋅,⋅; t) * f(⋅,⋅)\\]where:<br><li>\\(L(⋅,⋅; t)\\)&nbsp;is the image in Gaussian scale space at scale&nbsp;\\(t\\)</li>\n<li>\\(g(⋅,⋅; t)\\)&nbsp;is the Gaussian kernel with scale parameter&nbsp;\\(t\\)</li>\n<li>\\(f(⋅,⋅)\\)&nbsp;is the original image</li>\n<li>\\(*\\)&nbsp;denotes convolution</li>"
                    ],
                    "guid": "O~rKB?]RD$",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the formula for the Gaussian kernel&nbsp;\\((g(x, y; t))\\)?",
                        "\\[g(g,y;t)=\\frac{1}{2\\pi t}e^{-(x^2+y^2)/2t}\\]where:<br>\\(x\\)&nbsp;and&nbsp;\\(y\\)&nbsp;are the spatial coordinates<br>\\(t\\)&nbsp;is the scale parameter (related to the standard deviation&nbsp;\\(\\sigma\\)&nbsp;by&nbsp;\\(t = \\sigma^2\\))<br>"
                    ],
                    "guid": "Og):h(l<5y",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is a Gaussian pyramid?",
                        "A multi-scale representation of an image where each level is a downsampled and smoothed version of the previous level."
                    ],
                    "guid": "e8r4W.j@8F",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How is a Gaussian pyramid constructed?",
                        "<ol><li>Start with the original image.</li><li>\nSmooth the image with a Gaussian filter.</li><li>\nDownsample the smoothed image by a factor of 2 (reduce the width and height by half).</li><li>\nRepeat steps 2 and 3 for each level of the pyramid</li></ol>"
                    ],
                    "guid": "j8K/%%ZHjn",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the problem with applying the standard Harris &amp; Stephens detector directly to images with objects at different scales?",
                        "The detector uses a fixed-size window, so it might miss corners that are too small or too large relative to the window size."
                    ],
                    "guid": "ed-Te~V{z&",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How can you extend the Harris &amp; Stephens detector to work across scales?",
                        "By using a multi-scale approach, where you analyze the image at different scales (different window sizes)."
                    ],
                    "guid": "tE[,grkKz4",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is a common way to implement a multi-scale Harris &amp; Stephens detector?",
                        "<strong><ol><li><strong>Create a Gaussian scale space:</strong> Generate a set of images at different scales by smoothing the original image with Gaussian filters of increasing standard deviations.</li><li><strong>Apply the Harris &amp; Stephens detector at each scale:</strong><span style=\"font-weight: bold;\"> Compute the corner response function at every pixel in each scale.</span></li><li><strong>Threshold the responses:</strong><span style=\"font-weight: bold;\"> Keep only the pixels with a response above a certain threshold at each scale.</span></li></ol></strong><br>"
                    ],
                    "guid": "LUZ%Vt|+z%",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is scale space LoG?",
                        "Applying the LoG (Laplacian of Gaussian) operator to an image at multiple scales (different levels of smoothing). This allows you to detect blobs of different sizes."
                    ],
                    "guid": "rWcoj}IR6c",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the formula for normalized scale space LoG?",
                        "\\[\\nabla^2_{\\text{norm}}L(x,y;t)=t(L_{xx}+L_{yy})\\]where:<li>\\(∇^2_{\\text{norm}}L(x, y; t)\\)&nbsp;is the normalized Laplacian of Gaussian at scale&nbsp;\\(t\\)</li>\n<li>\\(L_{xx}\\)&nbsp;is the second derivative of the image&nbsp;\\(L\\)&nbsp;in the x-direction</li>\n<li>\\(L_{yy}\\)&nbsp;is the second derivative of the image&nbsp;\\(L\\)&nbsp;in the y-direction</li>\n<li>\\(t\\)&nbsp;is the scale parameter (related to the standard deviation of the Gaussian)</li>"
                    ],
                    "guid": "QhfV}$qH6E",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How can you find blobs using scale space LoG?",
                        "By finding extrema (maximum or minimum values) of the normalized scale space LoG function. These extrema correspond to the centers of blobs."
                    ],
                    "guid": "m*B1IwP!Yr",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the representative scale of a blob?",
                        "The scale at which the blob is most prominent in the scale space LoG. It's approximately&nbsp;\\(\\sqrt{2t}\\), where&nbsp;\\(t\\)&nbsp;is the scale parameter at which the extremum is found."
                    ],
                    "guid": "BZ6JgdCqrJ",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How do you find extrema in scale space LoG?",
                        "By comparing the value of the normalized scale space LoG at a pixel with its 26 neighbors (8 neighbors in the same scale and 9 neighbors in each of the adjacent scales)."
                    ],
                    "guid": "kN4/?E#_w0",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why is building a LoG scale space computationally expensive?",
                        "It involves convolving the image with multiple Gaussian kernels of different sizes and then computing the Laplacian at each scale."
                    ],
                    "guid": "x)M^TuP4$1",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the Difference of Gaussians (DoG)?",
                        "An approximation of the LoG that is computationally more efficient. It involves subtracting two Gaussian-smoothed images with different scales."
                    ],
                    "guid": "hgjjF,/gy4",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the formula for the DoG approximation of the normalized scale space LoG?",
                        "\\[∇^2_{\\text{norm}}L(x,y;t)\\approx\\frac{t}{\\Delta t}(L(x,y;t+\\Delta t)-L(x,y;t-\\Delta t))\\]where:<ul>\n<li>\\(∇^2_{\\text{norm}}L(x, y; t)\\)&nbsp;is the normalized Laplacian of Gaussian at scale&nbsp;\\(t\\)</li>\n<li>\\(L(x, y; t)\\)&nbsp;is the image in Gaussian scale space at scale&nbsp;\\(t\\)</li>\n<li>\\(Δt\\)&nbsp;is the difference in scale between the two Gaussian-smoothed images</li></ul>"
                    ],
                    "guid": "i*(B2l#zW]",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How can you build a LoG scale space using the DoG approximation?",
                        "By subtracting adjacent scales of a Gaussian scale space. This means taking the difference between consecutive levels in a Gaussian pyramid."
                    ],
                    "guid": "wXRw7T2Swm",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is a DoG (Difference of Gaussians) pyramid?",
                        "A multi-scale representation of an image where each level is obtained by subtracting two Gaussian-smoothed images with different scales (different standard deviations)."
                    ],
                    "guid": "DQ%;(*ZH(+",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How is a DoG pyramid related to a Gaussian pyramid?",
                        "A DoG pyramid can be constructed from a Gaussian pyramid by subtracting adjacent levels."
                    ],
                    "guid": "r/(3[QQbo,",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is an \"oversampled\" pyramid?",
                        "A pyramid where there are multiple images between a doubling of scale. This means that the scale parameter (\\(t\\)) doesn't double between consecutive levels."
                    ],
                    "guid": "yx{tJe`:Gz",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is an \"octave\" in the context of scale space?",
                        "The interval between two images where the scale parameter (\\(t\\)) doubles."
                    ],
                    "guid": "L1K:6,J$m)",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why is a DoG pyramid more efficient than a LoG (Laplacian of Gaussian) scale space?",
                        "It avoids the computationally expensive step of directly computing the Laplacian at each scale. The DoG provides an approximation of the LoG that can be obtained more efficiently by subtracting Gaussian-smoothed images."
                    ],
                    "guid": "ydxqv>d|^^",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the \"matching problem\" in stereo vision?",
                        "The problem of identifying corresponding points (pixels or features) in two or more images that represent the same point in the 3D scene."
                    ],
                    "guid": "yF:^2f5/}W",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are the two distinct types of matching problems in stereo vision?",
                        "<li><strong>Narrow-baseline stereo:</strong> The two cameras are relatively close together.</li>\n<li><strong>Wide-baseline stereo:</strong> The two cameras are far apart.</li>"
                    ],
                    "guid": "A{2Zf;`u}-",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are the challenges in narrow-baseline stereo?",
                        "<li>The images are very similar, making it difficult to find unique correspondences.</li>\n<li>Small errors in matching can lead to large errors in depth estimation.</li>"
                    ],
                    "guid": "E@1_/21:C0",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are the challenges in wide-baseline stereo?",
                        "<li>The images can be very different due to perspective changes and occlusions.</li>\n<li>Establishing correspondences can be more difficult due to significant changes in appearance.</li>"
                    ],
                    "guid": "BBC(IHJuVD",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why is robustness to rotation and lighting not as important in narrow-baseline stereo?",
                        "Because the images from the two cameras are very similar. There's typically less variation in viewpoint and lighting conditions."
                    ],
                    "guid": "pMGmv`!S+S",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why can distinctiveness be reduced in narrow-baseline stereo?",
                        "Because the search for matching points is over a smaller area. The corresponding points are likely to be closer together in the images."
                    ],
                    "guid": "e]Kq%8GHA8",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why is robustness to intensity change and rotation important in wide-baseline stereo?",
                        "Because the images from the two cameras can be very different due to changes in viewpoint and lighting conditions."
                    ],
                    "guid": "lL+_=%#Y5K",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why do descriptors need to be highly distinctive in wide-baseline stereo?",
                        "To avoid mismatches. The corresponding points in the images might look quite different, so the descriptors need to be unique enough to distinguish them from other points."
                    ],
                    "guid": "j<J%Pp-!j<",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What does it mean for a descriptor to be robust to small localization errors?",
                        "The descriptor should not change too much if the interest point is slightly mislocalized (shifted by a few pixels)."
                    ],
                    "guid": "w8_/8}R#DY",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How should a descriptor change with distance?",
                        "It should change gradually for small shifts (a few pixels) but change more rapidly as the distance increases. This helps to balance distinctiveness and robustness to localization errors."
                    ],
                    "guid": "A.eA+~Db[X",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are the requirements for the descriptors for wide baseline stereo",
                        "<ul><li>Need to be robust to intensity change, invariant to rotation<br></li><li>Need to be highly distinctive to avoid mismatches (but not so \ndistinctive that you can’t find any matches!)<br></li><li>Robust to small localisation errors of the interest point<br></li></ul>"
                    ],
                    "guid": "I8eVJlCIO7",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are some problems associated with wider baselines in stereo vision?",
                        "<li>Not robust to rotation: Changes in viewpoint can make it difficult to match features.</li>\n<li>Sensitive to localization of interest point: Small errors in locating interest points can lead to mismatches.</li>\n<li>Wider search range: The search for corresponding points needs to cover a larger area, increasing the chance of mismatches.</li>"
                    ],
                    "guid": "M%`~g$CH}`",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why is robustness to rotation important in wide-baseline stereo?",
                        "Because the images from the two cameras can have significant differences in viewpoint, making it harder to match features that have undergone rotation."
                    ],
                    "guid": "xHN5ug+=r;",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why is sensitivity to localization of interest points a problem?",
                        "If the interest points are not localized accurately, the search for matches might start in the wrong place, leading to incorrect correspondences."
                    ],
                    "guid": "9$%N*YQ@H",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why does a wider search range increase the likelihood of mismatches?",
                        "Because there are more potential candidates for matching, increasing the chance of finding incorrect matches."
                    ],
                    "guid": "ncG4`d+X<H",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How can you address the problems of wide-baseline stereo?",
                        "<li>Use more robust and distinctive features (e.g., SIFT, SURF).</li>\n<li>Employ more sophisticated matching algorithms that can handle geometric distortions and appearance changes.</li>\n<li>Use geometric constraints and context information to guide the matching process.</li>"
                    ],
                    "guid": "jy^nAzWxRd",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is a common way to describe the region around an interest point?",
                        "Using a local histogram. This histogram captures the distribution of pixel intensities or colors in the neighborhood of the interest point."
                    ],
                    "guid": "GvH-KP@iZ9",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the advantage of using local histograms instead of pixel patches?",
                        "Histograms are more robust to small shifts or misalignments in the interest point location. This is because they capture the overall distribution of intensities rather than the exact pixel values."
                    ],
                    "guid": "xS(u5B51tg",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How can you improve the distinctiveness of local histograms?",
                        "<li>Use a larger sampling window to capture more context.</li>\n<li>Use a more sophisticated histogram representation (e.g., orientation histograms).</li>\n<li>Combine local histograms with other features.</li>"
                    ],
                    "guid": "G4y}}+bW=i",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why are local histograms sensitive to interest point localization?",
                        "If the interest point is not localized accurately, the histogram might be computed from a slightly different region, leading to variations in the descriptor."
                    ],
                    "guid": "IJ@5z3.Su6",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Are local histograms invariant to illumination changes?",
                        "No, they are sensitive to changes in lighting conditions. If the brightness or contrast of the image changes, the histogram will also change."
                    ],
                    "guid": "nN<]p<&>#T",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How can you overcome the rotation invariance problem?",
                        "By using a circular window for sampling the pixel intensities."
                    ],
                    "guid": "Q2DH`Oc.eo",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why are local histograms not always rotation invariant?",
                        "If the sampling window used to compute the histogram is square or rectangular, rotating the image can change the distribution of intensities captured by the histogram."
                    ],
                    "guid": "Q?qeb7#WH{",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is a potential problem with using local histograms as descriptors?",
                        "They might not be very distinctive. Many interest points might have similar distributions of intensities, making it difficult to match them correctly."
                    ],
                    "guid": "J?S))]~8?W",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is localization sensitivity in the context of interest point descriptors?",
                        "The problem that the descriptor can change significantly if the interest point is not localized precisely. This can lead to mismatches when comparing features across different images."
                    ],
                    "guid": "kmg8z[vLz_",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How can you overcome localization sensitivity?",
                        "By applying a weighting to the pixels within the sampling region (or window) used to compute the descriptor."
                    ],
                    "guid": "ERDXQ)v-}3",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How should the weighting be applied?",
                        "Pixels near the edge of the sampling patch should have less effect on the descriptor, while pixels nearer the interest point should have more effect."
                    ],
                    "guid": "yzpxCFj-6?",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why does Gaussian weighting help with localization sensitivity?",
                        "It makes the descriptor less sensitive to small shifts in the interest point location because the descriptor is influenced more by the pixels near the center of the window, where the interest point is most likely to be located."
                    ],
                    "guid": "Mo3*6)`H$2",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is a gradient histogram?",
                        "A histogram that captures the distribution of gradient magnitudes and/or directions within a local region of an image."
                    ],
                    "guid": "MMoup`3JTO",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How is a gradient histogram different from a regular intensity histogram?",
                        "A regular histogram captures the distribution of pixel intensities, while a gradient histogram captures the distribution of gradient magnitudes and/or directions."
                    ],
                    "guid": "J3Xga=2,jQ",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why are gradient magnitudes (and directions) invariant to brightness change?",
                        "Because the gradient measures the rate of change of intensity, not the absolute intensity values. A change in brightness will shift all intensity values, but the differences between them (the gradients) will remain the same."
                    ],
                    "guid": "t&3<~O0G>g",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why are gradient histograms more distinctive than intensity histograms?",
                        "They capture more information about the local image structure. The distribution of gradients can better differentiate between different textures and patterns."
                    ],
                    "guid": "t?{fcQUETi",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How do you build a gradient histogram?",
                        "<ol><li><strong>Compute gradients:</strong> Calculate the gradient magnitude and direction for each pixel in the sampling patch (local region).</li><li><strong>Quantize directions:</strong> Divide the range of possible gradient directions (0-360 degrees) into a number of bins (usually around 8).</li><li><strong>Accumulate magnitudes:</strong> For each pixel, add its gradient magnitude to the bin corresponding to its gradient direction.</li></ol>\n\n"
                    ],
                    "guid": "odC&!FIDz*",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What does it mean to \"quantize\" the directions when building a gradient histogram?",
                        "To divide the continuous range of gradient directions into a discrete set of bins. For example, if you use 8 bins, each bin would represent a 45-degree range of directions."
                    ],
                    "guid": "H,f=D2`:m;",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the purpose of accumulating gradient magnitudes in orientation bins?",
                        "To create a histogram that represents the distribution of gradient directions in the local region. This histogram can be used as a descriptor for the interest point."
                    ],
                    "guid": "K#Nxpw|:rz",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is SIFT?",
                        "SIFT stands for Scale-Invariant Feature Transform. It's a widely used feature descriptor that is robust to scale, rotation, and illumination changes."
                    ],
                    "guid": "QniIC=T)q%",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How does SIFT build upon the idea of local gradient histograms?",
                        "It incorporates spatial binning. This means it divides the local region around the interest point into a grid and computes a separate gradient histogram for each cell in the grid."
                    ],
                    "guid": "KbYQ^lkNme",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the standard geometry used in SIFT?",
                        "A 4x4 grid of histograms with 8 orientation bins each."
                    ],
                    "guid": "q#?c;&;^xj",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How many dimensions does a standard SIFT feature vector have?",
                        "128 dimensions (4 x 4 x 8 = 128)."
                    ],
                    "guid": "nmL*PLNk$%",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are the advantages of adding spatial awareness to local gradient histograms?",
                        "It makes the descriptor more distinctive and robust to changes in viewpoint and minor localization errors."
                    ],
                    "guid": "kzY_&M#bc*",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is Euclidean matching in the context of SIFT features?",
                        "A simple method for matching SIFT features between two images by finding the nearest neighbor in feature space using Euclidean distance."
                    ],
                    "guid": "wV_l8j8)u7",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How does Euclidean matching work?",
                        "<ol><li>Take each SIFT feature from the first image.</li><li>\nFind the feature in the second image that has the smallest Euclidean distance to it.</li><li>\nConsider these two features as a match.</li></ol>"
                    ],
                    "guid": "O);K81:P&+",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How can you improve Euclidean matching?",
                        "By using a threshold to reject poor matches. This means discarding matches where the Euclidean distance is above a certain threshold."
                    ],
                    "guid": "~SYmSmT3_",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why doesn't Euclidean matching always work well?",
                        "<div>It can result in lots of mismatches because:</div><ul>\n<li>It only considers the nearest neighbor, even if there are other potential matches that are almost as close.</li>\n<li>It doesn't take into account the overall distribution of features in the images.</li></ul>"
                    ],
                    "guid": "P`Zit8l4@U",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are some alternatives to Euclidean matching for SIFT features?",
                        "<li><strong>Ratio test:</strong> Compare the distance to the nearest neighbor with the distance to the second-nearest neighbor.</li>\n<li><strong>Geometric constraints:</strong> Use geometric relationships between features to filter out mismatches.</li>\n<li><strong>RANSAC (Random Sample Consensus):</strong>  A robust method for finding the best set of matches in the presence of outliers.</li>"
                    ],
                    "guid": "I%P#NWK+N{",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                }
            ],
            "reviewLimit": null,
            "reviewLimitToday": null
        },
        {
            "__type__": "Deck",
            "children": [],
            "crowdanki_uuid": "e04f67a9-d758-11ef-88cb-204ef6f14df6",
            "deck_config_uuid": "0aa25128-d1e9-11ef-92a1-204ef6f14df6",
            "desc": "",
            "dyn": 0,
            "extendNew": 0,
            "extendRev": 0,
            "media_files": [
                "paste-ad4e82800eb3c72b227a6d2dbeacfe4119cb49e7.jpg"
            ],
            "name": "Week 7",
            "newLimit": null,
            "newLimitToday": null,
            "notes": [
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the equation for point transforms?",
                        "\\[x'=Tx\\]where:<br>\\(x'\\)&nbsp;is the transformed coordinate<br>\\(T\\)&nbsp;is the transform matrix<br>\\(x\\)&nbsp;is the original coordinate<br>"
                    ],
                    "guid": "C-4nblW?}0",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the general equation for an affine transformation?",
                        "\\[x'=Ax+b\\]where:<br>\\(x'\\)&nbsp;is the transformed vector<br>\\(x\\)&nbsp;is the original vector<br>\\(A\\)&nbsp;is a linear tranformation matrix<br>\\(b\\)&nbsp;is a translation vector<br>"
                    ],
                    "guid": "H_~76.#yW~",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How can you represent an affine transformation as a single matrix multiplication?",
                        "By adding an extra dimension (usually set to 1) to the vectors, we can rewrite the affine transformation as:<br>\\[\\begin{bmatrix}x' \\\\ y' \\\\ 1\\end{bmatrix} = \\begin{bmatrix}a_{11} &amp; a_{12} &amp; b_1 \\\\ a_{21} &amp; a_{22} &amp; b_2 \\\\ 0 &amp; 0 &amp; 1\\end{bmatrix}\\begin{bmatrix}x \\\\ y \\\\ 1\\end{bmatrix}\\]<br>"
                    ],
                    "guid": "A!Hc38=rWY",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the purpose of adding an extra dimension to the vectors in the single matrix representation of an affine transformation?",
                        "Adding the extra dimension allows us to incorporate both the linear transformation (rotation, scaling, shearing) and the translation into a single matrix multiplication, making calculations more convenient."
                    ],
                    "guid": "M~H{E+lMC%",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are the 6 degrees of freedom (DoF) in a 2D affine transformation?",
                        "<div>The 6 DoF are:</div><ol><li><div>Translation (x)</div></li><li><div>Translation (y)</div></li><li><div>Rotation</div></li><li><div>Scale</div></li><li><div>Aspect Ratio</div></li><li><div>Shear</div></li></ol>"
                    ],
                    "guid": "ldBxh;Nm;h",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are the 4 degrees of freedom (DoF) in a 2D similarity transformation, and how does it differ from an affine transformation?",
                        "<div>The 4 DoF are:</div><ol><li><div>Translation (x)</div></li><li><div>Translation (y)</div></li><li><div>Rotation</div></li><li><div>Scale</div></li></ol>A similarity transform is more restricted than an affine transform. It preserves shape (angles and proportions) but does not include shear or independent scaling of axes. It is also known as an equi-form transform because of this preservation of shape.<br>"
                    ],
                    "guid": "j5W}A/Lf{)",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the matrix form for a 2D translation transformation, and what do (\\(t_x\\)) and (\\(t_y\\)) represent?",
                        "\\[\\begin{bmatrix}\n1 &amp; 0 &amp; t_x \\\\\n0 &amp; 1 &amp; t_y \\\\\n0 &amp; 0 &amp; 1\n\\end{bmatrix}\n\\begin{bmatrix}\nx \\\\\ny \\\\\n1\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nx + t_x \\\\\ny + t_y \\\\\n1\n\\end{bmatrix}\\]where:<br><ul><li><div>(\\(t_x\\)) represents the translation distance in the x-direction.</div></li><li><div>(\\(t_y\\)) represents the translation distance in the y-direction.</div></li></ul>"
                    ],
                    "guid": "w2p.F]miCJ",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the matrix form for a combined 2D rotation and translation transformation, and what does (\\(\\theta\\)) represent?",
                        "\\[\\begin{bmatrix}\n\\cos(\\theta) &amp; -\\sin(\\theta) &amp; t_x \\\\\n\\sin(\\theta) &amp; \\cos(\\theta) &amp; t_y \\\\\n0 &amp; 0 &amp; 1\n\\end{bmatrix}\n\\begin{bmatrix}\nx \\\\\ny \\\\\n1\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nx' \\\\\ny' \\\\\n1\n\\end{bmatrix}\\]where:<br>(\\(\\theta\\)) represents the angle of rotation.<br>"
                    ],
                    "guid": "v-9-7^X[9q",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the matrix form for a 2D scaling transformation, and what does (\\(a\\)) represent?",
                        "\\[\\begin{bmatrix}\na &amp; 0 &amp; 0 \\\\\n0 &amp; a &amp; 0 \\\\\n0 &amp; 0 &amp; 1\n\\end{bmatrix}\n\\begin{bmatrix}\nx \\\\\ny \\\\\n1\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nx' \\\\\ny' \\\\\n1\n\\end{bmatrix}\\]where:<br>(\\(a\\)) represents the scaling factor (equal in both x and y directions for uniform scaling).<br>"
                    ],
                    "guid": "HCsC:-YP!i",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the matrix form for a 2D aspect ratio transformation, and how does it differ from uniform scaling?",
                        "\\[\\begin{bmatrix}\na &amp; 0 &amp; 0 \\\\\n0 &amp; 1/a &amp; 0 \\\\\n0 &amp; 0 &amp; 1\n\\end{bmatrix}\n\\begin{bmatrix}\nx \\\\\ny \\\\\n1\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nx' \\\\\ny' \\\\\n1\n\\end{bmatrix}\\]<br>It scales the x and y dimensions by reciprocal factors, changing the aspect ratio of the image.<br>"
                    ],
                    "guid": "ArYt(U_*y_",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the matrix form for a 2D shear transformation?",
                        "\\[\\begin{bmatrix}\n1 &amp; a &amp; 0 \\\\\nb &amp; 1 &amp; 0 \\\\\n0 &amp; 0 &amp; 1\n\\end{bmatrix}\n\\begin{bmatrix}\nx \\\\\ny \\\\\n1\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nx' \\\\\ny' \\\\\n1\n\\end{bmatrix}\\]where:<br>(\\(a\\))&nbsp;controls horizontal shear<br>(\\(b\\)) controls vertical shear.<br>"
                    ],
                    "guid": "L[WoX9qILj",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What does \"degrees of freedom\" mean in the context of a transformation?",
                        "The number of independent parameters that can be varied to define the transformation. In this case, the 3x3 matrix has 9 elements (a, b, c, d, e, f, g, h, 1), so it has more degrees of freedom than a simpler transformation like a 2x2 matrix."
                    ],
                    "guid": "I#sT*RRUK/",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the purpose of normalizing the transformed vector?",
                        "To represent the transformed point in a standardized way. <br><img src=\"paste-ad4e82800eb3c72b227a6d2dbeacfe4119cb49e7.jpg\"><br>Dividing&nbsp;\\(u\\)&nbsp;and&nbsp;\\(v\\)&nbsp;by&nbsp;\\(w\\)&nbsp;scales the vector so that its third component is 1."
                    ],
                    "guid": "jB{6MgK(b0",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are the formulas for the normalized coordinates (\\(x',y'\\))?",
                        "<li>\\(x' = u / w\\)<br></li>\n<li>\\(y' = v / w\\)<br></li>"
                    ],
                    "guid": "Rb=h![aN(!",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why might you want to normalize the transformed vector?",
                        "<li>To remove the effect of scaling.</li>\n<li>To represent the point in a projective space (homogeneous coordinates).</li>\n<li>To simplify further calculations or comparisons.</li>"
                    ],
                    "guid": "OSGYcyHz,)",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are homogeneous coordinates?",
                        "A system of coordinates used in projective geometry where an extra dimension (usually denoted as '\\(w\\)') is added to represent points.<br>e.g.<br><div>The vector&nbsp;\\(\\begin{bmatrix}wx' \\\\ wy' \\\\ w\\end{bmatrix}\\)&nbsp;represents a point in homogeneous coordinates.</div><ul>\n<li>\\(x'\\)&nbsp;and&nbsp;\\(y'\\)&nbsp;are the normalized coordinates of the point.</li>\n<li>\\(w\\)&nbsp;is a scaling factor.</li></ul>"
                    ],
                    "guid": "zXTt=kyoVO",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is a planar homography (or projective transformation)?",
                        "<div>A transformation that maps points from one plane to another plane. It's represented by a 3x3 matrix and can model various geometric distortions, including:</div><ul>\n<li><strong>Translation:</strong> Shifting the image</li>\n<li><strong>Rotation:</strong> Rotating the image</li>\n<li><strong>Scaling:</strong>  Resizing the image</li>\n<li><strong>Shear:</strong> Skewing the image</li>\n<li><strong>Perspective distortion:</strong>  The \"keystone\" effect seen when a camera is not perpendicular to a planar surface.</li></ul>"
                    ],
                    "guid": "ky4ApQFLh9",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is RANSAC?",
                        "RANSAC stands for <strong>RANdom SAmple Consensus</strong>. It's an iterative algorithm used to estimate the parameters of a mathematical model from a set of data that contains outliers."
                    ],
                    "guid": "frI.nOdt05",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "RANSAC Algorithm",
                        "Assume:\n<br><ul><li>\\(M\\)&nbsp;data items required to estimate model&nbsp;\\(T\\)</li><li>\\(N\\)&nbsp;data items in total</li></ul>Algorithm:\n<br><ol><li>select&nbsp;\\(M\\)&nbsp;data items at random\n</li><li>estimate model&nbsp;\\(T\\)</li><li>find how many of the&nbsp;\\(N\\)&nbsp;data items fit&nbsp;\\(T\\)&nbsp;within tolerance Tol, call this&nbsp;\\(K\\)&nbsp;(i.e. \ncompute how many times the absolute residual is less than Tol). The points that \nhave an absolute residual less than Tol are the inliers; the other points are the \noutliers.\n</li><li>if&nbsp;\\(K\\)&nbsp;is large enough, either accept&nbsp;\\(T\\)&nbsp;or compute the least-squares estimate \nusing all inliers, and exit with success.\n</li><li>repeat steps 1..4 for&nbsp;\\(n\\)&nbsp;Iterations\n</li><li>fail - no good&nbsp;\\(T\\)&nbsp;fit of data</li></ol>"
                    ],
                    "guid": "EJ$y?6=y7-",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the main computational bottleneck in local feature matching?",
                        "The large number of distance calculations required."
                    ],
                    "guid": "QVc0qej`p|",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Can local feature matching be optimized?",
                        "<div>Yes, there are various techniques to speed up the process, such as:</div><ul>\n<li><strong>Approximate nearest neighbor search:</strong> Using data structures like k-d trees or locality-sensitive hashing to efficiently find approximate nearest neighbors.</li>\n<li><strong>Vocabulary trees:</strong>  Creating a hierarchical structure to organize features and reduce the search space.</li>\n<li><strong>Early termination:</strong> Stopping the distance calculation early if it's clear that the features are not a good match.</li></ul>"
                    ],
                    "guid": "OUZ>}yP{kq",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why can local feature matching be slow?",
                        "<li><strong>Many features:</strong> A typical image can have thousands of interest points and corresponding feature descriptors.</li>\n<li><strong>High dimensionality:</strong> Each SIFT descriptor is 128-dimensional.</li>\n<li><strong>Large-scale comparisons:</strong> Matching often involves comparing features across a large database of images.</li>"
                    ],
                    "guid": "L%lYmz,s:!",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the nearest neighbor search problem?",
                        "Given a set of points in a high-dimensional space and a query point, find the point in the set that is closest to the query point."
                    ],
                    "guid": "B_h#?PSlSQ",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are some approaches to optimize nearest neighbor search?",
                        "<li><strong>Indexing with tree structures:</strong>  Organize the points in a tree-like structure (e.g., k-d tree, ball tree) to efficiently search for nearest neighbors.</li>\n<li><strong>Hashing:</strong> Use hashing techniques (e.g., locality-sensitive hashing) to map points to buckets, allowing for faster search within buckets.</li>\n<li><strong>Quantizing the space:</strong>  Divide the space into discrete regions and assign points to these regions, enabling faster search within regions.</li>"
                    ],
                    "guid": "e|C*nvbR+I",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is a k-d tree?",
                        "A binary tree structure used to partition a k-dimensional space. It's a space-partitioning data structure for organizing points in a k-dimensional space."
                    ],
                    "guid": "jO@lCxDAG[",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How does a k-d tree partition the space?",
                        "It uses axis-aligned hyperplanes to divide the space into smaller regions."
                    ],
                    "guid": "e>JMq7]2E4",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is an axis-aligned hyperplane?",
                        "A plane that is perpendicular to one of the coordinate axes. For example, in a 2D space, an axis-aligned hyperplane is just a vertical or horizontal line."
                    ],
                    "guid": "N2HsaT=|[*",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How are the hyperplanes chosen in a k-d tree?",
                        "Typically, the algorithm cycles through the dimensions (axes) and splits the data at the median value along that dimension."
                    ],
                    "guid": "lwJg<WHy{8",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "When does the k-d tree construction stop?",
                        "<li>When a certain depth is reached.</li>\n<li>When the number of points in a leaf node falls below a predefined threshold.</li>"
                    ],
                    "guid": "Dl>7m(EN^m",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why do k-d trees struggle in high dimensions?",
                        "Because you often end up needing to search most of the tree to find the nearest neighbor. This makes the search less efficient."
                    ],
                    "guid": "FfPuOG[}]t",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is a bag data structure?",
                        "An unordered collection of elements where duplicates are allowed. This means the same element can appear multiple times in the bag."
                    ],
                    "guid": "C&G/>[.}1&",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How is a bag similar to a set?",
                        "Both are unordered collections. This means the elements don't have a specific order."
                    ],
                    "guid": "ym%H?-S5M5",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are some other names for a bag?",
                        "<li>Multiset</li>\n<li>Counted set</li>"
                    ],
                    "guid": "H&%9XA([sM",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are the steps for text processing (feature extraction)?",
                        "<ul><li>Tokenisation</li><li>Stop-word Removal</li><li>Stemming/Lemmatisation</li><li>Bag of Words</li></ul>"
                    ],
                    "guid": "D07n#(w!Nl",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the vector space model?",
                        "A model for representing text documents as vectors in a multi-dimensional space. Each dimension typically corresponds to a word or term."
                    ],
                    "guid": "CC]tqp,jS$",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How are documents represented in the vector space model?",
                        "Each document is represented as a vector. The values in the vector typically indicate the frequency or weight of words in the document."
                    ],
                    "guid": "B_ox`dS!4r",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How are queries represented in the vector space model?",
                        "Queries are also represented as vectors in the same space as the documents."
                    ],
                    "guid": "Png25SVtyR",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the key assumption in the vector space model?",
                        "Documents that are \"close together\" in the vector space are similar in meaning."
                    ],
                    "guid": "k)L<K?R|/a",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is a lexicon in the context of text processing?",
                        "The set of all unique words (after preprocessing, like stop-word removal and stemming) across all documents in a corpus. It's essentially the vocabulary used in the collection of documents."
                    ],
                    "guid": "Hd,I=`t3b=",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is another way to think of a bag-of-words vector?",
                        "As a histogram of word occurrences in the document."
                    ],
                    "guid": "OFMvqaQRX-",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why are bag-of-words vectors typically high-dimensional?",
                        "Because the lexicon can contain a very large number of words."
                    ],
                    "guid": "Eo0#t%r9|f",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why are bag-of-words vectors typically sparse?",
                        "Because most documents only contain a small subset of the words in the lexicon. This means that most of the elements in the vector will be zero."
                    ],
                    "guid": "l,^Ug%U*y+",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How can you optimize the computation of cosine similarity for sparse vectors?",
                        "By pre-computing and storing the magnitudes of the vectors (\\(||p||\\)&nbsp;and&nbsp;\\(||q||\\)). This avoids redundant calculations when comparing many vectors."
                    ],
                    "guid": "iTOB(o]eyR",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the basic idea behind weighting vectors in the vector space model?",
                        "The number of times a word occurs in a document (term frequency) can be used to reflect the importance of that word in the document."
                    ],
                    "guid": "d@R^Oqy,?h",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why is a term that appears in many documents not considered important?",
                        "Because it's likely a common word (like \"the,\" \"a,\" \"is\") that doesn't carry much specific meaning in any particular document. These are often called \"stop words.\""
                    ],
                    "guid": "BBIDef=+4)",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "When is a term likely to be important in a document?",
                        "If it appears frequently in that document but is rare across other documents in the collection. This suggests that the term is specific to the topic of that document."
                    ],
                    "guid": "BjY12aOrJ{",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are some possible weighting schemes for weighting vectors?",
                        "<ol><li>Binary weights</li><li>Raw frequency</li><li>TF-IDF (Term Frequency-Inverse Document Frequency)</li></ol>"
                    ],
                    "guid": "J=Pc{L/ThE",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What does TF-IDF stand for, and what are its two main components?",
                        "<div>TF-IDF stands for Term Frequency-Inverse Document Frequency. Its two components are:</div><ol><li><div><strong>Term Frequency (TF):</strong>&nbsp;The frequency count of a term within a document.</div></li><li><div><strong>Inverse Document Frequency (IDF):</strong>&nbsp;A measure that gives high values to rare words and low values to common words across the entire document collection.</div></li></ol>"
                    ],
                    "guid": "z3]h+g:~Hf",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the purpose of the \"Inverse Document Frequency (IDF)\" in TF-IDF?",
                        "IDF aims to reduce the weight of terms that appear frequently in many documents (and are thus less discriminative) while increasing the weight of terms that are rare and potentially more informative for distinguishing between documents."
                    ],
                    "guid": "A])3IK%7o6",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Describe the \"Binary weights\" weighting scheme.",
                        "In binary weighting, a term is either present (1) or absent (0) in a document. The vector only records the presence or absence of a term, not its frequency."
                    ],
                    "guid": "jdTtL#!j3o",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Describe the \"Raw frequency\" weighting scheme.",
                        "In raw frequency weighting, the vector records the number of times a term appears in a document (its frequency of occurrence)."
                    ],
                    "guid": "C{DfC>eoZD",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is Vector Quantization?",
                        "A lossy data compression technique that uses a set of representative vectors to approximate a larger set of vectors."
                    ],
                    "guid": "O+;m7@Ol~0",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How can we learn a set of representative vectors for Vector Quantization?",
                        "Using a technique like K-Means clustering on a set of vectors, we can identify a fixed-size set of representative vectors (cluster centers)."
                    ],
                    "guid": "s`B=%ZKh9]",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is a \"codebook\" in the context of Vector Quantization?",
                        "The set of representation vectors derived from a technique like K-Means clustering."
                    ],
                    "guid": "MQSlY^D((*",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is a \"fixed size set\" refer to when learning a Vector Quantiser?",
                        "The number of representative vectors you want to end up with. In k-means clustering this would be&nbsp;\\(k\\)."
                    ],
                    "guid": "rSVvod?.s,",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How is Vector Quantization achieved?",
                        "By representing a vector with another <em>approximate</em> vector, chosen from a pool of representative vectors."
                    ],
                    "guid": "b4;k=vCRHP",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What happens to each input vector in Vector Quantization?",
                        "It is assigned to the \"closest\" vector from the pool of representative vectors."
                    ],
                    "guid": "qm1dw3oXFm",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "When an input vector is assigned to a vector in the pool, is that vector identical to the original?",
                        "No, it is an <em>approximate</em> vector. The assigned vector is chosen because it's the \"closest,\" not necessarily identical. This introduces the \"lossy\" aspect of the compression."
                    ],
                    "guid": "s0?wR:{PG@",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is a \"visual word\"?",
                        "A representative vector that replaces a descriptor (e.g., a SIFT descriptor) after vector quantization. It represents a small image patch with a specific pattern of pixels."
                    ],
                    "guid": "DXI~^NMm2}",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What does a visual word describe?",
                        "A small image patch with a certain pattern of pixels."
                    ],
                    "guid": "rv.G]L4^KF",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the process of applying vector quantization to local features analogous to?",
                        "The process of stemming words in natural language processing."
                    ],
                    "guid": "q8{o>jX^l)",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the \"codebook\" in this context, and what is it analogous to?",
                        "The codebook is the set of all visual words. It's the visual equivalent of a lexicon or vocabulary."
                    ],
                    "guid": "lHu95}B+P`",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How might replacing descriptors with visual words be similar to stemming?",
                        "Both processes reduce variations of a concept to a single representative form. Stemming reduces different word forms (e.g., \"running,\" \"runs,\" \"ran\") to a common stem (e.g., \"run\"). Similarly, visual words group similar image patches into a single representative visual word."
                    ],
                    "guid": "m6,n%ipL_C",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What can we build once we have a Bag of Visual Words (BoVW) and the codebook?",
                        "Histograms of visual word occurrences."
                    ],
                    "guid": "N%HD*./b.C",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What does a histogram of visual word occurrences represent?",
                        "The frequency of each visual word from the codebook within an image or a region of an image."
                    ],
                    "guid": "h2)p*IRdt&",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is one advantage of using histograms of visual words?",
                        "It allows us to aggregate a variable number of local descriptors into a fixed-length vector."
                    ],
                    "guid": "Jl/2PDw*+i",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why is having a fixed-length vector representation useful?",
                        "It's useful for machine learning algorithms that often require fixed-size input."
                    ],
                    "guid": "pvfB61X/OL",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Besides machine learning, what else does using visual word histograms allow us to do?",
                        "Apply techniques from text retrieval to images."
                    ],
                    "guid": "O9*wJY@]c{",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the key parameter when building visual word representations?",
                        "The size of the vocabulary (codebook size)."
                    ],
                    "guid": "m0]NihT@uh",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What happens if the codebook size is too small?",
                        "All vectors will look similar, meaning they won't be distinctive enough to differentiate between different image patches."
                    ],
                    "guid": "EbqhU5*4Df",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the problem with a codebook that is not distinctive?",
                        "It won't be very useful for tasks like image classification or retrieval because different images might end up having very similar representations."
                    ],
                    "guid": "ungI|VWW9Y",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What happens if the codebook size is too large?",
                        "The same visual words might never appear across different images, making it difficult to find similarities between images."
                    ],
                    "guid": "Ij:X+5^y2n",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the problem with a codebook that is too distinctive?",
                        "Similar image patches might be represented by completely different visual words, making it hard to identify related images."
                    ],
                    "guid": "IHCE/6jsP6",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the general goal when choosing a codebook size?",
                        "To find a balance where the visual words are distinctive enough to capture meaningful differences between image patches but also general enough to allow for comparisons and finding similarities across images."
                    ],
                    "guid": "vkVJ]|oAu)",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the main idea of Bag-of-Visual-Words (BoVW) retrieval?",
                        "Techniques used for text retrieval can be directly applied to images when using a visual word representation."
                    ],
                    "guid": "J}uhoO7Upg",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the \"vector space model,\" and how does it relate to BoVW retrieval?",
                        "It's a model used in information retrieval where items (e.g., documents or images) are represented as vectors. In BoVW, images are represented as vectors of visual word counts."
                    ],
                    "guid": "gI-5#ktKXd",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is \"cosine similarity,\" and how is it used in BoVW retrieval?",
                        "A measure of similarity between two vectors, calculated as the cosine of the angle between them. It can be used to compare the visual word histograms of images."
                    ],
                    "guid": "GV:ndL1jnT",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are \"weighting schemes,\" and why are they relevant to BoVW retrieval?",
                        "Methods for assigning different weights to terms (or visual words) based on their importance. Common examples include TF-IDF. They can improve the accuracy of image retrieval."
                    ],
                    "guid": "gIFXjvkXC>",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is an \"inverted index,\" and how does it help with BoVW retrieval?",
                        "A data structure that maps terms (or visual words) to the documents (or images) that contain them. It enables efficient searching and retrieval of images based on their visual word content."
                    ],
                    "guid": "Q~MZ*C{QPf",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Name four specific text retrieval concepts that can be applied to image retrieval using BoVW.",
                        "<li>Vector space model</li>\n<li>Cosine similarity</li>\n<li>Weighting schemes</li>\n<li>Inverted index</li>"
                    ],
                    "guid": "L.aLu&(j*-",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "When does an inverted index provide a performance gain in a visual word system?",
                        "When the visual word vectors are sparse. This means that each image is represented by only a small subset of the total visual words in the codebook."
                    ],
                    "guid": "cc@BoDP)]y",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why is it important for visual words to be distinctive?",
                        "To minimize mismatching, meaning that different types of image patches should ideally be represented by different visual words."
                    ],
                    "guid": "mPc;xc!}%w",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What does the need for sparse vectors and distinctive visual words imply about the optimal codebook size?",
                        "It implies that a very large codebook is generally needed."
                    ],
                    "guid": "ohAxNb,LL5",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is a typical codebook size used in modern visual word systems for SIFT vectors?",
                        "Often 1 million or more visual words."
                    ],
                    "guid": "e:YxQT_R0A",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why don't we want to end up explicitly scoring all documents (or images) in a retrieval system?",
                        "It would be computationally very expensive and inefficient. Sparse vectors and an inverted index help to avoid this."
                    ],
                    "guid": "qhVR<+s^yh",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the relationship between codebook size and the distinctiveness of visual words?",
                        "Generally, a larger codebook allows for more distinct visual words, as there are more \"slots\" to represent different types of image patches."
                    ],
                    "guid": "rv(}0&YN~&",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is a major challenge when working with very large codebooks for visual words?",
                        "The computational cost of clustering a massive number of high-dimensional features. For instance, using k-means to learn 1 million clusters in 128 dimensions from tens of millions of features."
                    ],
                    "guid": "z=@7.89&cz",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why is it \"non-trivial\" to perform k-means clustering with such large datasets and high dimensionality?",
                        "Because the computational complexity and memory requirements of standard k-means become extremely demanding in these scenarios."
                    ],
                    "guid": "n+(Uulc!aQ",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Does Vector Quantization face the same challenges as learning large codebooks?",
                        "Yes, vector quantization has similar computational challenges when dealing with a large number of high-dimensional vectors."
                    ],
                    "guid": "bMZF:=pKY:",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is a common approach to address the computational challenges of large codebooks and vector quantization?",
                        "Using approximate methods, such as approximate k-d trees."
                    ],
                    "guid": "nf)tc2r-/R",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is an \"approximate k-d tree\" and how might it be used?",
                        "It is a data structure that allows for faster, but approximate, nearest neighbor searches. In this context, it can be used for approximate vector quantization or to speed up the assignment of features to visual words during the creation of histograms."
                    ],
                    "guid": "poQ6d#!wVY",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the trade off when using approximate methods?",
                        "You get a significant improvement in speed, but may sacrifice some accuracy."
                    ],
                    "guid": "GnAl4yyGf_",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the first step in building a BoVW retrieval system?",
                        "Collect the corpus of images that will be indexed and made searchable."
                    ],
                    "guid": "zy*gK]]L]#",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How do we create the BoVW representation for each image?",
                        "Vector quantize the features extracted from each image using the learned codebook. This creates a histogram of visual word occurrences for each image."
                    ],
                    "guid": "Gm^)^Kw;WZ",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why do we build an inverted index in a BoVW system?",
                        "To enable efficient searching and retrieval of images based on their visual word content. The inverted index maps visual words to the images that contain them."
                    ],
                    "guid": "K(<l}3J~$z",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the purpose of learning a codebook from a <em>sample</em> of the features?",
                        "Learning the codebook from all features could be computationally too expensive. Sampling a subset makes the process more manageable."
                    ],
                    "guid": "nQPzfA<je6",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the overall goal of building a BoVW retrieval system?",
                        "To create a system that can efficiently search and retrieve images based on their visual content, similar to how text retrieval systems work with text documents."
                    ],
                    "guid": "I*DhW80u?r",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Steps for building a BoVW Retrieval System",
                        "<ol><li>Collect the corpus of images that are to be indexed and \nmade searchable</li><li>Extract local features from each image</li><li>Learn a large codebook from (a sample of) the features</li><li>Vector quantise the features, and build BoVW\n representations for each image</li><li>Construct an inverted index with the BoVW\n representations</li></ol>"
                    ],
                    "guid": "fhsb?#W(Y}",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "For machine learning tasks using visual words, how does the optimal codebook size compare to that used for image search?",
                        "The codebook vocabulary needs to be much smaller for machine learning tasks compared to image search."
                    ],
                    "guid": "P-Gw&Ft.q|",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why do machine learning techniques generally prefer smaller visual word vectors?",
                        "For both performance (computational efficiency) and effectiveness (better generalization and less overfitting)."
                    ],
                    "guid": "G3+P%cz~]R",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "In the context of machine learning with visual words, what is the trade-off when using a smaller codebook?",
                        "Visual words can be less distinctive, allowing a little more variation between matching features. This means that slightly different image patches might be mapped to the same visual word."
                    ],
                    "guid": "B^#N98J?gn",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is a typical range for the number of visual words in a codebook used for machine learning applications?",
                        "As small as a few hundred, and up to a few thousand."
                    ],
                    "guid": "wi;x8J+!51",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are the implications of allowing visual words to be less distinctive when using smaller codebooks?",
                        "<strong>Pro:</strong> More robust to small variations in features.<br>\n<strong>Con:</strong> Might reduce the precision of the model if very different image patches are mapped to the same visual word."
                    ],
                    "guid": "Jq0S94?-pz",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How does a smaller codebook contribute to improved performance in machine learning?",
                        "Smaller codebooks lead to lower-dimensional feature vectors, which are faster to process and require less memory."
                    ],
                    "guid": "G[0q,%#=;u",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the main idea behind Dense SIFT?",
                        "Extracting SIFT features across a regular grid covering the entire image, rather than just at Difference of Gaussian (DoG) interest points."
                    ],
                    "guid": "j6qEHq1tCD",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the primary advantage of Dense SIFT over traditional SIFT (using DoG interest points)?",
                        "It provides much more complete coverage of the image, capturing information from all regions, not just those identified as \"interesting\" by DoG."
                    ],
                    "guid": "w^c|IS_NcK",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What does \"sampling step\" refer to in the context of Dense SIFT?",
                        "The spacing between the grid points where SIFT features are extracted. A smaller sampling step leads to denser feature extraction."
                    ],
                    "guid": "z1q(a7aVy*",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How might the choice of sampling step affect the number of features extracted with Dense SIFT?",
                        "A smaller sampling step will result in a larger number of features extracted, as the grid points will be closer together."
                    ],
                    "guid": "kvMS@$igsx",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is a potential disadvantage of using Dense SIFT compared to using DoG interest points?",
                        "It can lead to a much larger number of features, which might increase computational cost and memory requirements. It might also capture more redundant information."
                    ],
                    "guid": "d$rS$g!&3b",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the main concept behind Pyramid Dense SIFT?",
                        "Sampling Dense SIFT features across multiple scales of a Gaussian pyramid of the image."
                    ],
                    "guid": "qe}Lrzp;t0",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "In Pyramid Dense SIFT, what is the relationship between the sampling region size and the scale level in the pyramid?",
                        "The sampling region size is fixed, so at higher levels of the pyramid (smaller image sizes), you sample more content relative to the original image size."
                    ],
                    "guid": "G=A3KUp1mF",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How does Pyramid Dense SIFT improve upon regular Dense SIFT?",
                        "By extracting features at multiple scales, it captures both fine-grained details (at lower levels of the pyramid) and larger structures (at higher levels), leading to a more comprehensive representation of the image."
                    ],
                    "guid": "cS96uJ5Xg8",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why might using Pyramid Dense SIFT lead to better performance in tasks like object recognition or image classification?",
                        "Because objects can appear at different sizes in images. By sampling at multiple scales, Pyramid Dense SIFT can capture features that are invariant to scale changes."
                    ],
                    "guid": "POz>g1shyx",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How are datasets typically divided for use in machine learning, especially for scene classification?",
                        "They are usually split into \"training\" and \"test\" sets."
                    ],
                    "guid": "mP#bToW.M]",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the purpose of the \"training\" set in a dataset?",
                        "It is used to train the parameters of the classifier."
                    ],
                    "guid": "h#o~L>x4IO",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the purpose of the \"test\" set in a dataset?",
                        "It is used to evaluate the performance of the trained classifier on unseen data."
                    ],
                    "guid": "i1~{z(y2{_",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is a common practice to ensure that researchers do not \"cheat\" when evaluating their classifiers?",
                        "Sometimes the labels of the test set are withheld completely, so the researchers cannot tune their classifiers to the test data."
                    ],
                    "guid": "tDu!QB6T^O",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why is it important to only use the training set during the training phase?",
                        "To ensure that the evaluation on the test set reflects the classifier's ability to generalize to <em>new, unseen</em> data, which is a crucial aspect of machine learning."
                    ],
                    "guid": "rW8b#c6c)w",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Steps to building a BoVW",
                        "<ol><li>Firstly the raw features need to be extracted from the training \nimages</li><li>Then (if necessary) learn a codebook from these features</li></ol><ul><li>i.e. using k-means on the raw features</li><li>might be a uniform random sample of all the features \nrather than all of them</li></ul>Apply (vector) quantisation to the raw features and count the \nnumber of occurrences to build histograms for each image<br>"
                    ],
                    "guid": "G:2@,3v$Ei",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "&nbsp;In the context of Bag-of-Visual-Words, what can be used to train classifiers?",
                        "The histograms of visual word occurrences."
                    ],
                    "guid": "B48rwD^A9!",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is \"OvR\" in the context of classifiers?",
                        "It stands for \"One-vs-Rest\" (or \"One-vs-All\"). It's a strategy for using binary classifiers for multi-class classification problems."
                    ],
                    "guid": "r(^Cjw.JV[",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is cross-validation?",
                        "A technique where the training data is split into subsets. The classifier is trained on some subsets and validated on the remaining subset. This process is repeated multiple times with different subsets."
                    ],
                    "guid": "Ppml*m#;d>",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "After finding the optimal parameters using cross-validation, what is the next step?",
                        "Re-train the classifier using the entire training set and the optimal parameter values."
                    ],
                    "guid": "eTQpH|/9!n",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why might you not train on all the training data initially?",
                        "To have a validation set to estimate performance on unseen data and optimise parameters."
                    ],
                    "guid": "JL@Oh$St^m",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why re-train with all the data after optimising parameters?",
                        "To leverage all available training data and potentially improve the classifier's performance."
                    ],
                    "guid": "oRGskKjCJJ",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "On which datasets is classifier performance typically evaluated?",
                        "The test set and (during training) the validation set."
                    ],
                    "guid": "An??pCrky@",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How is average precision calculated?",
                        "It's the proportion of the number of correct classifications to the total number of predictions made. In other words: (Number of Correct Predictions) / (Total Number of Predictions)"
                    ],
                    "guid": "uoQzyi?8nc",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Why is average precision considered a \"summary measure\"?",
                        "Because it provides a single value that summarizes the overall performance of the classifier across all classes."
                    ],
                    "guid": "PyEN..NW-_",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                }
            ],
            "reviewLimit": null,
            "reviewLimitToday": null
        },
        {
            "__type__": "Deck",
            "children": [],
            "crowdanki_uuid": "e05051ee-d758-11ef-9177-204ef6f14df6",
            "deck_config_uuid": "0aa25128-d1e9-11ef-92a1-204ef6f14df6",
            "desc": "",
            "dyn": 0,
            "extendNew": 0,
            "extendRev": 0,
            "media_files": [
                "paste-03c2f90764e4f469286265c2279a6562d2a533fc.jpg",
                "paste-d9c79e76c7fbce8e457345749b992942d6596acf.jpg"
            ],
            "name": "Week 8",
            "newLimit": null,
            "newLimitToday": null,
            "notes": [
                {
                    "__type__": "Note",
                    "fields": [
                        "Camera Calibration: Formula<br><img src=\"paste-d9c79e76c7fbce8e457345749b992942d6596acf.jpg\">",
                        "\\[\\lambda \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} = \n\\begin{bmatrix} f_x &amp; 0 &amp; p_x \\\\ 0 &amp; f_y &amp; p_y \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}\n\\begin{bmatrix} R &amp; t \\\\ 0&amp; 1\\end{bmatrix}\n\\begin{bmatrix} X \\\\ Y \\\\ Z \\\\ 1 \\end{bmatrix}\\]where:<br>\\(\\lambda \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix}\\):&nbsp;\\(\\lambda\\)&nbsp;is a scalar value representing the depth or scale factor and&nbsp;\\(\\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix}\\)&nbsp;is the 2D image point coordinates in homogeneous form.&nbsp;\\(x\\)&nbsp;and&nbsp;\\(y\\)&nbsp;are the pixel coordinates, and the&nbsp;\\(1\\)&nbsp;is added for mathematical convenience in matrix operations.<br>\\(\\begin{bmatrix} f_x &amp; 0 &amp; p_x \\\\ 0 &amp; f_y &amp; p_y \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}\\): This is the <strong>camera intrinsic matrix</strong>.&nbsp;<li>\\(f_x\\),&nbsp;\\(f_y\\): Focal lengths of the camera along the x and y axes, respectively (in pixels).</li>\n<li>\\(p_x\\),&nbsp;\\(p_y\\): Coordinates of the principal point (center of the image plane) in pixels.</li>The zeros in the matrix ensure that x projection depends only on the X coordinate and not Y, and vice versa. The final row,&nbsp;\\(0, 0, 1\\), is added because of the use of homogeneous coordinates.<br>\\(\\begin{bmatrix} R &amp; t \\\\ 0 &amp; 1\\end{bmatrix}\\): This represents the <strong>camera extrinsic matrix</strong>.&nbsp;<li>\\(R\\): A 3x3 rotation matrix representing the camera's orientation in the world.</li>\n<li>\\(t\\): A 3x1 translation vector representing the camera's position in the world.</li>\\(\\begin{bmatrix} X \\\\ Y \\\\ Z \\\\ 1 \\end{bmatrix}\\): The 3D world point coordinates in homogeneous form.&nbsp;\\(X\\),&nbsp;\\(Y\\), and&nbsp;\\(Z\\)&nbsp;represent the point's location in 3D space, and&nbsp;\\(1\\)&nbsp;is added for mathematical convenience. The matrix operation can not be performed without making it a 4x1 matrix, since&nbsp;\\([R,t]\\)&nbsp;is a 3x4 matrix.<br>"
                    ],
                    "guid": "EGc{#xJe@f",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "\\[\\lambda \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} = \n\\begin{bmatrix} f_x &amp; 0 &amp; p_x \\\\ 0 &amp; f_y &amp; p_y \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}\n\\begin{bmatrix} R &amp; t \\\\ 0&amp; 1\\end{bmatrix}\n\\begin{bmatrix} X \\\\ Y \\\\ Z \\\\ 1 \\end{bmatrix}\\]Layout of this formula geometrically<br>",
                        "<img src=\"paste-03c2f90764e4f469286265c2279a6562d2a533fc.jpg\">"
                    ],
                    "guid": "i;XC&@bf4k",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is Camera Calibration?",
                        "The process of estimating the intrinsic and extrinsic parameters of a camera."
                    ],
                    "guid": "JCOFXMS!e&",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What are the two main types of parameters estimated during camera calibration?",
                        "<li><strong>Intrinsic Parameters:</strong> Relate to the camera's internal characteristics (e.g., focal length, principal point).</li>\n<li><strong>Extrinsic Parameters:</strong> Relate to the camera's position and orientation in the world (e.g., rotation and translation).</li>"
                    ],
                    "guid": "AJ%@p*,})V",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How is camera calibration typically performed?",
                        "By solving sets of point correspondences from images of known \"calibration patterns\" (e.g., a checkerboard)."
                    ],
                    "guid": "lM!uUjWe#a",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is a calibration pattern, and why is it used?",
                        "A calibration pattern is a known object (like a checkerboard) with easily detectable features. These known points on the pattern are used to solve the equations and estimate the camera parameters. The pattern provides a set of known 3D points and their corresponding 2D image projections."
                    ],
                    "guid": "eQx^%I/Cwf",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is Epipolar Geometry?",
                        "The geometry of stereo vision. It describes the geometric relationship between two views of the same scene, obtained from different viewpoints.  It's independent of scene structure and only depends on the cameras' internal parameters and relative pose."
                    ],
                    "guid": "hnS/e]*TK+",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the Baseline in Epipolar Geometry?",
                        "The line segment connecting the two camera centers (\\(O_L\\)&nbsp;and&nbsp;\\(O_R\\))."
                    ],
                    "guid": "sikg&I?E9J",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is an Epipole?",
                        "The point where the baseline intersects the image plane."
                    ],
                    "guid": "e,(+F75oJn",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is an Epipolar Line?",
                        "The intersection of an epipolar plane with an image plane.  All epipolar lines in an image intersect at the epipole for that image.  A point in one image defines the epipolar line in the other image."
                    ],
                    "guid": "Cpv7vz>64W",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Define an Epipolar Plane.",
                        "A plane that contains the baseline (\\(O_L - O_R\\)) and a point&nbsp;\\(X\\)&nbsp;in the 3D scene."
                    ],
                    "guid": "gDr</6_D)Y",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "If you have a point&nbsp;\\(X_L\\)&nbsp;in the left image, how can Epipolar Geometry help you find its corresponding point&nbsp;\\(X_R\\)&nbsp;in the right image?",
                        "Epipolar geometry constrains the search for&nbsp;\\(X_R\\)&nbsp;to lie along the epipolar line in the right image that corresponds to the point&nbsp;\\(X_L\\)&nbsp;in the left image. The point&nbsp;\\(X_R\\)&nbsp;will lie somewhere on this line."
                    ],
                    "guid": "N|E+kDV%}|",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is Image Rectification in the context of stereo vision?",
                        "A transformation process that reprojects image planes onto a common plane parallel to the baseline connecting the two camera centers. This makes the epipolar lines in both images parallel and horizontally aligned, simplifying the search for corresponding points."
                    ],
                    "guid": "wm:yFtWe!8",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How does rectification simplify the process of finding corresponding points (stereo matching)?",
                        "After rectification, corresponding points lie on the same horizontal line (scanline) in both images. This reduces the search space from 2D to 1D, making the matching process more efficient and robust."
                    ],
                    "guid": "u$3-Qfy_zx",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How to compute a disparity map?",
                        "By matching pixels along the epipolar lines."
                    ],
                    "guid": "S9Eeo/Wnd",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "What is the formula for calculating depth (\\(Z\\)) in a parallel stereo camera system?",
                        "\\[Z=\\lambda-\\frac{\\lambda B}{x_{I_1}-x_{I_2}}\\]where:<br>\\(Z\\)&nbsp;is the depth<br>\\(\\lambda\\)&nbsp;is the focal length of the camera<br>\\(B\\)&nbsp;is the baseline (distance between the camera centers)<br>\\((x_{I_1} - x_{I_2})\\)&nbsp;is the disparity (difference in x-coordinates of the corresponding points in the two images)<br>"
                    ],
                    "guid": "Bf,$Ui,vd[",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "Define Disparity in the context of stereo vision.",
                        "The difference in the image location of an object seen by the left and right cameras, resulting from the parallax between the two viewpoints. In a parallel stereo system, this is typically the difference in the x-coordinates"
                    ],
                    "guid": "D^9iVqcU)y",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                },
                {
                    "__type__": "Note",
                    "fields": [
                        "How are corresponding points related in a parallel stereo camera system after rectification?",
                        "They have the same y-coordinate and differ only in their x-coordinate by the disparity:<br>\\[I_1(x,y)=I_2(x+d(x,y),y)\\]<br>"
                    ],
                    "guid": "LbU@&}sf8+",
                    "note_model_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
                    "tags": []
                }
            ],
            "reviewLimit": null,
            "reviewLimitToday": null
        }
    ],
    "crowdanki_uuid": "e0495d2c-d758-11ef-8abb-204ef6f14df6",
    "deck_config_uuid": "0aa25128-d1e9-11ef-92a1-204ef6f14df6",
    "deck_configurations": [
        {
            "__type__": "DeckConfig",
            "answerAction": 0,
            "autoplay": true,
            "buryInterdayLearning": false,
            "crowdanki_uuid": "0aa25128-d1e9-11ef-92a1-204ef6f14df6",
            "desiredRetention": 0.9,
            "dyn": false,
            "easyDaysPercentages": [
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0
            ],
            "fsrsParams5": [],
            "fsrsWeights": [],
            "ignoreRevlogsBeforeDate": "",
            "interdayLearningMix": 0,
            "lapse": {
                "delays": [
                    10.0
                ],
                "leechAction": 1,
                "leechFails": 8,
                "minInt": 1,
                "mult": 0.0
            },
            "maxTaken": 60,
            "name": "Default",
            "new": {
                "bury": false,
                "delays": [
                    1.0,
                    10.0
                ],
                "initialFactor": 2500,
                "ints": [
                    1,
                    4,
                    0
                ],
                "order": 1,
                "perDay": 20
            },
            "newGatherPriority": 4,
            "newMix": 0,
            "newPerDayMinimum": 0,
            "newSortOrder": 0,
            "questionAction": 0,
            "replayq": true,
            "rev": {
                "bury": false,
                "ease4": 1.3,
                "hardFactor": 1.2,
                "ivlFct": 1.0,
                "maxIvl": 36500,
                "perDay": 200
            },
            "reviewOrder": 0,
            "secondsToShowAnswer": 0.0,
            "secondsToShowQuestion": 0.0,
            "sm2Retention": 0.9,
            "stopTimerOnAnswer": false,
            "timer": 0,
            "waitForAudio": true,
            "weightSearch": ""
        }
    ],
    "desc": "",
    "dyn": 0,
    "extendNew": 0,
    "extendRev": 0,
    "media_files": [],
    "name": "Computer Vision",
    "newLimit": null,
    "newLimitToday": null,
    "note_models": [
        {
            "__type__": "NoteModel",
            "crowdanki_uuid": "0aa2c666-d1e9-11ef-90da-204ef6f14df6",
            "css": ".card {\n    font-family: arial;\n    font-size: 20px;\n    text-align: center;\n    color: black;\n    background-color: white;\n}\n",
            "flds": [
                {
                    "collapsed": false,
                    "description": "",
                    "excludeFromSearch": false,
                    "font": "Arial",
                    "id": 5442347928588566219,
                    "name": "Front",
                    "ord": 0,
                    "plainText": false,
                    "preventDeletion": false,
                    "rtl": false,
                    "size": 20,
                    "sticky": false,
                    "tag": null
                },
                {
                    "collapsed": false,
                    "description": "",
                    "excludeFromSearch": false,
                    "font": "Arial",
                    "id": 994669028030884850,
                    "name": "Back",
                    "ord": 1,
                    "plainText": false,
                    "preventDeletion": false,
                    "rtl": false,
                    "size": 20,
                    "sticky": false,
                    "tag": null
                }
            ],
            "latexPost": "\\end{document}",
            "latexPre": "\\documentclass[12pt]{article}\n\\special{papersize=3in,5in}\n\\usepackage[utf8]{inputenc}\n\\usepackage{amssymb,amsmath}\n\\pagestyle{empty}\n\\setlength{\\parindent}{0in}\n\\begin{document}\n",
            "latexsvg": false,
            "name": "Basic",
            "originalStockKind": 1,
            "req": [
                [
                    0,
                    "any",
                    [
                        0
                    ]
                ]
            ],
            "sortf": 0,
            "tmpls": [
                {
                    "afmt": "{{FrontSide}}\n\n<hr id=answer>\n\n{{Back}}",
                    "bafmt": "",
                    "bfont": "",
                    "bqfmt": "",
                    "bsize": 0,
                    "did": null,
                    "id": -4982990792209515436,
                    "name": "Card 1",
                    "ord": 0,
                    "qfmt": "{{Front}}"
                }
            ],
            "type": 0
        }
    ],
    "notes": [],
    "reviewLimit": null,
    "reviewLimitToday": null
}